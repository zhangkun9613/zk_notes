{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, MultiLabelBinarizer, scale, minmax_scale\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import sparse\n",
    "import lightgbm as lgb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sc3/miniconda2/envs/ydzhang/lib/python3.6/site-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv('data_df_new.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[['user_id','question_id']].groupby('user_id').count().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sc3/miniconda2/envs/ydzhang/lib/python3.6/site-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "train_invite_files = [\n",
    "                      'invite_info_[3840, 3860]-[3861, 3867]_quest-hist_topics_quest-user_follow_topics_quest-inn_topics_dist.csv',\n",
    "#                      'invite_info_[3840, 3860]-[3861, 3867]_quest_hist_meanpool_dist.csv', \n",
    "#                      'invite_info_[3840, 3860]-[3861, 3867]_quest_hist_minpool_dist.csv',\n",
    "#                     'invite_info_[3840, 3860]-[3861, 3867]_quest_hist_maxpool_dist.csv',\n",
    "                    ]\n",
    "test_invite_files = [\n",
    "                     'invite_info_test-[3847, 3867]-[3868, 3874]_quest-hist_topics_quest-user_follow_topics_quest-inn_topics_dist.csv',\n",
    "#                     'invite_info_test-[3847, 3867]-[3868, 3874]_quest_hist_maxpool_dist.csv',\n",
    "#                     'invite_info_test-[3847, 3867]-[3868, 3874]_quest_hist_meanpool_dist.csv',\n",
    "#                     'invite_info_test-[3847, 3867]-[3868, 3874]_quest_hist_minpool_dist.csv',\n",
    "]\n",
    "\n",
    "train_base_df = pd.read_csv('../ydzhang/data/'+train_invite_files[0],sep='\\t',index_col=0)\n",
    "\n",
    "test_base_df = pd.read_csv('../ydzhang/data/'+test_invite_files[0],sep='\\t',index_col=0)\n",
    "length = len(train_invite_files)\n",
    "for i in range(1,length):\n",
    "    train_t = pd.read_csv('../ydzhang/data/'+train_invite_files[i],sep='\\t',index_col=0)\n",
    "    train_base_df = pd.concat([train_base_df,train_t],axis = 1)\n",
    "for i in range(1,length):\n",
    "    test_t = pd.read_csv('../ydzhang/data/'+test_invite_files[i],sep='\\t',index_col=0)\n",
    "    test_base_df = pd.concat([test_base_df,test_t],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = pd.concat([train_base_df,test_base_df],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([data_df,base_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.drop(columns = base_df.columns,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sec in base_df.columns:\n",
    "    data_df[sec] = minmax_scale(data_df[sec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sc3/miniconda2/envs/ydzhang/lib/python3.6/site-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "test_df =pd.read_csv('evalute.csv',index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df =pd.read_csv('invite_info.csv',index_col=0)\n",
    "test_df =pd.read_csv('evalute.csv',index_col=0)\n",
    "\n",
    "def get_day(str):\n",
    "    return int(str.split('-')[0][1:])\n",
    "train_df['invite_create_day'] = train_df['create_time'].apply(get_day)\n",
    "\n",
    "ques_vecs_secs = ['title_SW_vec_his_mean', 'title_W_vec_his_mean','describe_W_vec_his_mean','title_SW_vec_his_max', 'title_W_vec_his_max','describe_W_vec_his_max']\n",
    "for sec in ques_vecs_secs:\n",
    "    data_df[sec] = minmax_scale(data_df[sec])\n",
    "\n",
    "drop_columns = ['is_answer', 'question_id', 'user_id', 'topic_id','follow_topics', 'interest_topics']\n",
    "train_secs = data_df.columns.drop(drop_columns)\n",
    "trian_start = 3838; train_end = 3867\n",
    "x_train = data_df[train_secs][:train_df.shape[0]][train_df['invite_create_day']>train_end-7].values\n",
    "y_train = data_df[:train_df.shape[0]]['is_answer'][train_df['invite_create_day']>train_end-7].values\n",
    "x_test = data_df[train_secs][train_df.shape[0]:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "params = {\n",
    "    'boosting_type':'gbdt','objective':'binary','metric':['auc', 'binary_logloss'],\n",
    "    'n_jobs':-1,'num_threads':30,'n_estimators':2000,'subsample':0.8,'subsample_freq':1,\n",
    "    'num_leaves':64,'learning_rate':0.01,\n",
    "    'max_bin':425,'subsample_for_bin':50000,'min_split_gain':0,\n",
    "    'min_child_weight':5,'min_child_samples':10,\n",
    "    'colsample_bytree':1,'reg_alpha':3,'reg_lambda':5,\n",
    "    'seed':1000,'silent':True    \n",
    "}\n",
    "lgb_train = lgb.Dataset(x_train,y_train,free_raw_data=False)#, feature_name=train_secs.to_list(), categorical_feature=category_columns)\n",
    "gbm = lgb.train(params,lgb_train,valid_sets=lgb_train,early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f570a719f98>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.save_model('base_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = lgb.Booster(model_file='base_model.txt')\n",
    "y_pred = boost.predict(x_test)\n",
    "test_df['is_answer'] = y_pred\n",
    "test_df.to_csv('result.txt', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(x_test)\n",
    "test_df['is_answer'] = y_pred\n",
    "test_df.to_csv('result_base.txt', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.predict(x_test[0:10],raw_score=True,pred_leaf=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deepctr.models import DeepFM,xDeepFM\n",
    "from deepctr.inputs import  SparseFeat, DenseFeat,get_feature_names\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from DeepCTR_master.deepctr.models import DeepFM,xDeepFM\n",
    "from DeepCTR_master.deepctr.inputs import  SparseFeat, DenseFeat,get_feature_names\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mDeepFM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlinear_feature_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdnn_feature_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_fm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdnn_hidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ml2_reg_linear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ml2_reg_embedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ml2_reg_dnn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minit_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdnn_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdnn_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mrelu\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7fb9717f2e18\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdnn_use_bn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Instantiates the DeepFM Network architecture.\n",
       "\n",
       ":param linear_feature_columns: An iterable containing all the features used by linear part of the model.\n",
       ":param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
       ":param embedding_size: positive integer,sparse feature embedding_size\n",
       ":param use_fm: bool,use FM part or not\n",
       ":param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN\n",
       ":param l2_reg_linear: float. L2 regularizer strength applied to linear part\n",
       ":param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
       ":param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
       ":param init_std: float,to use as the initialize std of embedding vector\n",
       ":param seed: integer ,to use as random seed.\n",
       ":param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
       ":param dnn_activation: Activation function to use in DNN\n",
       ":param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN\n",
       ":param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
       ":param device: str, ``\"cpu\"`` or ``\"cuda:0\"``\n",
       ":return: A PyTorch model instance.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/users/ydzhang/zhihu/zk/DeepCTR/deepctr_torch/models/deepfm.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DeepFM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepCTR.deepctr_torch.models import DeepFM,WDL,xDeepFM\n",
    "from DeepCTR.deepctr_torch.inputs import  SparseFeat, DenseFeat,get_feature_names\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = lgb.Booster(model_file='base_model.txt')\n",
    "train_gbdt_feat = boost.predict(x_train,pred_leaf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_gbdt_feat.npy',train_gbdt_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gbdt_feat = boost.predict(x_test,pred_leaf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = [str(i) for i in range(2000)]\n",
    "# sparse_features = []\n",
    "# dense_features = ['question_idcount', 'salt_value', 'user_idcount', 'answer_prob', 'invite_create_hour', 'question_answer_day', 'user_answer_nums', 'invite_create_day', 'answer_create_day_user', 'create_day']\n",
    "dense_pos = [train_secs.get_loc(sec) for sec in dense_features]\n",
    "dense_features = []\n",
    "# data_df[dense_features] = data_df[dense_features].fillna(0, )\n",
    "fixlen_feature_columns = [SparseFeat(feat, 64) for feat in sparse_features] + [DenseFeat(feat, 1,) for feat in dense_features]\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "feature_names = get_feature_names(\n",
    "    linear_feature_columns+ dnn_feature_columns)\n",
    "\n",
    "train_model_input={}\n",
    "for sec in sparse_features:\n",
    "    train_model_input[sec]=train_gbdt_feat[:,int(sec)]\n",
    "for i,sec in enumerate(dense_features):\n",
    "    train_model_input[sec]=x_train[:,dense_pos[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:26",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-084cddde7649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model = DeepFM(linear_feature_columns=linear_feature_columns, dnn_feature_columns=dnn_feature_columns,task='binary',\n\u001b[0;32m----> 2\u001b[0;31m                l2_reg_embedding=1e-5,device='cuda:1')\n\u001b[0m\u001b[1;32m      3\u001b[0m model.compile(\"adam\", \"binary_crossentropy\",\n\u001b[1;32m      4\u001b[0m               metrics=[\"binary_crossentropy\",\"accuracy\"],)\n",
      "\u001b[0;32m~/users/ydzhang/zhihu/zk/DeepCTR/deepctr_torch/models/deepfm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, linear_feature_columns, dnn_feature_columns, embedding_size, use_fm, dnn_hidden_units, l2_reg_linear, l2_reg_embedding, l2_reg_dnn, init_std, seed, dnn_dropout, dnn_activation, dnn_use_bn, task, device)\u001b[0m\n\u001b[1;32m     49\u001b[0m                                      \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                      \u001b[0mdnn_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdnn_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnn_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdnn_activation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                      task=task, device=device)\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_fm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_fm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/users/ydzhang/zhihu/zk/DeepCTR/deepctr_torch/models/basemodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, linear_feature_columns, dnn_feature_columns, embedding_size, dnn_hidden_units, l2_reg_linear, l2_reg_embedding, l2_reg_dnn, init_std, seed, dnn_dropout, dnn_activation, task, device)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m  \u001b[0;31m# device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:26"
     ]
    }
   ],
   "source": [
    "model = DeepFM(linear_feature_columns=linear_feature_columns, dnn_feature_columns=dnn_feature_columns,task='binary',\n",
    "               l2_reg_embedding=1e-5,device='cuda:1')\n",
    "model.compile(\"adam\", \"binary_crossentropy\",\n",
    "              metrics=[\"binary_crossentropy\",\"accuracy\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-b088256c03b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(train_model_input, y_train,\n\u001b[0;32m----> 2\u001b[0;31m           batch_size=256, epochs=15, validation_split=0.1, verbose=20)\n\u001b[0m",
      "\u001b[0;32m~/users/ydzhang/zhihu/zk/DeepCTR/deepctr_torch/models/basemodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, initial_epoch, validation_split, validation_data, shuffle)\u001b[0m\n\u001b[1;32m    186\u001b[0m         train_tensor_data = Data.TensorDataset(\n\u001b[1;32m    187\u001b[0m             torch.from_numpy(\n\u001b[0;32m--> 188\u001b[0;31m                 np.concatenate(x, axis=-1)),\n\u001b[0m\u001b[1;32m    189\u001b[0m             torch.from_numpy(y))\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_model_input, y_train,\n",
    "          batch_size=256, epochs=15, validation_split=0.1, verbose=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-30602697b5a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = pd.read_csv('data_df.csv',index_col=0)\n",
    "\n",
    "# category_df = member_df[['user_id', 'category_A', 'category_B','category_C', 'category_D', 'category_E','gender', 'visit_freq']]\n",
    "\n",
    "# category_df.columns = ['user_id','category_A_raw','category_B_raw','category_C_raw','category_D_raw','category_E_raw']\n",
    "# category_df.columns = [x+'_raw' if x !='user_id' else x for x in category_df.columns ]\n",
    "\n",
    "# data_df = pd.merge(data_df,category_df,on='user_id',how='left')\n",
    "drop_columns = ['is_answer', 'question_id', 'user_id', 'topic_id','follow_topics', 'interest_topics']\n",
    "train_secs = data_df.columns.drop(drop_columns)\n",
    "# category_columns = category_df.columns.drop('user_id')\n",
    "category_columns= []\n",
    "dense_features = data_df.columns.drop(category_columns).drop(drop_columns)\n",
    "sparse_features = category_columns\n",
    "target = ['is_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[sparse_features] = data_df[sparse_features].fillna('-1', )\n",
    "data_df[dense_features] = data_df[dense_features].fillna(0, )\n",
    "\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data_df[feat] = lbe.fit_transform(data_df[feat])\n",
    "\n",
    "# 2.count #unique features for each sparse field,and record dense feature field name\n",
    "\n",
    "fixlen_feature_columns = [SparseFeat(feat, data_df[feat].nunique())\n",
    "                          for feat in sparse_features] + [DenseFeat(feat, 1,)\n",
    "                                                          for feat in dense_features]\n",
    "\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "feature_names = get_feature_names(\n",
    "    linear_feature_columns+ dnn_feature_columns)\n",
    "\n",
    "trian_start = 3838; train_end = 3867\n",
    "train = data_df[train_secs.tolist()+['is_answer']][:train_df.shape[0]][train_df['invite_create_day']>train_end-7]\n",
    "\n",
    "train_model_input = {name:train[name] for name in feature_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dnn_feature_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9bcde617a5bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = WDL(linear_feature_columns=dnn_feature_columns, dnn_feature_columns=dnn_feature_columns, task='binary',\n\u001b[0m\u001b[1;32m      2\u001b[0m                l2_reg_embedding=1e-5, device='cuda:1')\n\u001b[1;32m      3\u001b[0m model.compile(\"adam\", \"binary_crossentropy\",\n\u001b[1;32m      4\u001b[0m               metrics=[\"binary_crossentropy\", \"auc\"],)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dnn_feature_columns' is not defined"
     ]
    }
   ],
   "source": [
    "model = WDL(linear_feature_columns=dnn_feature_columns, dnn_feature_columns=dnn_feature_columns, task='binary',\n",
    "               l2_reg_embedding=1e-5, device='cuda:1')\n",
    "model.compile(\"adam\", \"binary_crossentropy\",\n",
    "              metrics=[\"binary_crossentropy\", \"auc\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "Train on 2593669 samples, validate on 0 samples, 5066 steps per epoch\n",
      "Epoch 1/35\n",
      "100s - loss:  0.3925 - binary_crossentropy:  0.3925 - auc:  0.7277\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5d2775877814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(train_model_input, train[target].values,\n\u001b[0;32m----> 2\u001b[0;31m           batch_size=512, epochs=35, validation_split=0, verbose=2)\n\u001b[0m",
      "\u001b[0;32m~/users/ydzhang/zhihu/zk/DeepCTR/deepctr_torch/models/basemodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, initial_epoch, validation_split, validation_data, shuffle)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ydzhang/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/users/ydzhang/zhihu/zk/DeepCTR/deepctr_torch/models/wdl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     63\u001b[0m         sparse_embedding_list, dense_value_list = self.input_from_feature_columns(X, self.dnn_feature_columns,\n\u001b[1;32m     64\u001b[0m                                                                                   self.embedding_dict)\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_dnn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ydzhang/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/users/ydzhang/zhihu/zk/DeepCTR/deepctr_torch/models/basemodel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         dense_value_list = [X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]] for feat in\n\u001b[0;32m---> 59\u001b[0;31m                             self.dense_feature_columns]\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_embedding_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_value_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/users/ydzhang/zhihu/zk/DeepCTR/deepctr_torch/models/basemodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m             feat in self.sparse_feature_columns]\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         dense_value_list = [X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]] for feat in\n\u001b[0m\u001b[1;32m     59\u001b[0m                             self.dense_feature_columns]\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_model_input, train[target].values,\n",
    "          batch_size=512, epochs=35, validation_split=0, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepFM(linear_feature_columns=linear_feature_columns, dnn_feature_columns=dnn_feature_columns, task='binary',\n",
    "               l2_reg_embedding=1e-5, device='cuda:1')\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\",\n",
    "              metrics=[\"binary_crossentropy\", \"auc\"],)\n",
    "model.fit(train_model_input, train[target].values,\n",
    "          batch_size=256, epochs=50, validation_split=0, verbose=2)\n",
    "\n",
    "test = data_df[train_secs.tolist()+['is_answer']][train_df.shape[0]:].reset_index(drop=True)\n",
    "\n",
    "test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "pred_ans = model.predict(test_model_input, 256)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "boost = lgb.Booster(model_file='base_model.txt')\n",
    "\n",
    "train_gbdt_feat = boost.predict(x_train,pred_leaf=True)\n",
    "\n",
    "test_gbdt_feat = boost.predict(x_test,pred_leaf=True)\n",
    "\n",
    "y_pred = lr.predict_proba(test_gbdt_feat)\n",
    "\n",
    "y_pred = y_pred[:,1]\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss='log',tol=1e-4,penalty='l2',alpha=0.01,n_jobs=-1)\n",
    "\n",
    "sgd.partial_fit(train_gbdt_feat,y_train)\n",
    "import numpy as np\n",
    "train_gbdt_feat = np.load('train_gbdt_feat.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lm = LogisticRegression(penalty='l2',C=0.05,n_jobs=-1,verbose=20)\n",
    "\n",
    "sgd.partial_fit(train_gbdt_feat[0:10000],y_train[0:10000],classes=np.array([0., 1.]))\n",
    "\n",
    "np.unique(y_train[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:1000].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2593669, 2000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gbdt_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 10.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.05, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=20,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(train_gbdt_feat[-100000:],y_train[-100000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8243454106181634"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lm.predict_proba(train_gbdt_feat)\n",
    "roc_auc_score(y_train,y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['is_answer'] = warm_pred\n",
    "test_df.to_csv('result_lr.txt', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_lm =  LogisticRegression(penalty='l2',C=0.05,n_jobs=-1,verbose=20,warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats = train_gbdt_feat[0:1000000]\n",
    "train_feats_y = y_train[0:1000000]\n",
    "\n",
    "now_state = np.random.get_state()\n",
    "np.random.shuffle(train_feats)\n",
    "np.random.set_state(now_state)\n",
    "np.random.shuffle(train_feats_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  9.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  9.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  8.9min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  8.8min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 10.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 11.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 11.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  9.8min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  9.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  8.9min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  8.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  9.4min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  8.8min finished\n"
     ]
    }
   ],
   "source": [
    "step = 100000\n",
    "for i in range(0,train_gbdt_feat.shape[0],step):\n",
    "    warm_lm.fit(train_gbdt_feat[i:i+step],y_train[i:i+step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00025849]),\n",
       " array([[-0.00014527,  0.00015248, -0.00328656, ...,  0.00022596,\n",
       "          0.00245392,  0.00196918]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warm_lm.intercept_,warm_lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00027655]),\n",
       " array([[ 0.00726306,  0.00117516,  0.00672448, ...,  0.00335834,\n",
       "         -0.00269841, -0.00351842]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warm_lm.intercept_,warm_lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_gbdt_feat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-cc453c2afba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_gbdt_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_gbdt_feat' is not defined"
     ]
    }
   ],
   "source": [
    "lm_pred = lm.predict_proba(test_gbdt_feat)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_pred = warm_lm.predict_proba(test_gbdt_feat)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gbdt_feat = np.load('test_gbdt_feat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['is_answer'] = warm_pred\n",
    "test_df.to_csv('result_lr.txt', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8830620855937312"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_train[-100:],lm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.835652043580662"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_train,y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lm1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b269f732b4a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lm1.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lm1' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('lm_warm.pickle','wb') as f:\n",
    "    pickle.dump(warm_lm,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lm1.pickle','rb') as f:\n",
    "    lm1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lm1.predict_proba(train_gbdt_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['is_answer'] = y_pred[:,1]\n",
    "test_df.to_csv('result_lr.txt', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68189172, 0.31810828],\n",
       "       [0.99704143, 0.00295857],\n",
       "       [0.8947785 , 0.1052215 ],\n",
       "       ...,\n",
       "       [0.52821261, 0.47178739],\n",
       "       [0.96166116, 0.03833884],\n",
       "       [0.94026906, 0.05973094]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
