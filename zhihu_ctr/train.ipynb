{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, MultiLabelBinarizer, scale, minmax_scale\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import sparse\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import multiprocessing\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sc3/miniconda2/envs/ydzhang/lib/python3.6/site-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv('data_df_new.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_len(text):\n",
    "    if text=='-1':\n",
    "        return 0\n",
    "    return len(text.split(','))\n",
    "def get_day(str):\n",
    "    return int(str.split('-')[0][1:])\n",
    "def get_hour(str):\n",
    "    return int(str.split('-')[1][1:])\n",
    "def split_topic(str):\n",
    "    if str == '-1':\n",
    "        return []\n",
    "    topics = str.split(',')\n",
    "    for i in range(len(topics)):\n",
    "        topics[i] = int(topics[i][1:])\n",
    "    return topics\n",
    "\n",
    "def split_interest(str):\n",
    "    if(str)=='-1':\n",
    "        return {};\n",
    "    re = str.split(',')\n",
    "    red = {}\n",
    "    for i in re:\n",
    "        i = i.split(':')\n",
    "        val = float(i[1])\n",
    "        if(val==float('inf')):\n",
    "            val = 10\n",
    "        red[int(i[0][1:])] = val\n",
    "    return red\n",
    "\n",
    "def topic_sim(follow,interest,question):\n",
    "    sq = set(question); sf = set(follow) ;si = set(interest.keys())\n",
    "    sqsf = sq&sf ; sqsi = sq&si\n",
    "    sqf = sq | sf; sqi = sq | si\n",
    "    sqfi = sqf | sqi\n",
    "    val = 0\n",
    "    max_in = 0\n",
    "    for t in sqsi:\n",
    "        val += interest[t]\n",
    "        if interest[t] > max_in:\n",
    "            max_in = interest[t]\n",
    "    return np.array([val,max_in,len(sqsf),len(sqsi),len(sqf),len(sqi),len(sqfi)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '../data/zhihu competition/'\n",
    "# member_df = pd.read_csv(os.path.join(data_dir, 'member_info_0926.txt'), header=None, sep='\\t')\n",
    "# member_df.columns = ['user_id', 'gender', 'keywords', 'creation_num_level', 'creation_heat_level', 'register_type', 'register_platform',\n",
    "#                'visit_freq', 'binary_A', 'binary_B', 'binary_C', 'binary_D', 'binary_E', 'category_A', 'category_B', 'category_C', \n",
    "#                'category_D', 'category_E', 'salt_value', 'follow_topics', 'interest_topics']\n",
    "# member_df.to_csv('member_info.csv')\n",
    "member_df = pd.read_csv('member_info.csv',index_col=0,header=0)\n",
    "encoder = {}\n",
    "encoder['gender'] = LabelEncoder()\n",
    "member_df['gender_label'] = encoder['gender'].fit_transform(member_df['gender'])\n",
    "\n",
    "visit_label = dict([['unknown',-1],['new',0],['monthly',1],['daily',2],['weekly',3]])\n",
    "member_df['visit_freq_label'] = member_df['visit_freq'].apply(lambda x:visit_label[x])\n",
    "\n",
    "multi_sectors = ['category_A','category_B','category_C','category_D','category_E']\n",
    "for sec in multi_sectors:\n",
    "    encoder[sec+'_label'] = LabelEncoder()\n",
    "    member_df[sec + '_label'] = encoder[sec+'_label'].fit_transform(member_df[sec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_columns = ['question_id','create_time', 'title_SW', 'title_W','describe_SW','describe_W','topic_id']\n",
    "# question_df = pd.read_csv(os.path.join(data_dir, 'question_info_0926.txt'),\n",
    "#                           nrows = None,sep= '\\t',names = question_columns)\n",
    "# question_df.to_csv('question_info.csv')\n",
    "question_df = pd.read_csv('question_info.csv',index_col=0)\n",
    "question_df['create_day'] = question_df['create_time'].apply(get_day)\n",
    "question_df['create_hour'] = question_df['create_time'].apply(get_hour)\n",
    "question_df['title_len'] = question_df['title_SW'].apply(count_len)\n",
    "question_df['describe_len'] = question_df['describe_SW'].apply(count_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = pd.merge(data_df,member_df[['user_id','user_id_count_userdf']],on = 'user_id',how='left')\n",
    "# data_df = pd.merge(data_df,question_df[['question_id','question_id_count_questiondf']], on = 'question_id', how = 'left')\n",
    "# for sec in ['user_id_count_userdf','question_id_count_questiondf']:\n",
    "#     data_df[sec] = minmax_scale(data_df[sec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '../data/zhihu competition/'\n",
    "# ans_columns = ['answer_id', 'question_id', 'user_id', 'create_time', 'answer_SW', 'answer_W',\n",
    "#                  'is_good', 'is_recommend', 'is_include', 'has_picture', 'has_video', 'word_count', 'num_agree','num_cancel_angree',\n",
    "#                  'num_comment', 'num_favorite', 'num_thanks', 'num_report', 'num_useless', 'num_oppose']\n",
    "# answer_df = pd.read_csv(os.path.join(data_dir, 'answer_info_0926.txt'), sep= '\\t', nrows = None, names = ans_columns)\n",
    "# answer_df.to_csv('answer_info.csv')\n",
    "answer_df = pd.read_csv('answer_info.csv',index_col=0)\n",
    "answer_df['answer_create_day'] = answer_df['create_time'].apply(get_day)\n",
    "answer_df['answer_create_hour'] = answer_df['create_time'].apply(get_hour)\n",
    "answer_df = pd.merge(answer_df,question_df[['question_id','title_len','describe_len']],how='left',on='question_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trian_start = 3838; train_end = 3867\n",
    "test_start = 3868; test_end = 3874\n",
    "ans_start = 3807; ans_end = 3867\n",
    "ques_start = 753;qus_end = 3874 # question 暂时不用处理\n",
    "ans_numeric_secs = ['answer_create_day','is_good','has_picture', 'has_video', 'word_count', 'num_agree','num_cancel_angree','num_comment', 'num_favorite', 'num_thanks', 'num_report', 'num_useless', 'num_oppose']\n",
    "# ans_numeric_secs = ['word_count', 'num_agree', 'answer_create_day']\n",
    "def get_ans_data(answer_df):\n",
    "    member_ans_df = answer_df[['user_id','title_len','describe_len']+ans_numeric_secs].groupby('user_id').mean()\n",
    "    member_ans_df.reset_index(inplace=True)\n",
    "    member_ans_df['user_answer_nums'] = member_ans_df['user_id'].map(answer_df['user_id'].value_counts())\n",
    "    member_ans_df.columns = [i+'_user' if i[0:4]!= 'user' else i for i in member_ans_df.columns ]\n",
    "\n",
    "    question_ans_df = answer_df[['question_id']+ans_numeric_secs].groupby('question_id').mean()\n",
    "    question_ans_df.reset_index(inplace=True)\n",
    "    question_ans_df['question_answer_nums'] = question_ans_df['question_id'].map(answer_df['question_id'].value_counts())\n",
    "    question_ans_df.columns = [i+'_question' if i[0:8]!= 'question' else i for i in question_ans_df.columns ]\n",
    "    return member_ans_df,question_ans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA数据未处理\n",
    "test_member_ans_df,test_question_ans_df = get_ans_data(answer_df)\n",
    "train_member_ans_df,train_question_ans_df = get_ans_data(answer_df[answer_df['answer_create_day']<train_end-7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(os.path.join(data_dir, 'invite_info_0926.txt'), header = None, names = ['question_id', 'user_id', 'create_time','is_answer'],\n",
    "#                 sep= '\\t')\n",
    "# train_df.to_csv('invite_info.csv')\n",
    "# test_df = pd.read_csv(os.path.join(data_dir, 'invite_info_evaluate_1_0926.txt'), header = None, names = ['question_id', 'user_id', 'create_time'],\n",
    "#                 sep= '\\t')\n",
    "# test_df.to_csv('evalute.csv')\n",
    "train_df =pd.read_csv('invite_info.csv',index_col=0)\n",
    "test_df =pd.read_csv('evalute.csv',index_col=0)\n",
    "train_df['invite_create_day'] = train_df['create_time'].apply(get_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ans_df = pd.merge(train_df,train_member_ans_df,how='left', on='user_id')\n",
    "train_ans_df = pd.merge(train_ans_df,train_question_ans_df,how='left', on='question_id')\n",
    "test_ans_df = pd.merge(test_df,test_member_ans_df,how='left', on='user_id')\n",
    "test_ans_df = pd.merge(test_ans_df,test_question_ans_df,how='left', on='question_id')\n",
    "\n",
    "train_df['invite_create_day'] = train_ans_df['create_time'].apply(get_day)\n",
    "test_ans_df['invite_create_day'] = test_df['create_time'].apply(get_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "invite_df = pd.concat([train_ans_df,test_ans_df],axis=0,sort=True)\n",
    "invite_df['invite_create_day'] = invite_df['create_time'].apply(get_day)\n",
    "invite_df['invite_create_hour'] = invite_df['create_time'].apply(get_hour)\n",
    "invite_df.drop(columns=['create_time'],inplace=True)\n",
    "\n",
    "member_data = member_df[['user_id','salt_value','gender_label', 'visit_freq_label', 'binary_A', 'binary_B', 'binary_C',\n",
    "       'binary_D', 'binary_E', 'category_A_label', 'category_B_label','category_C_label', 'category_D_label', 'category_E_label',\n",
    "                        'follow_topics', 'interest_topics']]\n",
    "question_data = question_df[['question_id','create_day','create_hour','title_len','describe_len','topic_id']]\n",
    "\n",
    "#合并数据\n",
    "data_df = pd.merge(invite_df,question_data,on='question_id',how='left')\n",
    "data_df = pd.merge(data_df,member_data,on='user_id',how='left')\n",
    "# 处理id\n",
    "for sec in ['question_id', 'user_id']:\n",
    "    encoder[sec+'_label'] = LabelEncoder()\n",
    "    data_df[sec + '_label'] = encoder[sec+'_label'].fit_transform(data_df[sec])\n",
    "\n",
    "#邀请和问题创建时间的间隔\n",
    "data_df['question_answer_day'] = data_df['invite_create_day'] - data_df['create_day']  \n",
    "    \n",
    "#统计频数\n",
    "for feat_name in [ 'category_A_label', 'category_B_label','category_C_label', 'category_D_label', 'category_E_label','gender_label', 'visit_freq_label', 'binary_A', 'binary_B', 'binary_C',\n",
    "       'binary_D', 'binary_E','question_id', 'user_id']:\n",
    "    sec = feat_name+'count'\n",
    "    data_df[sec] = data_df[feat_name].map(data_df[feat_name].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_topics = question_df['topic_id'].apply(split_topic).values.tolist()\n",
    "# question_topics = data_df['topic_id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_topics = member_df['follow_topics'].apply(split_topic).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_matrix = np.zeros([100000,100000],dtype = 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topics in question_topics:\n",
    "    topics = [x-1 for x in topics]\n",
    "    for x in topics:\n",
    "        topics_matrix[x][topics] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_matrix = topics_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topics in follow_topics:\n",
    "    topics = [x-1 for x in topics]\n",
    "    for x in topics:\n",
    "        topics_matrix[x][topics] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100, n_iter=5, random_state=42,ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_svd_vecs = svd.fit_transform(topics_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    topics_matrix[i][i] = int(topics_matrix[i][i]/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 100)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_svd_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('topic_svd_vecs.npy',topic_svd_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_svd_vecs = np.vstack([np.zeros(100),topic_svd_vecs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sc3/miniconda2/envs/ydzhang/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: Mean of empty slice.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/sc3/miniconda2/envs/ydzhang/lib/python3.6/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    }
   ],
   "source": [
    "question_svd_vecs = list(map(lambda x:topic_svd_vecs[x].mean(axis=0),question_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sc3/miniconda2/envs/ydzhang/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: Mean of empty slice.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "user_svd_vecs = list(map(lambda x:topic_svd_vecs[x].mean(axis=0),follow_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['topic_svd_dis'] = np.sum(np.square(np.array(user_svd_vecs)-np.array(question_svd_vecs)),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['topic_svd_dis'] = minmax_scale(data_df['topic_svd_dis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_topics = data_df['topic_id'].apply(split_topic).values.tolist()\n",
    "# question_topics = data_df['topic_id'].values.tolist()\n",
    "\n",
    "follow_topics = data_df['follow_topics'].apply(split_topic).values.tolist()\n",
    "# follow_topics = data_df['follow_topics'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_topics = data_df['interest_topics'].apply(split_interest).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于问题和用户协同过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于问题词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_index_df = pd.read_csv('question_index.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_vecs = np.load('question_vecs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids_dicts = {}\n",
    "for uid in answer_df['user_id'].unique():\n",
    "    qids_dicts[uid] = que_filter_df.get_group(uid).values[:,1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ques_vecs(items):\n",
    "    qid,uid = items\n",
    "    re =  np.array([np.nan,np.nan,np.nan,np.nan,np.nan,np.nan])\n",
    "    if uid not in uids:\n",
    "        return re\n",
    "    else:\n",
    "        q_vecs = ques_vecs[ques_index_df.loc[qid,'index']]\n",
    "        qids = qids_dicts[uid]\n",
    "        try:\n",
    "            qids.remove(qid)\n",
    "        except:\n",
    "            pass\n",
    "        if(len(qids)==0):\n",
    "            return re\n",
    "        q_indexs = ques_index_df.loc[qids,'index'].values\n",
    "        his_vecs = ques_vecs[q_indexs]\n",
    "        val = his_vecs - q_vecs\n",
    "        val = np.sqrt(np.square(val).sum(axis=2))\n",
    "        re[0:3] = val.mean(axis=0)\n",
    "        re[3:6] = val.min(axis=0)\n",
    "    return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "que_filter_df = answer_df[['user_id','question_id']].groupby('user_id')\n",
    "uids = answer_df['user_id'].unique().tolist()\n",
    "all_id = data_df[['question_id','user_id']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_ques_vecs(all_id[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_train.npy',y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()-5\n",
    "pool = multiprocessing.Pool(processes=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "ques_vec_dis = pool.map(compare_ques_vecs,all_id)\n",
    "end = time.time()\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_vecs_secs = ['title_SW_vec_his_mean', 'title_W_vec_his_mean','describe_W_vec_his_mean','title_SW_vec_his_max', 'title_W_vec_his_max','describe_W_vec_his_max']\n",
    "data_df[ques_vecs_secs] = quesVecDis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sec in ques_vecs_secs:\n",
    "    data_df[sec] = minmax_scale(data_df[sec])\n",
    "#     data_df[sec] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quesVecDis = [x[0] if type(i)==tuple else x for x in ques_vec_dis[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(quesVecDis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('data_df_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[ques_vecs_secs].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question_topics_dict = {}\n",
    "for _,items in question_df[['question_id','topic_id']].iterrows():\n",
    "     question_topics_dict[items['question_id']] = split_topic(items['topic_id'])\n",
    "\n",
    "def jaccard_dis(set1,set2):\n",
    "    set1 = set(set1); set2 = set(set2)\n",
    "    return (len(set1&set2))/(len(set1|set2))\n",
    "\n",
    "def compare_ques_his(items):\n",
    "    qid,uid = items\n",
    "    if uid not in uids:\n",
    "        re =  np.nan,np.nan\n",
    "    else:\n",
    "        qids = que_filter_df.get_group(uid).values[:,1].tolist()\n",
    "        re = []\n",
    "        set1 = question_topics_dict[qid]\n",
    "        if(len(set1)==0):\n",
    "            return np.nan,np.nan\n",
    "        for qid2 in qids:\n",
    "            if(qid==qid2):\n",
    "                continue\n",
    "            set2 = question_topics_dict[qid2]\n",
    "            if(len(set2)==0):\n",
    "                val = np.nan\n",
    "            else:\n",
    "                val = jaccard_dis(set1,set2)\n",
    "            re.append(val)         \n",
    "        if re==[]:\n",
    "            re = np.nan,np.nan\n",
    "        else:\n",
    "            re = np.array(re)\n",
    "            re = (re.mean(),re.min())\n",
    "    return re\n",
    "\n",
    "que_filter_df = answer_df[['user_id','question_id']].groupby('user_id')\n",
    "uids = answer_df['user_id'].unique().tolist()\n",
    "compare_re = []\n",
    "all_id = data_df[['question_id','user_id']].values.tolist()\n",
    "\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()-5\n",
    "pool = multiprocessing.Pool(processes=cores)\n",
    "\n",
    "start = time.time()\n",
    "questions_dis = pool.map(compare_ques_his,all_id)\n",
    "end = time.time()\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[['his_quesiont_dis_mean', 'his_quesiont_dis_max']] = questions_dis\n",
    "for sec in ['his_quesiont_dis_mean','his_quesiont_dis_max']:\n",
    "    data_df[sec] = minmax_scale(data_df[sec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# member_df.shape,question_df.shape\n",
    "\n",
    "# user_topics_dict = {}\n",
    "# for _,items in member_df[['user_id','follow_topics']].iterrows():\n",
    "#      user_topics_dict[items['user_id']] = split_topic(items['follow_topics'])\n",
    "\n",
    "# uids_dicts = {}\n",
    "# for uid in answer_df['question_id'].unique():\n",
    "#     uids_dicts[qid] = que_filter_df.get_group(qid).values[:,1].tolist()\n",
    "\n",
    "# def jaccard_dis(set1,set2):\n",
    "#     set1 = set(set1); set2 = set(set2)\n",
    "#     return (len(set1&set2))/(len(set1|set2))\n",
    "\n",
    "# def compare_users_his(items):\n",
    "#     qid,uid = items\n",
    "# #     if qid not in qids:\n",
    "#         re =  np.nan,np.nan\n",
    "#     else:\n",
    "#         uids = user_filter_df.get_group(qid).values[:,0].tolist()\n",
    "#         re = []\n",
    "#         set1 = user_topics_dict[uid]\n",
    "#         if(len(set1)==0):\n",
    "#             return np.nan,np.nan\n",
    "#         for uid2 in uids:\n",
    "#             if(uid==uid2):\n",
    "#                 continue\n",
    "#             set2 = user_topics_dict[uid2]\n",
    "#             if(len(set2)==0):\n",
    "#                 val = np.nan\n",
    "#             else:\n",
    "#                 val = jaccard_dis(set1,set2)\n",
    "#             re.append(val)         \n",
    "#         if re==[]:\n",
    "#             re = np.nan,np.nan\n",
    "#         else:\n",
    "#             re = np.array(re)\n",
    "#             re = (re.mean(),re.min())\n",
    "#     return re\n",
    "\n",
    "# user_filter_df = answer_df[['user_id','question_id']].groupby('question_id')\n",
    "# qids = answer_df['question_id'].unique().tolist()\n",
    "# compare_re = []\n",
    "# all_id = data_df[['question_id','user_id']].values.tolist()\n",
    "\n",
    "# import time\n",
    "\n",
    "# import multiprocessing\n",
    "# cores = multiprocessing.cpu_count()-5\n",
    "# pool = multiprocessing.Pool(processes=cores)\n",
    "\n",
    "# start = time.time()\n",
    "# users_dis = pool.map(compare_users_his,all_id[0:10000])\n",
    "# end = time.time()\n",
    "# print(time.time()-start)\n",
    "\n",
    "# print(end-start)\n",
    "\n",
    "# data_df[['his_user_dis_mean', 'his_user_dis_max']] = users_dis\n",
    "# for sec in ['his_user_dis_mean','his_user_dis_max']:\n",
    "#     data_df[sec] = minmax_scale(data_df[sec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user question 的共现矩阵\n",
    "# question topic_id 的共现矩阵\n",
    "\n",
    "# topic 处理 \n",
    "# 同一个问题下的所有topic，算一类，看能划分出多少并查集\n",
    "# 计算每个用户关注的话题，兴趣话题 问题的话题的topic的平均值，求相似度 注意用余弦距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '../data/zhihu competition/'\n",
    "# with open(data_dir + 'topic_vectors_64d.txt') as f:\n",
    "#     lines = f.readlines()\n",
    "# lines = [i.strip().split() for i in lines]\n",
    "# topic_vecs = np.array(lines)\n",
    "# topic_vecs = topic_vecs[:,1:].astype('float32')\n",
    "# topic_vecs = np.vstack([np.zeros([1,64]),topic_vecs])\n",
    "# np.save('topic_vecs.npy',topic_vecs)\n",
    "\n",
    "topic_vecs = np.load('topic_vecs.npy')\n",
    "\n",
    "question_vecs = list(map(lambda x:topic_vecs[x].mean(axis=0),question_topics))\n",
    "user_vecs = list(map(lambda x:topic_vecs[x].mean(axis=0),follow_topics))\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "data_df['topic_cosdis'] = list(map(cosine,user_vecs,question_vecs))\n",
    "data_df['topic_eucliddis'] = np.sum(np.square(np.array(user_vecs)-np.array(question_vecs)),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del user_vecs, question_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_secs = ['total_interest','max_interest','que&fol','que&inter','que|fol','que|inter','que|fol|inter']\n",
    "topic_inter = np.zeros([data_df.shape[0],len(topic_secs)])\n",
    "for i in range(data_df.shape[0]):\n",
    "    topic_inter[i] = topic_sim(follow_topics[i],interest_topics[i],question_topics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['qt_len'] = [len(i) for i in question_topics]\n",
    "data_df['follow_len'] = [len(i) for i in follow_topics]\n",
    "for i in range(len(topic_secs)):\n",
    "    data_df[topic_secs[i]] = topic_inter[:,i]\n",
    "data_df['topic_dis'] = data_df['que&fol'] / data_df['que|fol']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重新计算天数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_member_ans_df,test_question_ans_df = get_ans_data(answer_df)\n",
    "dd = 14\n",
    "train_member_ans_df,train_question_ans_df = get_ans_data(answer_df[answer_df['answer_create_day']<=train_end-dd])\n",
    "\n",
    "train_ans_df = pd.merge(train_df,train_member_ans_df,how='left', on='user_id')\n",
    "train_ans_df = pd.merge(train_ans_df,train_question_ans_df,how='left', on='question_id')\n",
    "\n",
    "update_columns = train_ans_df.columns.drop(['question_id','user_id','is_answer','create_time'])\n",
    "\n",
    "data_df.loc[0:train_df.shape[0]-1,update_columns] = train_ans_df[update_columns] #.loc()左闭右闭\n",
    "for sec in update_columns:\n",
    "    data_df.loc[0:train_df.shape[0]-1,sec] = minmax_scale(data_df[sec][:train_df.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = {}\n",
    "train_answer_praba = train_df[train_df['invite_create_day']<train_end-dd][['user_id','is_answer']].groupby('user_id').mean().to_dict()['is_answer']\n",
    "prob['train_answer_prob'] = train_ans_df['user_id'].map(train_answer_praba)\n",
    "train_answer_praba = train_df[train_df['invite_create_day']<train_end-dd][['question_id','is_answer']].groupby('question_id').mean().to_dict()['is_answer']\n",
    "prob['train_question_prob'] = train_ans_df['question_id'].map(train_answer_praba)\n",
    "data_df.loc[0:train_df.shape[0]-1,'answer_prob'] = prob['train_answer_prob'].values\n",
    "data_df.loc[0:train_df.shape[0]-1,'question_prob'] = prob['train_question_prob'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answer_praba = train_df[['user_id','is_answer']].groupby('user_id').mean().to_dict()['is_answer']\n",
    "prob['test_answer_prob'] = test_ans_df['user_id'].map(test_answer_praba)\n",
    "test_answer_praba = train_df[['question_id','is_answer']].groupby('question_id').mean().to_dict()['is_answer']\n",
    "prob['test_question_prob'] = test_ans_df['question_id'].map(test_answer_praba)\n",
    "\n",
    "data_df.loc[train_df.shape[0]:,'answer_prob'] = prob['test_answer_prob'].values\n",
    "data_df.loc[train_df.shape[0]:,'question_prob'] = prob['test_question_prob'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.to_csv('data_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_secs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-676e157a2fa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_secs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'invite_create_day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mtrain_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'invite_create_day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mtrain_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'invite_create_day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mtrain_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'invite_create_day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mtrain_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_secs' is not defined"
     ]
    }
   ],
   "source": [
    "x_train = data_df[train_secs][:train_df.shape[0]][(train_df['invite_create_day']>train_end-dd) & (train_df['invite_create_day']<=train_end-dd+7)].values\n",
    "y_train = data_df['is_answer'][:train_df.shape[0]][(train_df['invite_create_day']>train_end-dd) & (train_df['invite_create_day']<=train_end-dd+7)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1847807, 88)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.vstack([x_train7,x_train14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.hstack([y_train7,y_train14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4a8403f101dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_train.shape,x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc[train_df.shape[0]:,'answer_prob'].shape,prob['test_answer_prob'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['is_answer', 'question_id', 'user_id', 'topic_id','follow_topics', 'interest_topics']\n",
    "train_secs = data_df.columns.drop(drop_columns)\n",
    "category_columns = ['user_id_label','question_id_label','category_A_label', 'category_B_label','category_C_label', 'category_D_label', 'category_E_label','gender_label', 'visit_freq_label', 'binary_A', 'binary_B', 'binary_C','binary_D', 'binary_E']\n",
    "scale_columns = train_secs.drop(category_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 归一化数据\n",
    "for sec in train_secs:\n",
    "    data_df[sec] = minmax_scale(data_df[sec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_secs = data_df.columns.drop(drop_columns)\n",
    "ques_vecs_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_df[train_secs][:train_df.shape[0]][train_df['invite_create_day']>train_end-7].values\n",
    "y_train = data_df[:train_df.shape[0]]['is_answer'][train_df['invite_create_day']>train_end-7].values\n",
    "x_test = data_df[train_secs][train_df.shape[0]:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10630845, 94), (4837532, 88), 88)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape,x_train.shape,len(train_secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's auc: 0.779217\ttraining's binary_logloss: 0.438097\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\ttraining's auc: 0.78682\ttraining's binary_logloss: 0.436541\n",
      "[3]\ttraining's auc: 0.786009\ttraining's binary_logloss: 0.435046\n",
      "[4]\ttraining's auc: 0.78548\ttraining's binary_logloss: 0.433587\n",
      "[5]\ttraining's auc: 0.786686\ttraining's binary_logloss: 0.432176\n",
      "[6]\ttraining's auc: 0.788371\ttraining's binary_logloss: 0.430783\n",
      "[7]\ttraining's auc: 0.789758\ttraining's binary_logloss: 0.429441\n",
      "[8]\ttraining's auc: 0.791125\ttraining's binary_logloss: 0.428129\n",
      "[9]\ttraining's auc: 0.791888\ttraining's binary_logloss: 0.42685\n",
      "[10]\ttraining's auc: 0.792178\ttraining's binary_logloss: 0.425605\n",
      "[11]\ttraining's auc: 0.79283\ttraining's binary_logloss: 0.424378\n",
      "[12]\ttraining's auc: 0.793235\ttraining's binary_logloss: 0.423192\n",
      "[13]\ttraining's auc: 0.79368\ttraining's binary_logloss: 0.422028\n",
      "[14]\ttraining's auc: 0.793712\ttraining's binary_logloss: 0.42089\n",
      "[15]\ttraining's auc: 0.793747\ttraining's binary_logloss: 0.419785\n",
      "[16]\ttraining's auc: 0.793677\ttraining's binary_logloss: 0.418703\n",
      "[17]\ttraining's auc: 0.793846\ttraining's binary_logloss: 0.417648\n",
      "[18]\ttraining's auc: 0.794045\ttraining's binary_logloss: 0.416613\n",
      "[19]\ttraining's auc: 0.794619\ttraining's binary_logloss: 0.41557\n",
      "[20]\ttraining's auc: 0.795048\ttraining's binary_logloss: 0.414555\n",
      "[21]\ttraining's auc: 0.795677\ttraining's binary_logloss: 0.413547\n",
      "[22]\ttraining's auc: 0.796049\ttraining's binary_logloss: 0.41258\n",
      "[23]\ttraining's auc: 0.796174\ttraining's binary_logloss: 0.411647\n",
      "[24]\ttraining's auc: 0.796278\ttraining's binary_logloss: 0.410721\n",
      "[25]\ttraining's auc: 0.796484\ttraining's binary_logloss: 0.409815\n",
      "[26]\ttraining's auc: 0.796741\ttraining's binary_logloss: 0.408922\n",
      "[27]\ttraining's auc: 0.797136\ttraining's binary_logloss: 0.408019\n",
      "[28]\ttraining's auc: 0.797276\ttraining's binary_logloss: 0.407165\n",
      "[29]\ttraining's auc: 0.797456\ttraining's binary_logloss: 0.406309\n",
      "[30]\ttraining's auc: 0.797599\ttraining's binary_logloss: 0.405491\n",
      "[31]\ttraining's auc: 0.798016\ttraining's binary_logloss: 0.404654\n",
      "[32]\ttraining's auc: 0.798232\ttraining's binary_logloss: 0.403847\n",
      "[33]\ttraining's auc: 0.798541\ttraining's binary_logloss: 0.403035\n",
      "[34]\ttraining's auc: 0.798689\ttraining's binary_logloss: 0.40227\n",
      "[35]\ttraining's auc: 0.798955\ttraining's binary_logloss: 0.401488\n",
      "[36]\ttraining's auc: 0.799096\ttraining's binary_logloss: 0.400752\n",
      "[37]\ttraining's auc: 0.799327\ttraining's binary_logloss: 0.400011\n",
      "[38]\ttraining's auc: 0.799715\ttraining's binary_logloss: 0.399267\n",
      "[39]\ttraining's auc: 0.799869\ttraining's binary_logloss: 0.398561\n",
      "[40]\ttraining's auc: 0.800136\ttraining's binary_logloss: 0.397851\n",
      "[41]\ttraining's auc: 0.800272\ttraining's binary_logloss: 0.397176\n",
      "[42]\ttraining's auc: 0.800526\ttraining's binary_logloss: 0.396486\n",
      "[43]\ttraining's auc: 0.800618\ttraining's binary_logloss: 0.395829\n",
      "[44]\ttraining's auc: 0.800836\ttraining's binary_logloss: 0.395161\n",
      "[45]\ttraining's auc: 0.800973\ttraining's binary_logloss: 0.394531\n",
      "[46]\ttraining's auc: 0.801111\ttraining's binary_logloss: 0.393901\n",
      "[47]\ttraining's auc: 0.801254\ttraining's binary_logloss: 0.39328\n",
      "[48]\ttraining's auc: 0.801393\ttraining's binary_logloss: 0.392665\n",
      "[49]\ttraining's auc: 0.801684\ttraining's binary_logloss: 0.392032\n",
      "[50]\ttraining's auc: 0.801801\ttraining's binary_logloss: 0.391437\n",
      "[51]\ttraining's auc: 0.801898\ttraining's binary_logloss: 0.390855\n",
      "[52]\ttraining's auc: 0.802193\ttraining's binary_logloss: 0.390251\n",
      "[53]\ttraining's auc: 0.802356\ttraining's binary_logloss: 0.389688\n",
      "[54]\ttraining's auc: 0.802647\ttraining's binary_logloss: 0.389108\n",
      "[55]\ttraining's auc: 0.802754\ttraining's binary_logloss: 0.388564\n",
      "[56]\ttraining's auc: 0.802884\ttraining's binary_logloss: 0.388022\n",
      "[57]\ttraining's auc: 0.803144\ttraining's binary_logloss: 0.387463\n",
      "[58]\ttraining's auc: 0.803312\ttraining's binary_logloss: 0.386941\n",
      "[59]\ttraining's auc: 0.803442\ttraining's binary_logloss: 0.386419\n",
      "[60]\ttraining's auc: 0.803574\ttraining's binary_logloss: 0.385914\n",
      "[61]\ttraining's auc: 0.803787\ttraining's binary_logloss: 0.385384\n",
      "[62]\ttraining's auc: 0.803918\ttraining's binary_logloss: 0.38489\n",
      "[63]\ttraining's auc: 0.804064\ttraining's binary_logloss: 0.384402\n",
      "[64]\ttraining's auc: 0.80418\ttraining's binary_logloss: 0.38392\n",
      "[65]\ttraining's auc: 0.804343\ttraining's binary_logloss: 0.38345\n",
      "[66]\ttraining's auc: 0.804537\ttraining's binary_logloss: 0.382952\n",
      "[67]\ttraining's auc: 0.804654\ttraining's binary_logloss: 0.382494\n",
      "[68]\ttraining's auc: 0.80487\ttraining's binary_logloss: 0.382015\n",
      "[69]\ttraining's auc: 0.804994\ttraining's binary_logloss: 0.381555\n",
      "[70]\ttraining's auc: 0.805207\ttraining's binary_logloss: 0.381091\n",
      "[71]\ttraining's auc: 0.805345\ttraining's binary_logloss: 0.380661\n",
      "[72]\ttraining's auc: 0.80551\ttraining's binary_logloss: 0.380226\n",
      "[73]\ttraining's auc: 0.805653\ttraining's binary_logloss: 0.379805\n",
      "[74]\ttraining's auc: 0.805848\ttraining's binary_logloss: 0.379363\n",
      "[75]\ttraining's auc: 0.806031\ttraining's binary_logloss: 0.37893\n",
      "[76]\ttraining's auc: 0.806154\ttraining's binary_logloss: 0.378528\n",
      "[77]\ttraining's auc: 0.806277\ttraining's binary_logloss: 0.378129\n",
      "[78]\ttraining's auc: 0.806455\ttraining's binary_logloss: 0.377715\n",
      "[79]\ttraining's auc: 0.806606\ttraining's binary_logloss: 0.377326\n",
      "[80]\ttraining's auc: 0.806758\ttraining's binary_logloss: 0.376925\n",
      "[81]\ttraining's auc: 0.806904\ttraining's binary_logloss: 0.376528\n",
      "[82]\ttraining's auc: 0.807051\ttraining's binary_logloss: 0.376136\n",
      "[83]\ttraining's auc: 0.807193\ttraining's binary_logloss: 0.375765\n",
      "[84]\ttraining's auc: 0.807325\ttraining's binary_logloss: 0.375389\n",
      "[85]\ttraining's auc: 0.807469\ttraining's binary_logloss: 0.375022\n",
      "[86]\ttraining's auc: 0.807642\ttraining's binary_logloss: 0.374655\n",
      "[87]\ttraining's auc: 0.807798\ttraining's binary_logloss: 0.374289\n",
      "[88]\ttraining's auc: 0.807895\ttraining's binary_logloss: 0.373942\n",
      "[89]\ttraining's auc: 0.808048\ttraining's binary_logloss: 0.37359\n",
      "[90]\ttraining's auc: 0.808178\ttraining's binary_logloss: 0.373241\n",
      "[91]\ttraining's auc: 0.808324\ttraining's binary_logloss: 0.372895\n",
      "[92]\ttraining's auc: 0.808443\ttraining's binary_logloss: 0.372553\n",
      "[93]\ttraining's auc: 0.808602\ttraining's binary_logloss: 0.372216\n",
      "[94]\ttraining's auc: 0.808761\ttraining's binary_logloss: 0.371865\n",
      "[95]\ttraining's auc: 0.808934\ttraining's binary_logloss: 0.371533\n",
      "[96]\ttraining's auc: 0.809062\ttraining's binary_logloss: 0.371193\n",
      "[97]\ttraining's auc: 0.80922\ttraining's binary_logloss: 0.370874\n",
      "[98]\ttraining's auc: 0.809365\ttraining's binary_logloss: 0.370561\n",
      "[99]\ttraining's auc: 0.809496\ttraining's binary_logloss: 0.370251\n",
      "[100]\ttraining's auc: 0.809641\ttraining's binary_logloss: 0.369937\n",
      "[101]\ttraining's auc: 0.809751\ttraining's binary_logloss: 0.369624\n",
      "[102]\ttraining's auc: 0.809899\ttraining's binary_logloss: 0.369322\n",
      "[103]\ttraining's auc: 0.810073\ttraining's binary_logloss: 0.369011\n",
      "[104]\ttraining's auc: 0.81018\ttraining's binary_logloss: 0.368717\n",
      "[105]\ttraining's auc: 0.810334\ttraining's binary_logloss: 0.368409\n",
      "[106]\ttraining's auc: 0.810449\ttraining's binary_logloss: 0.368121\n",
      "[107]\ttraining's auc: 0.810611\ttraining's binary_logloss: 0.367824\n",
      "[108]\ttraining's auc: 0.810721\ttraining's binary_logloss: 0.367543\n",
      "[109]\ttraining's auc: 0.81088\ttraining's binary_logloss: 0.367252\n",
      "[110]\ttraining's auc: 0.811039\ttraining's binary_logloss: 0.36697\n",
      "[111]\ttraining's auc: 0.811155\ttraining's binary_logloss: 0.366689\n",
      "[112]\ttraining's auc: 0.811288\ttraining's binary_logloss: 0.366415\n",
      "[113]\ttraining's auc: 0.811398\ttraining's binary_logloss: 0.366145\n",
      "[114]\ttraining's auc: 0.811521\ttraining's binary_logloss: 0.365886\n",
      "[115]\ttraining's auc: 0.811705\ttraining's binary_logloss: 0.365603\n",
      "[116]\ttraining's auc: 0.811848\ttraining's binary_logloss: 0.365333\n",
      "[117]\ttraining's auc: 0.81199\ttraining's binary_logloss: 0.365076\n",
      "[118]\ttraining's auc: 0.812123\ttraining's binary_logloss: 0.36482\n",
      "[119]\ttraining's auc: 0.812252\ttraining's binary_logloss: 0.36456\n",
      "[120]\ttraining's auc: 0.812363\ttraining's binary_logloss: 0.3643\n",
      "[121]\ttraining's auc: 0.812536\ttraining's binary_logloss: 0.36404\n",
      "[122]\ttraining's auc: 0.812676\ttraining's binary_logloss: 0.363789\n",
      "[123]\ttraining's auc: 0.812782\ttraining's binary_logloss: 0.363547\n",
      "[124]\ttraining's auc: 0.812907\ttraining's binary_logloss: 0.363294\n",
      "[125]\ttraining's auc: 0.813046\ttraining's binary_logloss: 0.363051\n",
      "[126]\ttraining's auc: 0.813186\ttraining's binary_logloss: 0.362801\n",
      "[127]\ttraining's auc: 0.813295\ttraining's binary_logloss: 0.362572\n",
      "[128]\ttraining's auc: 0.81344\ttraining's binary_logloss: 0.362333\n",
      "[129]\ttraining's auc: 0.813564\ttraining's binary_logloss: 0.362108\n",
      "[130]\ttraining's auc: 0.813692\ttraining's binary_logloss: 0.361876\n",
      "[131]\ttraining's auc: 0.813852\ttraining's binary_logloss: 0.361632\n",
      "[132]\ttraining's auc: 0.814012\ttraining's binary_logloss: 0.361401\n",
      "[133]\ttraining's auc: 0.814097\ttraining's binary_logloss: 0.361186\n",
      "[134]\ttraining's auc: 0.814228\ttraining's binary_logloss: 0.360957\n",
      "[135]\ttraining's auc: 0.814378\ttraining's binary_logloss: 0.360722\n",
      "[136]\ttraining's auc: 0.814547\ttraining's binary_logloss: 0.360491\n",
      "[137]\ttraining's auc: 0.814703\ttraining's binary_logloss: 0.360264\n",
      "[138]\ttraining's auc: 0.814795\ttraining's binary_logloss: 0.360057\n",
      "[139]\ttraining's auc: 0.814952\ttraining's binary_logloss: 0.359838\n",
      "[140]\ttraining's auc: 0.815091\ttraining's binary_logloss: 0.359632\n",
      "[141]\ttraining's auc: 0.815256\ttraining's binary_logloss: 0.359414\n",
      "[142]\ttraining's auc: 0.815354\ttraining's binary_logloss: 0.359216\n",
      "[143]\ttraining's auc: 0.815503\ttraining's binary_logloss: 0.358998\n",
      "[144]\ttraining's auc: 0.815595\ttraining's binary_logloss: 0.358801\n",
      "[145]\ttraining's auc: 0.815751\ttraining's binary_logloss: 0.358586\n",
      "[146]\ttraining's auc: 0.815881\ttraining's binary_logloss: 0.358384\n",
      "[147]\ttraining's auc: 0.816009\ttraining's binary_logloss: 0.358184\n",
      "[148]\ttraining's auc: 0.816105\ttraining's binary_logloss: 0.357998\n",
      "[149]\ttraining's auc: 0.816193\ttraining's binary_logloss: 0.357809\n",
      "[150]\ttraining's auc: 0.816318\ttraining's binary_logloss: 0.357618\n",
      "[151]\ttraining's auc: 0.816451\ttraining's binary_logloss: 0.357424\n",
      "[152]\ttraining's auc: 0.816581\ttraining's binary_logloss: 0.357225\n",
      "[153]\ttraining's auc: 0.816723\ttraining's binary_logloss: 0.357016\n",
      "[154]\ttraining's auc: 0.816842\ttraining's binary_logloss: 0.356831\n",
      "[155]\ttraining's auc: 0.816956\ttraining's binary_logloss: 0.356642\n",
      "[156]\ttraining's auc: 0.817062\ttraining's binary_logloss: 0.356455\n",
      "[157]\ttraining's auc: 0.817183\ttraining's binary_logloss: 0.356278\n",
      "[158]\ttraining's auc: 0.817313\ttraining's binary_logloss: 0.356092\n",
      "[159]\ttraining's auc: 0.817416\ttraining's binary_logloss: 0.355917\n",
      "[160]\ttraining's auc: 0.817533\ttraining's binary_logloss: 0.355732\n",
      "[161]\ttraining's auc: 0.817638\ttraining's binary_logloss: 0.355561\n",
      "[162]\ttraining's auc: 0.817738\ttraining's binary_logloss: 0.355393\n",
      "[163]\ttraining's auc: 0.817852\ttraining's binary_logloss: 0.355223\n",
      "[164]\ttraining's auc: 0.817967\ttraining's binary_logloss: 0.355045\n",
      "[165]\ttraining's auc: 0.818076\ttraining's binary_logloss: 0.354868\n",
      "[166]\ttraining's auc: 0.818182\ttraining's binary_logloss: 0.354696\n",
      "[167]\ttraining's auc: 0.81828\ttraining's binary_logloss: 0.354534\n",
      "[168]\ttraining's auc: 0.818375\ttraining's binary_logloss: 0.354371\n",
      "[169]\ttraining's auc: 0.818479\ttraining's binary_logloss: 0.354211\n",
      "[170]\ttraining's auc: 0.818572\ttraining's binary_logloss: 0.354052\n",
      "[171]\ttraining's auc: 0.818659\ttraining's binary_logloss: 0.353898\n",
      "[172]\ttraining's auc: 0.81875\ttraining's binary_logloss: 0.353742\n",
      "[173]\ttraining's auc: 0.818851\ttraining's binary_logloss: 0.353582\n",
      "[174]\ttraining's auc: 0.818955\ttraining's binary_logloss: 0.353421\n",
      "[175]\ttraining's auc: 0.819058\ttraining's binary_logloss: 0.353267\n",
      "[176]\ttraining's auc: 0.819163\ttraining's binary_logloss: 0.353112\n",
      "[177]\ttraining's auc: 0.819267\ttraining's binary_logloss: 0.352964\n",
      "[178]\ttraining's auc: 0.819378\ttraining's binary_logloss: 0.352805\n",
      "[179]\ttraining's auc: 0.819492\ttraining's binary_logloss: 0.352646\n",
      "[180]\ttraining's auc: 0.819575\ttraining's binary_logloss: 0.352506\n",
      "[181]\ttraining's auc: 0.819659\ttraining's binary_logloss: 0.352364\n",
      "[182]\ttraining's auc: 0.819757\ttraining's binary_logloss: 0.352227\n",
      "[183]\ttraining's auc: 0.819861\ttraining's binary_logloss: 0.352085\n",
      "[184]\ttraining's auc: 0.819935\ttraining's binary_logloss: 0.351955\n",
      "[185]\ttraining's auc: 0.820038\ttraining's binary_logloss: 0.351809\n",
      "[186]\ttraining's auc: 0.82013\ttraining's binary_logloss: 0.351664\n",
      "[187]\ttraining's auc: 0.820226\ttraining's binary_logloss: 0.351531\n",
      "[188]\ttraining's auc: 0.82031\ttraining's binary_logloss: 0.351393\n",
      "[189]\ttraining's auc: 0.820407\ttraining's binary_logloss: 0.351255\n",
      "[190]\ttraining's auc: 0.820482\ttraining's binary_logloss: 0.351126\n",
      "[191]\ttraining's auc: 0.820568\ttraining's binary_logloss: 0.350988\n",
      "[192]\ttraining's auc: 0.820652\ttraining's binary_logloss: 0.350861\n",
      "[193]\ttraining's auc: 0.820743\ttraining's binary_logloss: 0.35073\n",
      "[194]\ttraining's auc: 0.820838\ttraining's binary_logloss: 0.350593\n",
      "[195]\ttraining's auc: 0.820908\ttraining's binary_logloss: 0.350463\n",
      "[196]\ttraining's auc: 0.820987\ttraining's binary_logloss: 0.350337\n",
      "[197]\ttraining's auc: 0.821071\ttraining's binary_logloss: 0.350205\n",
      "[198]\ttraining's auc: 0.821148\ttraining's binary_logloss: 0.350084\n",
      "[199]\ttraining's auc: 0.82122\ttraining's binary_logloss: 0.349967\n",
      "[200]\ttraining's auc: 0.821294\ttraining's binary_logloss: 0.349846\n",
      "[201]\ttraining's auc: 0.821383\ttraining's binary_logloss: 0.349727\n",
      "[202]\ttraining's auc: 0.821467\ttraining's binary_logloss: 0.349605\n",
      "[203]\ttraining's auc: 0.82155\ttraining's binary_logloss: 0.349492\n",
      "[204]\ttraining's auc: 0.821633\ttraining's binary_logloss: 0.349378\n",
      "[205]\ttraining's auc: 0.821715\ttraining's binary_logloss: 0.349249\n",
      "[206]\ttraining's auc: 0.821794\ttraining's binary_logloss: 0.349134\n",
      "[207]\ttraining's auc: 0.821882\ttraining's binary_logloss: 0.349013\n",
      "[208]\ttraining's auc: 0.821957\ttraining's binary_logloss: 0.348907\n",
      "[209]\ttraining's auc: 0.822041\ttraining's binary_logloss: 0.348793\n",
      "[210]\ttraining's auc: 0.822104\ttraining's binary_logloss: 0.348683\n",
      "[211]\ttraining's auc: 0.822183\ttraining's binary_logloss: 0.348569\n",
      "[212]\ttraining's auc: 0.822263\ttraining's binary_logloss: 0.348452\n",
      "[213]\ttraining's auc: 0.822348\ttraining's binary_logloss: 0.348339\n",
      "[214]\ttraining's auc: 0.822427\ttraining's binary_logloss: 0.348227\n",
      "[215]\ttraining's auc: 0.822506\ttraining's binary_logloss: 0.348125\n",
      "[216]\ttraining's auc: 0.822592\ttraining's binary_logloss: 0.348018\n",
      "[217]\ttraining's auc: 0.822662\ttraining's binary_logloss: 0.347914\n",
      "[218]\ttraining's auc: 0.822753\ttraining's binary_logloss: 0.347793\n",
      "[219]\ttraining's auc: 0.822825\ttraining's binary_logloss: 0.347694\n",
      "[220]\ttraining's auc: 0.822894\ttraining's binary_logloss: 0.347592\n",
      "[221]\ttraining's auc: 0.822962\ttraining's binary_logloss: 0.34749\n",
      "[222]\ttraining's auc: 0.823037\ttraining's binary_logloss: 0.34739\n",
      "[223]\ttraining's auc: 0.823093\ttraining's binary_logloss: 0.347291\n",
      "[224]\ttraining's auc: 0.823163\ttraining's binary_logloss: 0.347194\n",
      "[225]\ttraining's auc: 0.823251\ttraining's binary_logloss: 0.347077\n",
      "[226]\ttraining's auc: 0.82333\ttraining's binary_logloss: 0.346974\n",
      "[227]\ttraining's auc: 0.823408\ttraining's binary_logloss: 0.346869\n",
      "[228]\ttraining's auc: 0.823483\ttraining's binary_logloss: 0.346773\n",
      "[229]\ttraining's auc: 0.823543\ttraining's binary_logloss: 0.346681\n",
      "[230]\ttraining's auc: 0.823605\ttraining's binary_logloss: 0.346586\n",
      "[231]\ttraining's auc: 0.823675\ttraining's binary_logloss: 0.346493\n",
      "[232]\ttraining's auc: 0.823749\ttraining's binary_logloss: 0.346393\n",
      "[233]\ttraining's auc: 0.823826\ttraining's binary_logloss: 0.346287\n",
      "[234]\ttraining's auc: 0.8239\ttraining's binary_logloss: 0.346191\n",
      "[235]\ttraining's auc: 0.823975\ttraining's binary_logloss: 0.346097\n",
      "[236]\ttraining's auc: 0.824041\ttraining's binary_logloss: 0.34601\n",
      "[237]\ttraining's auc: 0.824123\ttraining's binary_logloss: 0.345914\n",
      "[238]\ttraining's auc: 0.824204\ttraining's binary_logloss: 0.345808\n",
      "[239]\ttraining's auc: 0.824268\ttraining's binary_logloss: 0.345716\n",
      "[240]\ttraining's auc: 0.82434\ttraining's binary_logloss: 0.345627\n",
      "[241]\ttraining's auc: 0.824402\ttraining's binary_logloss: 0.345541\n",
      "[242]\ttraining's auc: 0.824472\ttraining's binary_logloss: 0.345445\n",
      "[243]\ttraining's auc: 0.824538\ttraining's binary_logloss: 0.345359\n",
      "[244]\ttraining's auc: 0.824602\ttraining's binary_logloss: 0.34527\n",
      "[245]\ttraining's auc: 0.824664\ttraining's binary_logloss: 0.345181\n",
      "[246]\ttraining's auc: 0.824732\ttraining's binary_logloss: 0.34509\n",
      "[247]\ttraining's auc: 0.824781\ttraining's binary_logloss: 0.345015\n",
      "[248]\ttraining's auc: 0.824841\ttraining's binary_logloss: 0.34493\n",
      "[249]\ttraining's auc: 0.824922\ttraining's binary_logloss: 0.34483\n",
      "[250]\ttraining's auc: 0.824986\ttraining's binary_logloss: 0.34475\n",
      "[251]\ttraining's auc: 0.825074\ttraining's binary_logloss: 0.344649\n",
      "[252]\ttraining's auc: 0.825139\ttraining's binary_logloss: 0.344555\n",
      "[253]\ttraining's auc: 0.825194\ttraining's binary_logloss: 0.344476\n",
      "[254]\ttraining's auc: 0.825259\ttraining's binary_logloss: 0.344396\n",
      "[255]\ttraining's auc: 0.825315\ttraining's binary_logloss: 0.34432\n",
      "[256]\ttraining's auc: 0.825368\ttraining's binary_logloss: 0.344246\n",
      "[257]\ttraining's auc: 0.825452\ttraining's binary_logloss: 0.344153\n",
      "[258]\ttraining's auc: 0.825521\ttraining's binary_logloss: 0.34406\n",
      "[259]\ttraining's auc: 0.825576\ttraining's binary_logloss: 0.343985\n",
      "[260]\ttraining's auc: 0.825636\ttraining's binary_logloss: 0.343906\n",
      "[261]\ttraining's auc: 0.825692\ttraining's binary_logloss: 0.343832\n",
      "[262]\ttraining's auc: 0.825744\ttraining's binary_logloss: 0.343756\n",
      "[263]\ttraining's auc: 0.825817\ttraining's binary_logloss: 0.343667\n",
      "[264]\ttraining's auc: 0.825872\ttraining's binary_logloss: 0.3436\n",
      "[265]\ttraining's auc: 0.825929\ttraining's binary_logloss: 0.343516\n",
      "[266]\ttraining's auc: 0.825984\ttraining's binary_logloss: 0.343443\n",
      "[267]\ttraining's auc: 0.826037\ttraining's binary_logloss: 0.343369\n",
      "[268]\ttraining's auc: 0.826104\ttraining's binary_logloss: 0.343285\n",
      "[269]\ttraining's auc: 0.82617\ttraining's binary_logloss: 0.343207\n",
      "[270]\ttraining's auc: 0.826225\ttraining's binary_logloss: 0.34314\n",
      "[271]\ttraining's auc: 0.826276\ttraining's binary_logloss: 0.343067\n",
      "[272]\ttraining's auc: 0.826328\ttraining's binary_logloss: 0.342997\n",
      "[273]\ttraining's auc: 0.826399\ttraining's binary_logloss: 0.342903\n",
      "[274]\ttraining's auc: 0.826457\ttraining's binary_logloss: 0.34283\n",
      "[275]\ttraining's auc: 0.826521\ttraining's binary_logloss: 0.34275\n",
      "[276]\ttraining's auc: 0.826586\ttraining's binary_logloss: 0.342677\n",
      "[277]\ttraining's auc: 0.826638\ttraining's binary_logloss: 0.34261\n",
      "[278]\ttraining's auc: 0.826729\ttraining's binary_logloss: 0.342505\n",
      "[279]\ttraining's auc: 0.826792\ttraining's binary_logloss: 0.342424\n",
      "[280]\ttraining's auc: 0.826855\ttraining's binary_logloss: 0.342349\n",
      "[281]\ttraining's auc: 0.826905\ttraining's binary_logloss: 0.342282\n",
      "[282]\ttraining's auc: 0.826958\ttraining's binary_logloss: 0.342214\n",
      "[283]\ttraining's auc: 0.827013\ttraining's binary_logloss: 0.342136\n",
      "[284]\ttraining's auc: 0.827087\ttraining's binary_logloss: 0.342055\n",
      "[285]\ttraining's auc: 0.827147\ttraining's binary_logloss: 0.341987\n",
      "[286]\ttraining's auc: 0.827206\ttraining's binary_logloss: 0.341916\n",
      "[287]\ttraining's auc: 0.827262\ttraining's binary_logloss: 0.341847\n",
      "[288]\ttraining's auc: 0.827314\ttraining's binary_logloss: 0.341779\n",
      "[289]\ttraining's auc: 0.827365\ttraining's binary_logloss: 0.341713\n",
      "[290]\ttraining's auc: 0.827424\ttraining's binary_logloss: 0.341637\n",
      "[291]\ttraining's auc: 0.8275\ttraining's binary_logloss: 0.341551\n",
      "[292]\ttraining's auc: 0.827579\ttraining's binary_logloss: 0.341467\n",
      "[293]\ttraining's auc: 0.827633\ttraining's binary_logloss: 0.3414\n",
      "[294]\ttraining's auc: 0.827673\ttraining's binary_logloss: 0.341346\n",
      "[295]\ttraining's auc: 0.82773\ttraining's binary_logloss: 0.341271\n",
      "[296]\ttraining's auc: 0.827809\ttraining's binary_logloss: 0.341184\n",
      "[297]\ttraining's auc: 0.827871\ttraining's binary_logloss: 0.341109\n",
      "[298]\ttraining's auc: 0.827909\ttraining's binary_logloss: 0.341052\n",
      "[299]\ttraining's auc: 0.827964\ttraining's binary_logloss: 0.340978\n",
      "[300]\ttraining's auc: 0.828009\ttraining's binary_logloss: 0.340918\n",
      "[301]\ttraining's auc: 0.828069\ttraining's binary_logloss: 0.340844\n",
      "[302]\ttraining's auc: 0.828121\ttraining's binary_logloss: 0.340787\n",
      "[303]\ttraining's auc: 0.828169\ttraining's binary_logloss: 0.340728\n",
      "[304]\ttraining's auc: 0.828223\ttraining's binary_logloss: 0.340655\n",
      "[305]\ttraining's auc: 0.828288\ttraining's binary_logloss: 0.34059\n",
      "[306]\ttraining's auc: 0.828337\ttraining's binary_logloss: 0.340531\n",
      "[307]\ttraining's auc: 0.82839\ttraining's binary_logloss: 0.34046\n",
      "[308]\ttraining's auc: 0.828449\ttraining's binary_logloss: 0.340389\n",
      "[309]\ttraining's auc: 0.828509\ttraining's binary_logloss: 0.340319\n",
      "[310]\ttraining's auc: 0.828582\ttraining's binary_logloss: 0.340241\n",
      "[311]\ttraining's auc: 0.82865\ttraining's binary_logloss: 0.340162\n",
      "[312]\ttraining's auc: 0.8287\ttraining's binary_logloss: 0.3401\n",
      "[313]\ttraining's auc: 0.828758\ttraining's binary_logloss: 0.340036\n",
      "[314]\ttraining's auc: 0.828822\ttraining's binary_logloss: 0.339967\n",
      "[315]\ttraining's auc: 0.828895\ttraining's binary_logloss: 0.339895\n",
      "[316]\ttraining's auc: 0.82896\ttraining's binary_logloss: 0.339823\n",
      "[317]\ttraining's auc: 0.829007\ttraining's binary_logloss: 0.339771\n",
      "[318]\ttraining's auc: 0.829049\ttraining's binary_logloss: 0.339718\n",
      "[319]\ttraining's auc: 0.829096\ttraining's binary_logloss: 0.339665\n",
      "[320]\ttraining's auc: 0.829143\ttraining's binary_logloss: 0.339612\n",
      "[321]\ttraining's auc: 0.829209\ttraining's binary_logloss: 0.339533\n",
      "[322]\ttraining's auc: 0.829242\ttraining's binary_logloss: 0.33949\n",
      "[323]\ttraining's auc: 0.829289\ttraining's binary_logloss: 0.339432\n",
      "[324]\ttraining's auc: 0.829342\ttraining's binary_logloss: 0.33937\n",
      "[325]\ttraining's auc: 0.82939\ttraining's binary_logloss: 0.339313\n",
      "[326]\ttraining's auc: 0.829433\ttraining's binary_logloss: 0.339261\n",
      "[327]\ttraining's auc: 0.829487\ttraining's binary_logloss: 0.339203\n",
      "[328]\ttraining's auc: 0.829536\ttraining's binary_logloss: 0.339147\n",
      "[329]\ttraining's auc: 0.829576\ttraining's binary_logloss: 0.339096\n",
      "[330]\ttraining's auc: 0.82963\ttraining's binary_logloss: 0.339036\n",
      "[331]\ttraining's auc: 0.829677\ttraining's binary_logloss: 0.338979\n",
      "[332]\ttraining's auc: 0.829727\ttraining's binary_logloss: 0.338926\n",
      "[333]\ttraining's auc: 0.829764\ttraining's binary_logloss: 0.338882\n",
      "[334]\ttraining's auc: 0.829828\ttraining's binary_logloss: 0.338815\n",
      "[335]\ttraining's auc: 0.829879\ttraining's binary_logloss: 0.338755\n",
      "[336]\ttraining's auc: 0.82992\ttraining's binary_logloss: 0.338707\n",
      "[337]\ttraining's auc: 0.829956\ttraining's binary_logloss: 0.33866\n",
      "[338]\ttraining's auc: 0.830017\ttraining's binary_logloss: 0.338591\n",
      "[339]\ttraining's auc: 0.830071\ttraining's binary_logloss: 0.338531\n",
      "[340]\ttraining's auc: 0.830142\ttraining's binary_logloss: 0.338461\n",
      "[341]\ttraining's auc: 0.830193\ttraining's binary_logloss: 0.338402\n",
      "[342]\ttraining's auc: 0.830231\ttraining's binary_logloss: 0.338355\n",
      "[343]\ttraining's auc: 0.830268\ttraining's binary_logloss: 0.33831\n",
      "[344]\ttraining's auc: 0.830328\ttraining's binary_logloss: 0.338241\n",
      "[345]\ttraining's auc: 0.830373\ttraining's binary_logloss: 0.338191\n",
      "[346]\ttraining's auc: 0.830413\ttraining's binary_logloss: 0.338143\n",
      "[347]\ttraining's auc: 0.83046\ttraining's binary_logloss: 0.33809\n",
      "[348]\ttraining's auc: 0.830512\ttraining's binary_logloss: 0.338032\n",
      "[349]\ttraining's auc: 0.830556\ttraining's binary_logloss: 0.337984\n",
      "[350]\ttraining's auc: 0.830608\ttraining's binary_logloss: 0.337926\n",
      "[351]\ttraining's auc: 0.83069\ttraining's binary_logloss: 0.337837\n",
      "[352]\ttraining's auc: 0.830727\ttraining's binary_logloss: 0.337792\n",
      "[353]\ttraining's auc: 0.830772\ttraining's binary_logloss: 0.337744\n",
      "[354]\ttraining's auc: 0.830812\ttraining's binary_logloss: 0.337695\n",
      "[355]\ttraining's auc: 0.830868\ttraining's binary_logloss: 0.337638\n",
      "[356]\ttraining's auc: 0.830921\ttraining's binary_logloss: 0.337582\n",
      "[357]\ttraining's auc: 0.830983\ttraining's binary_logloss: 0.337514\n",
      "[358]\ttraining's auc: 0.831022\ttraining's binary_logloss: 0.337472\n",
      "[359]\ttraining's auc: 0.831059\ttraining's binary_logloss: 0.337421\n",
      "[360]\ttraining's auc: 0.831088\ttraining's binary_logloss: 0.337383\n",
      "[361]\ttraining's auc: 0.831126\ttraining's binary_logloss: 0.337338\n",
      "[362]\ttraining's auc: 0.831174\ttraining's binary_logloss: 0.337283\n",
      "[363]\ttraining's auc: 0.831211\ttraining's binary_logloss: 0.337241\n",
      "[364]\ttraining's auc: 0.831249\ttraining's binary_logloss: 0.337195\n",
      "[365]\ttraining's auc: 0.831291\ttraining's binary_logloss: 0.337142\n",
      "[366]\ttraining's auc: 0.831337\ttraining's binary_logloss: 0.337091\n",
      "[367]\ttraining's auc: 0.831371\ttraining's binary_logloss: 0.337051\n",
      "[368]\ttraining's auc: 0.831417\ttraining's binary_logloss: 0.337002\n",
      "[369]\ttraining's auc: 0.831462\ttraining's binary_logloss: 0.336953\n",
      "[370]\ttraining's auc: 0.831515\ttraining's binary_logloss: 0.336891\n",
      "[371]\ttraining's auc: 0.831556\ttraining's binary_logloss: 0.336846\n",
      "[372]\ttraining's auc: 0.831593\ttraining's binary_logloss: 0.336805\n",
      "[373]\ttraining's auc: 0.831633\ttraining's binary_logloss: 0.336759\n",
      "[374]\ttraining's auc: 0.831683\ttraining's binary_logloss: 0.336704\n",
      "[375]\ttraining's auc: 0.831726\ttraining's binary_logloss: 0.336655\n",
      "[376]\ttraining's auc: 0.831764\ttraining's binary_logloss: 0.336612\n",
      "[377]\ttraining's auc: 0.831802\ttraining's binary_logloss: 0.336572\n",
      "[378]\ttraining's auc: 0.831846\ttraining's binary_logloss: 0.336525\n",
      "[379]\ttraining's auc: 0.831881\ttraining's binary_logloss: 0.336487\n",
      "[380]\ttraining's auc: 0.831927\ttraining's binary_logloss: 0.336439\n",
      "[381]\ttraining's auc: 0.831958\ttraining's binary_logloss: 0.336402\n",
      "[382]\ttraining's auc: 0.832018\ttraining's binary_logloss: 0.336333\n",
      "[383]\ttraining's auc: 0.832065\ttraining's binary_logloss: 0.336285\n",
      "[384]\ttraining's auc: 0.8321\ttraining's binary_logloss: 0.336245\n",
      "[385]\ttraining's auc: 0.832148\ttraining's binary_logloss: 0.336198\n",
      "[386]\ttraining's auc: 0.832179\ttraining's binary_logloss: 0.336164\n",
      "[387]\ttraining's auc: 0.832221\ttraining's binary_logloss: 0.336116\n",
      "[388]\ttraining's auc: 0.832263\ttraining's binary_logloss: 0.336074\n",
      "[389]\ttraining's auc: 0.832302\ttraining's binary_logloss: 0.336027\n",
      "[390]\ttraining's auc: 0.832343\ttraining's binary_logloss: 0.335984\n",
      "[391]\ttraining's auc: 0.832382\ttraining's binary_logloss: 0.335941\n",
      "[392]\ttraining's auc: 0.832421\ttraining's binary_logloss: 0.335897\n",
      "[393]\ttraining's auc: 0.832463\ttraining's binary_logloss: 0.335852\n",
      "[394]\ttraining's auc: 0.832503\ttraining's binary_logloss: 0.335811\n",
      "[395]\ttraining's auc: 0.832537\ttraining's binary_logloss: 0.335769\n",
      "[396]\ttraining's auc: 0.832574\ttraining's binary_logloss: 0.33573\n",
      "[397]\ttraining's auc: 0.832601\ttraining's binary_logloss: 0.3357\n",
      "[398]\ttraining's auc: 0.83264\ttraining's binary_logloss: 0.335663\n",
      "[399]\ttraining's auc: 0.832673\ttraining's binary_logloss: 0.335624\n",
      "[400]\ttraining's auc: 0.8327\ttraining's binary_logloss: 0.33559\n",
      "[401]\ttraining's auc: 0.832732\ttraining's binary_logloss: 0.335553\n",
      "[402]\ttraining's auc: 0.83279\ttraining's binary_logloss: 0.335491\n",
      "[403]\ttraining's auc: 0.832825\ttraining's binary_logloss: 0.335451\n",
      "[404]\ttraining's auc: 0.832854\ttraining's binary_logloss: 0.335419\n",
      "[405]\ttraining's auc: 0.83289\ttraining's binary_logloss: 0.335379\n",
      "[406]\ttraining's auc: 0.832939\ttraining's binary_logloss: 0.335322\n",
      "[407]\ttraining's auc: 0.832975\ttraining's binary_logloss: 0.335283\n",
      "[408]\ttraining's auc: 0.833006\ttraining's binary_logloss: 0.335245\n",
      "[409]\ttraining's auc: 0.83304\ttraining's binary_logloss: 0.335206\n",
      "[410]\ttraining's auc: 0.833081\ttraining's binary_logloss: 0.335166\n",
      "[411]\ttraining's auc: 0.833122\ttraining's binary_logloss: 0.335119\n",
      "[412]\ttraining's auc: 0.833165\ttraining's binary_logloss: 0.335073\n",
      "[413]\ttraining's auc: 0.833199\ttraining's binary_logloss: 0.335037\n",
      "[414]\ttraining's auc: 0.833224\ttraining's binary_logloss: 0.335003\n",
      "[415]\ttraining's auc: 0.833249\ttraining's binary_logloss: 0.334976\n",
      "[416]\ttraining's auc: 0.833287\ttraining's binary_logloss: 0.334936\n",
      "[417]\ttraining's auc: 0.833319\ttraining's binary_logloss: 0.334904\n",
      "[418]\ttraining's auc: 0.833363\ttraining's binary_logloss: 0.334859\n",
      "[419]\ttraining's auc: 0.833388\ttraining's binary_logloss: 0.334831\n",
      "[420]\ttraining's auc: 0.833422\ttraining's binary_logloss: 0.334793\n",
      "[421]\ttraining's auc: 0.833465\ttraining's binary_logloss: 0.334752\n",
      "[422]\ttraining's auc: 0.833501\ttraining's binary_logloss: 0.334712\n",
      "[423]\ttraining's auc: 0.833525\ttraining's binary_logloss: 0.334684\n",
      "[424]\ttraining's auc: 0.833555\ttraining's binary_logloss: 0.334649\n",
      "[425]\ttraining's auc: 0.833582\ttraining's binary_logloss: 0.334619\n",
      "[426]\ttraining's auc: 0.833609\ttraining's binary_logloss: 0.33459\n",
      "[427]\ttraining's auc: 0.833647\ttraining's binary_logloss: 0.334551\n",
      "[428]\ttraining's auc: 0.833684\ttraining's binary_logloss: 0.33451\n",
      "[429]\ttraining's auc: 0.833714\ttraining's binary_logloss: 0.33448\n",
      "[430]\ttraining's auc: 0.833744\ttraining's binary_logloss: 0.334446\n",
      "[431]\ttraining's auc: 0.833786\ttraining's binary_logloss: 0.334404\n",
      "[432]\ttraining's auc: 0.833817\ttraining's binary_logloss: 0.33437\n",
      "[433]\ttraining's auc: 0.833851\ttraining's binary_logloss: 0.334332\n",
      "[434]\ttraining's auc: 0.833886\ttraining's binary_logloss: 0.334295\n",
      "[435]\ttraining's auc: 0.833916\ttraining's binary_logloss: 0.334264\n",
      "[436]\ttraining's auc: 0.83395\ttraining's binary_logloss: 0.334225\n",
      "[437]\ttraining's auc: 0.833983\ttraining's binary_logloss: 0.334191\n",
      "[438]\ttraining's auc: 0.834026\ttraining's binary_logloss: 0.334151\n",
      "[439]\ttraining's auc: 0.834067\ttraining's binary_logloss: 0.334108\n",
      "[440]\ttraining's auc: 0.834093\ttraining's binary_logloss: 0.334077\n",
      "[441]\ttraining's auc: 0.83413\ttraining's binary_logloss: 0.334037\n",
      "[442]\ttraining's auc: 0.834165\ttraining's binary_logloss: 0.334001\n",
      "[443]\ttraining's auc: 0.834217\ttraining's binary_logloss: 0.33395\n",
      "[444]\ttraining's auc: 0.834252\ttraining's binary_logloss: 0.333914\n",
      "[445]\ttraining's auc: 0.834281\ttraining's binary_logloss: 0.333886\n",
      "[446]\ttraining's auc: 0.834307\ttraining's binary_logloss: 0.333859\n",
      "[447]\ttraining's auc: 0.834347\ttraining's binary_logloss: 0.333815\n",
      "[448]\ttraining's auc: 0.834401\ttraining's binary_logloss: 0.333752\n",
      "[449]\ttraining's auc: 0.834438\ttraining's binary_logloss: 0.333711\n",
      "[450]\ttraining's auc: 0.834472\ttraining's binary_logloss: 0.333678\n",
      "[451]\ttraining's auc: 0.834506\ttraining's binary_logloss: 0.333642\n",
      "[452]\ttraining's auc: 0.834535\ttraining's binary_logloss: 0.333609\n",
      "[453]\ttraining's auc: 0.834568\ttraining's binary_logloss: 0.333575\n",
      "[454]\ttraining's auc: 0.834607\ttraining's binary_logloss: 0.333534\n",
      "[455]\ttraining's auc: 0.83464\ttraining's binary_logloss: 0.333499\n",
      "[456]\ttraining's auc: 0.834675\ttraining's binary_logloss: 0.333464\n",
      "[457]\ttraining's auc: 0.834705\ttraining's binary_logloss: 0.333431\n",
      "[458]\ttraining's auc: 0.834737\ttraining's binary_logloss: 0.333398\n",
      "[459]\ttraining's auc: 0.834771\ttraining's binary_logloss: 0.333363\n",
      "[460]\ttraining's auc: 0.834795\ttraining's binary_logloss: 0.333335\n",
      "[461]\ttraining's auc: 0.834838\ttraining's binary_logloss: 0.333293\n",
      "[462]\ttraining's auc: 0.834871\ttraining's binary_logloss: 0.333258\n",
      "[463]\ttraining's auc: 0.8349\ttraining's binary_logloss: 0.333229\n",
      "[464]\ttraining's auc: 0.834945\ttraining's binary_logloss: 0.33319\n",
      "[465]\ttraining's auc: 0.834981\ttraining's binary_logloss: 0.333151\n",
      "[466]\ttraining's auc: 0.835013\ttraining's binary_logloss: 0.333116\n",
      "[467]\ttraining's auc: 0.835039\ttraining's binary_logloss: 0.333088\n",
      "[468]\ttraining's auc: 0.835067\ttraining's binary_logloss: 0.333057\n",
      "[469]\ttraining's auc: 0.8351\ttraining's binary_logloss: 0.333018\n",
      "[470]\ttraining's auc: 0.835127\ttraining's binary_logloss: 0.33299\n",
      "[471]\ttraining's auc: 0.835168\ttraining's binary_logloss: 0.332949\n",
      "[472]\ttraining's auc: 0.835196\ttraining's binary_logloss: 0.33292\n",
      "[473]\ttraining's auc: 0.835225\ttraining's binary_logloss: 0.332887\n",
      "[474]\ttraining's auc: 0.835256\ttraining's binary_logloss: 0.332854\n",
      "[475]\ttraining's auc: 0.835287\ttraining's binary_logloss: 0.332822\n",
      "[476]\ttraining's auc: 0.835322\ttraining's binary_logloss: 0.332785\n",
      "[477]\ttraining's auc: 0.83535\ttraining's binary_logloss: 0.332756\n",
      "[478]\ttraining's auc: 0.835378\ttraining's binary_logloss: 0.332727\n",
      "[479]\ttraining's auc: 0.835412\ttraining's binary_logloss: 0.332693\n",
      "[480]\ttraining's auc: 0.835444\ttraining's binary_logloss: 0.332662\n",
      "[481]\ttraining's auc: 0.835477\ttraining's binary_logloss: 0.332629\n",
      "[482]\ttraining's auc: 0.835503\ttraining's binary_logloss: 0.332599\n",
      "[483]\ttraining's auc: 0.835528\ttraining's binary_logloss: 0.332573\n",
      "[484]\ttraining's auc: 0.835554\ttraining's binary_logloss: 0.332546\n",
      "[485]\ttraining's auc: 0.835577\ttraining's binary_logloss: 0.332521\n",
      "[486]\ttraining's auc: 0.835601\ttraining's binary_logloss: 0.332496\n",
      "[487]\ttraining's auc: 0.835632\ttraining's binary_logloss: 0.332466\n",
      "[488]\ttraining's auc: 0.83566\ttraining's binary_logloss: 0.332439\n",
      "[489]\ttraining's auc: 0.835688\ttraining's binary_logloss: 0.332412\n",
      "[490]\ttraining's auc: 0.835718\ttraining's binary_logloss: 0.332382\n",
      "[491]\ttraining's auc: 0.835744\ttraining's binary_logloss: 0.332355\n",
      "[492]\ttraining's auc: 0.83578\ttraining's binary_logloss: 0.332319\n",
      "[493]\ttraining's auc: 0.835815\ttraining's binary_logloss: 0.332282\n",
      "[494]\ttraining's auc: 0.835841\ttraining's binary_logloss: 0.332258\n",
      "[495]\ttraining's auc: 0.835865\ttraining's binary_logloss: 0.332231\n",
      "[496]\ttraining's auc: 0.835894\ttraining's binary_logloss: 0.332203\n",
      "[497]\ttraining's auc: 0.835926\ttraining's binary_logloss: 0.332173\n",
      "[498]\ttraining's auc: 0.83595\ttraining's binary_logloss: 0.332148\n",
      "[499]\ttraining's auc: 0.835973\ttraining's binary_logloss: 0.332125\n",
      "[500]\ttraining's auc: 0.835999\ttraining's binary_logloss: 0.332098\n",
      "[501]\ttraining's auc: 0.836036\ttraining's binary_logloss: 0.332062\n",
      "[502]\ttraining's auc: 0.836061\ttraining's binary_logloss: 0.332037\n",
      "[503]\ttraining's auc: 0.836093\ttraining's binary_logloss: 0.332007\n",
      "[504]\ttraining's auc: 0.836123\ttraining's binary_logloss: 0.331976\n",
      "[505]\ttraining's auc: 0.836153\ttraining's binary_logloss: 0.331943\n",
      "[506]\ttraining's auc: 0.836177\ttraining's binary_logloss: 0.331917\n",
      "[507]\ttraining's auc: 0.836209\ttraining's binary_logloss: 0.331887\n",
      "[508]\ttraining's auc: 0.836243\ttraining's binary_logloss: 0.331854\n",
      "[509]\ttraining's auc: 0.836276\ttraining's binary_logloss: 0.331822\n",
      "[510]\ttraining's auc: 0.836305\ttraining's binary_logloss: 0.331794\n",
      "[511]\ttraining's auc: 0.836336\ttraining's binary_logloss: 0.331765\n",
      "[512]\ttraining's auc: 0.836358\ttraining's binary_logloss: 0.33174\n",
      "[513]\ttraining's auc: 0.836389\ttraining's binary_logloss: 0.331711\n",
      "[514]\ttraining's auc: 0.836413\ttraining's binary_logloss: 0.331685\n",
      "[515]\ttraining's auc: 0.836435\ttraining's binary_logloss: 0.331664\n",
      "[516]\ttraining's auc: 0.83647\ttraining's binary_logloss: 0.331631\n",
      "[517]\ttraining's auc: 0.8365\ttraining's binary_logloss: 0.3316\n",
      "[518]\ttraining's auc: 0.836523\ttraining's binary_logloss: 0.331577\n",
      "[519]\ttraining's auc: 0.836547\ttraining's binary_logloss: 0.331554\n",
      "[520]\ttraining's auc: 0.836575\ttraining's binary_logloss: 0.331525\n",
      "[521]\ttraining's auc: 0.836597\ttraining's binary_logloss: 0.331502\n",
      "[522]\ttraining's auc: 0.836622\ttraining's binary_logloss: 0.331476\n",
      "[523]\ttraining's auc: 0.836665\ttraining's binary_logloss: 0.331433\n",
      "[524]\ttraining's auc: 0.83669\ttraining's binary_logloss: 0.33141\n",
      "[525]\ttraining's auc: 0.836722\ttraining's binary_logloss: 0.331379\n",
      "[526]\ttraining's auc: 0.83674\ttraining's binary_logloss: 0.33136\n",
      "[527]\ttraining's auc: 0.836774\ttraining's binary_logloss: 0.331326\n",
      "[528]\ttraining's auc: 0.836806\ttraining's binary_logloss: 0.331295\n",
      "[529]\ttraining's auc: 0.836835\ttraining's binary_logloss: 0.331264\n",
      "[530]\ttraining's auc: 0.836864\ttraining's binary_logloss: 0.331238\n",
      "[531]\ttraining's auc: 0.836901\ttraining's binary_logloss: 0.331202\n",
      "[532]\ttraining's auc: 0.836936\ttraining's binary_logloss: 0.331166\n",
      "[533]\ttraining's auc: 0.836965\ttraining's binary_logloss: 0.331134\n",
      "[534]\ttraining's auc: 0.836996\ttraining's binary_logloss: 0.331101\n",
      "[535]\ttraining's auc: 0.83702\ttraining's binary_logloss: 0.331076\n",
      "[536]\ttraining's auc: 0.837052\ttraining's binary_logloss: 0.33104\n",
      "[537]\ttraining's auc: 0.837078\ttraining's binary_logloss: 0.331015\n",
      "[538]\ttraining's auc: 0.837105\ttraining's binary_logloss: 0.33099\n",
      "[539]\ttraining's auc: 0.837138\ttraining's binary_logloss: 0.330958\n",
      "[540]\ttraining's auc: 0.837163\ttraining's binary_logloss: 0.330935\n",
      "[541]\ttraining's auc: 0.837189\ttraining's binary_logloss: 0.330905\n",
      "[542]\ttraining's auc: 0.837229\ttraining's binary_logloss: 0.330868\n",
      "[543]\ttraining's auc: 0.837254\ttraining's binary_logloss: 0.330843\n",
      "[544]\ttraining's auc: 0.837285\ttraining's binary_logloss: 0.330813\n",
      "[545]\ttraining's auc: 0.837309\ttraining's binary_logloss: 0.330791\n",
      "[546]\ttraining's auc: 0.837338\ttraining's binary_logloss: 0.330761\n",
      "[547]\ttraining's auc: 0.837367\ttraining's binary_logloss: 0.330734\n",
      "[548]\ttraining's auc: 0.837403\ttraining's binary_logloss: 0.330701\n",
      "[549]\ttraining's auc: 0.837433\ttraining's binary_logloss: 0.33067\n",
      "[550]\ttraining's auc: 0.837464\ttraining's binary_logloss: 0.330641\n",
      "[551]\ttraining's auc: 0.837499\ttraining's binary_logloss: 0.330605\n",
      "[552]\ttraining's auc: 0.837518\ttraining's binary_logloss: 0.330588\n",
      "[553]\ttraining's auc: 0.837543\ttraining's binary_logloss: 0.330562\n",
      "[554]\ttraining's auc: 0.837574\ttraining's binary_logloss: 0.330534\n",
      "[555]\ttraining's auc: 0.837602\ttraining's binary_logloss: 0.330507\n",
      "[556]\ttraining's auc: 0.837624\ttraining's binary_logloss: 0.330485\n",
      "[557]\ttraining's auc: 0.837647\ttraining's binary_logloss: 0.33046\n",
      "[558]\ttraining's auc: 0.837672\ttraining's binary_logloss: 0.330435\n",
      "[559]\ttraining's auc: 0.837694\ttraining's binary_logloss: 0.330414\n",
      "[560]\ttraining's auc: 0.837727\ttraining's binary_logloss: 0.330383\n",
      "[561]\ttraining's auc: 0.837755\ttraining's binary_logloss: 0.330358\n",
      "[562]\ttraining's auc: 0.837779\ttraining's binary_logloss: 0.330335\n",
      "[563]\ttraining's auc: 0.837814\ttraining's binary_logloss: 0.3303\n",
      "[564]\ttraining's auc: 0.837844\ttraining's binary_logloss: 0.330273\n",
      "[565]\ttraining's auc: 0.837876\ttraining's binary_logloss: 0.330244\n",
      "[566]\ttraining's auc: 0.837897\ttraining's binary_logloss: 0.330223\n",
      "[567]\ttraining's auc: 0.837919\ttraining's binary_logloss: 0.3302\n",
      "[568]\ttraining's auc: 0.837953\ttraining's binary_logloss: 0.330169\n",
      "[569]\ttraining's auc: 0.837975\ttraining's binary_logloss: 0.330147\n",
      "[570]\ttraining's auc: 0.837996\ttraining's binary_logloss: 0.330127\n",
      "[571]\ttraining's auc: 0.838023\ttraining's binary_logloss: 0.330101\n",
      "[572]\ttraining's auc: 0.838043\ttraining's binary_logloss: 0.330082\n",
      "[573]\ttraining's auc: 0.838065\ttraining's binary_logloss: 0.330059\n",
      "[574]\ttraining's auc: 0.838091\ttraining's binary_logloss: 0.330034\n",
      "[575]\ttraining's auc: 0.838116\ttraining's binary_logloss: 0.330009\n",
      "[576]\ttraining's auc: 0.838137\ttraining's binary_logloss: 0.329987\n",
      "[577]\ttraining's auc: 0.838159\ttraining's binary_logloss: 0.329965\n",
      "[578]\ttraining's auc: 0.838184\ttraining's binary_logloss: 0.329939\n",
      "[579]\ttraining's auc: 0.838209\ttraining's binary_logloss: 0.329915\n",
      "[580]\ttraining's auc: 0.838239\ttraining's binary_logloss: 0.329881\n",
      "[581]\ttraining's auc: 0.838264\ttraining's binary_logloss: 0.329856\n",
      "[582]\ttraining's auc: 0.838293\ttraining's binary_logloss: 0.329827\n",
      "[583]\ttraining's auc: 0.838322\ttraining's binary_logloss: 0.329798\n",
      "[584]\ttraining's auc: 0.838341\ttraining's binary_logloss: 0.329778\n",
      "[585]\ttraining's auc: 0.838367\ttraining's binary_logloss: 0.329751\n",
      "[586]\ttraining's auc: 0.838392\ttraining's binary_logloss: 0.329726\n",
      "[587]\ttraining's auc: 0.838411\ttraining's binary_logloss: 0.329708\n",
      "[588]\ttraining's auc: 0.838432\ttraining's binary_logloss: 0.329688\n",
      "[589]\ttraining's auc: 0.838454\ttraining's binary_logloss: 0.329663\n",
      "[590]\ttraining's auc: 0.838477\ttraining's binary_logloss: 0.329639\n",
      "[591]\ttraining's auc: 0.838502\ttraining's binary_logloss: 0.329615\n",
      "[592]\ttraining's auc: 0.83853\ttraining's binary_logloss: 0.32959\n",
      "[593]\ttraining's auc: 0.838557\ttraining's binary_logloss: 0.329565\n",
      "[594]\ttraining's auc: 0.83858\ttraining's binary_logloss: 0.329544\n",
      "[595]\ttraining's auc: 0.838609\ttraining's binary_logloss: 0.329512\n",
      "[596]\ttraining's auc: 0.838635\ttraining's binary_logloss: 0.329489\n",
      "[597]\ttraining's auc: 0.838658\ttraining's binary_logloss: 0.329466\n",
      "[598]\ttraining's auc: 0.838689\ttraining's binary_logloss: 0.329439\n",
      "[599]\ttraining's auc: 0.838708\ttraining's binary_logloss: 0.329419\n",
      "[600]\ttraining's auc: 0.838736\ttraining's binary_logloss: 0.329393\n",
      "[601]\ttraining's auc: 0.838756\ttraining's binary_logloss: 0.329374\n",
      "[602]\ttraining's auc: 0.83878\ttraining's binary_logloss: 0.329351\n",
      "[603]\ttraining's auc: 0.838805\ttraining's binary_logloss: 0.329323\n",
      "[604]\ttraining's auc: 0.838829\ttraining's binary_logloss: 0.329298\n",
      "[605]\ttraining's auc: 0.838853\ttraining's binary_logloss: 0.329275\n",
      "[606]\ttraining's auc: 0.838879\ttraining's binary_logloss: 0.329252\n",
      "[607]\ttraining's auc: 0.838897\ttraining's binary_logloss: 0.329232\n",
      "[608]\ttraining's auc: 0.83892\ttraining's binary_logloss: 0.329209\n",
      "[609]\ttraining's auc: 0.838941\ttraining's binary_logloss: 0.329188\n",
      "[610]\ttraining's auc: 0.838968\ttraining's binary_logloss: 0.329164\n",
      "[611]\ttraining's auc: 0.83899\ttraining's binary_logloss: 0.329145\n",
      "[612]\ttraining's auc: 0.839009\ttraining's binary_logloss: 0.329123\n",
      "[613]\ttraining's auc: 0.839036\ttraining's binary_logloss: 0.329096\n",
      "[614]\ttraining's auc: 0.839052\ttraining's binary_logloss: 0.329079\n",
      "[615]\ttraining's auc: 0.839078\ttraining's binary_logloss: 0.329053\n",
      "[616]\ttraining's auc: 0.839102\ttraining's binary_logloss: 0.329032\n",
      "[617]\ttraining's auc: 0.839125\ttraining's binary_logloss: 0.329009\n",
      "[618]\ttraining's auc: 0.839151\ttraining's binary_logloss: 0.328986\n",
      "[619]\ttraining's auc: 0.839173\ttraining's binary_logloss: 0.328963\n",
      "[620]\ttraining's auc: 0.8392\ttraining's binary_logloss: 0.328938\n",
      "[621]\ttraining's auc: 0.839234\ttraining's binary_logloss: 0.328901\n",
      "[622]\ttraining's auc: 0.839261\ttraining's binary_logloss: 0.328878\n",
      "[623]\ttraining's auc: 0.839282\ttraining's binary_logloss: 0.328857\n",
      "[624]\ttraining's auc: 0.839303\ttraining's binary_logloss: 0.328832\n",
      "[625]\ttraining's auc: 0.839329\ttraining's binary_logloss: 0.328804\n",
      "[626]\ttraining's auc: 0.83935\ttraining's binary_logloss: 0.328785\n",
      "[627]\ttraining's auc: 0.839375\ttraining's binary_logloss: 0.328759\n",
      "[628]\ttraining's auc: 0.839394\ttraining's binary_logloss: 0.328741\n",
      "[629]\ttraining's auc: 0.839411\ttraining's binary_logloss: 0.328724\n",
      "[630]\ttraining's auc: 0.839431\ttraining's binary_logloss: 0.328704\n",
      "[631]\ttraining's auc: 0.839457\ttraining's binary_logloss: 0.32868\n",
      "[632]\ttraining's auc: 0.839482\ttraining's binary_logloss: 0.328659\n",
      "[633]\ttraining's auc: 0.839503\ttraining's binary_logloss: 0.328638\n",
      "[634]\ttraining's auc: 0.839526\ttraining's binary_logloss: 0.328616\n",
      "[635]\ttraining's auc: 0.839545\ttraining's binary_logloss: 0.328599\n",
      "[636]\ttraining's auc: 0.839567\ttraining's binary_logloss: 0.328576\n",
      "[637]\ttraining's auc: 0.839597\ttraining's binary_logloss: 0.328546\n",
      "[638]\ttraining's auc: 0.839617\ttraining's binary_logloss: 0.328527\n",
      "[639]\ttraining's auc: 0.839645\ttraining's binary_logloss: 0.328499\n",
      "[640]\ttraining's auc: 0.839671\ttraining's binary_logloss: 0.328472\n",
      "[641]\ttraining's auc: 0.839691\ttraining's binary_logloss: 0.328452\n",
      "[642]\ttraining's auc: 0.839711\ttraining's binary_logloss: 0.328433\n",
      "[643]\ttraining's auc: 0.839728\ttraining's binary_logloss: 0.328418\n",
      "[644]\ttraining's auc: 0.83975\ttraining's binary_logloss: 0.328396\n",
      "[645]\ttraining's auc: 0.839769\ttraining's binary_logloss: 0.328378\n",
      "[646]\ttraining's auc: 0.83979\ttraining's binary_logloss: 0.328358\n",
      "[647]\ttraining's auc: 0.839819\ttraining's binary_logloss: 0.328325\n",
      "[648]\ttraining's auc: 0.839839\ttraining's binary_logloss: 0.328306\n",
      "[649]\ttraining's auc: 0.839863\ttraining's binary_logloss: 0.328282\n",
      "[650]\ttraining's auc: 0.839886\ttraining's binary_logloss: 0.328261\n",
      "[651]\ttraining's auc: 0.839906\ttraining's binary_logloss: 0.328241\n",
      "[652]\ttraining's auc: 0.839924\ttraining's binary_logloss: 0.328223\n",
      "[653]\ttraining's auc: 0.839953\ttraining's binary_logloss: 0.328196\n",
      "[654]\ttraining's auc: 0.839972\ttraining's binary_logloss: 0.328178\n",
      "[655]\ttraining's auc: 0.84\ttraining's binary_logloss: 0.328149\n",
      "[656]\ttraining's auc: 0.840025\ttraining's binary_logloss: 0.328125\n",
      "[657]\ttraining's auc: 0.840042\ttraining's binary_logloss: 0.328108\n",
      "[658]\ttraining's auc: 0.840065\ttraining's binary_logloss: 0.328087\n",
      "[659]\ttraining's auc: 0.840087\ttraining's binary_logloss: 0.328066\n",
      "[660]\ttraining's auc: 0.840113\ttraining's binary_logloss: 0.328042\n",
      "[661]\ttraining's auc: 0.840134\ttraining's binary_logloss: 0.32802\n",
      "[662]\ttraining's auc: 0.840162\ttraining's binary_logloss: 0.327993\n",
      "[663]\ttraining's auc: 0.840187\ttraining's binary_logloss: 0.327964\n",
      "[664]\ttraining's auc: 0.84021\ttraining's binary_logloss: 0.327943\n",
      "[665]\ttraining's auc: 0.840233\ttraining's binary_logloss: 0.327921\n",
      "[666]\ttraining's auc: 0.840253\ttraining's binary_logloss: 0.327903\n",
      "[667]\ttraining's auc: 0.840269\ttraining's binary_logloss: 0.327887\n",
      "[668]\ttraining's auc: 0.840287\ttraining's binary_logloss: 0.327867\n",
      "[669]\ttraining's auc: 0.840305\ttraining's binary_logloss: 0.327851\n",
      "[670]\ttraining's auc: 0.840328\ttraining's binary_logloss: 0.327831\n",
      "[671]\ttraining's auc: 0.840345\ttraining's binary_logloss: 0.327814\n",
      "[672]\ttraining's auc: 0.840362\ttraining's binary_logloss: 0.327796\n",
      "[673]\ttraining's auc: 0.840382\ttraining's binary_logloss: 0.327777\n",
      "[674]\ttraining's auc: 0.840405\ttraining's binary_logloss: 0.327755\n",
      "[675]\ttraining's auc: 0.840426\ttraining's binary_logloss: 0.327736\n",
      "[676]\ttraining's auc: 0.84045\ttraining's binary_logloss: 0.327714\n",
      "[677]\ttraining's auc: 0.840475\ttraining's binary_logloss: 0.327689\n",
      "[678]\ttraining's auc: 0.840495\ttraining's binary_logloss: 0.327671\n",
      "[679]\ttraining's auc: 0.840512\ttraining's binary_logloss: 0.327653\n",
      "[680]\ttraining's auc: 0.840531\ttraining's binary_logloss: 0.327636\n",
      "[681]\ttraining's auc: 0.840551\ttraining's binary_logloss: 0.327613\n",
      "[682]\ttraining's auc: 0.840571\ttraining's binary_logloss: 0.327596\n",
      "[683]\ttraining's auc: 0.840592\ttraining's binary_logloss: 0.327576\n",
      "[684]\ttraining's auc: 0.84061\ttraining's binary_logloss: 0.327557\n",
      "[685]\ttraining's auc: 0.840633\ttraining's binary_logloss: 0.327533\n",
      "[686]\ttraining's auc: 0.840651\ttraining's binary_logloss: 0.327516\n",
      "[687]\ttraining's auc: 0.840671\ttraining's binary_logloss: 0.327496\n",
      "[688]\ttraining's auc: 0.840691\ttraining's binary_logloss: 0.327476\n",
      "[689]\ttraining's auc: 0.840717\ttraining's binary_logloss: 0.32745\n",
      "[690]\ttraining's auc: 0.840731\ttraining's binary_logloss: 0.327437\n",
      "[691]\ttraining's auc: 0.840749\ttraining's binary_logloss: 0.327419\n",
      "[692]\ttraining's auc: 0.840767\ttraining's binary_logloss: 0.3274\n",
      "[693]\ttraining's auc: 0.840782\ttraining's binary_logloss: 0.327385\n",
      "[694]\ttraining's auc: 0.840799\ttraining's binary_logloss: 0.327369\n",
      "[695]\ttraining's auc: 0.840813\ttraining's binary_logloss: 0.327357\n",
      "[696]\ttraining's auc: 0.840838\ttraining's binary_logloss: 0.327335\n",
      "[697]\ttraining's auc: 0.840857\ttraining's binary_logloss: 0.327315\n",
      "[698]\ttraining's auc: 0.840882\ttraining's binary_logloss: 0.327289\n",
      "[699]\ttraining's auc: 0.840899\ttraining's binary_logloss: 0.327274\n",
      "[700]\ttraining's auc: 0.840922\ttraining's binary_logloss: 0.32725\n",
      "[701]\ttraining's auc: 0.840953\ttraining's binary_logloss: 0.327222\n",
      "[702]\ttraining's auc: 0.840978\ttraining's binary_logloss: 0.327196\n",
      "[703]\ttraining's auc: 0.840995\ttraining's binary_logloss: 0.327182\n",
      "[704]\ttraining's auc: 0.841018\ttraining's binary_logloss: 0.327159\n",
      "[705]\ttraining's auc: 0.841039\ttraining's binary_logloss: 0.327139\n",
      "[706]\ttraining's auc: 0.841061\ttraining's binary_logloss: 0.327116\n",
      "[707]\ttraining's auc: 0.841081\ttraining's binary_logloss: 0.327099\n",
      "[708]\ttraining's auc: 0.841105\ttraining's binary_logloss: 0.327075\n",
      "[709]\ttraining's auc: 0.841124\ttraining's binary_logloss: 0.327057\n",
      "[710]\ttraining's auc: 0.841152\ttraining's binary_logloss: 0.32703\n",
      "[711]\ttraining's auc: 0.841172\ttraining's binary_logloss: 0.327013\n",
      "[712]\ttraining's auc: 0.841191\ttraining's binary_logloss: 0.326996\n",
      "[713]\ttraining's auc: 0.841209\ttraining's binary_logloss: 0.326979\n",
      "[714]\ttraining's auc: 0.841226\ttraining's binary_logloss: 0.326962\n",
      "[715]\ttraining's auc: 0.841241\ttraining's binary_logloss: 0.326948\n",
      "[716]\ttraining's auc: 0.841257\ttraining's binary_logloss: 0.326932\n",
      "[717]\ttraining's auc: 0.841273\ttraining's binary_logloss: 0.326916\n",
      "[718]\ttraining's auc: 0.841295\ttraining's binary_logloss: 0.326896\n",
      "[719]\ttraining's auc: 0.841321\ttraining's binary_logloss: 0.326874\n",
      "[720]\ttraining's auc: 0.841345\ttraining's binary_logloss: 0.326852\n",
      "[721]\ttraining's auc: 0.841368\ttraining's binary_logloss: 0.32683\n",
      "[722]\ttraining's auc: 0.841386\ttraining's binary_logloss: 0.326812\n",
      "[723]\ttraining's auc: 0.841406\ttraining's binary_logloss: 0.326793\n",
      "[724]\ttraining's auc: 0.841422\ttraining's binary_logloss: 0.326778\n",
      "[725]\ttraining's auc: 0.841437\ttraining's binary_logloss: 0.326762\n",
      "[726]\ttraining's auc: 0.84146\ttraining's binary_logloss: 0.32674\n",
      "[727]\ttraining's auc: 0.841485\ttraining's binary_logloss: 0.326715\n",
      "[728]\ttraining's auc: 0.841502\ttraining's binary_logloss: 0.326699\n",
      "[729]\ttraining's auc: 0.841518\ttraining's binary_logloss: 0.326683\n",
      "[730]\ttraining's auc: 0.841537\ttraining's binary_logloss: 0.326665\n",
      "[731]\ttraining's auc: 0.841558\ttraining's binary_logloss: 0.326644\n",
      "[732]\ttraining's auc: 0.841575\ttraining's binary_logloss: 0.32663\n",
      "[733]\ttraining's auc: 0.841589\ttraining's binary_logloss: 0.326616\n",
      "[734]\ttraining's auc: 0.841606\ttraining's binary_logloss: 0.3266\n",
      "[735]\ttraining's auc: 0.841629\ttraining's binary_logloss: 0.326579\n",
      "[736]\ttraining's auc: 0.841646\ttraining's binary_logloss: 0.326563\n",
      "[737]\ttraining's auc: 0.841669\ttraining's binary_logloss: 0.326541\n",
      "[738]\ttraining's auc: 0.841695\ttraining's binary_logloss: 0.326518\n",
      "[739]\ttraining's auc: 0.841709\ttraining's binary_logloss: 0.326504\n",
      "[740]\ttraining's auc: 0.841735\ttraining's binary_logloss: 0.326482\n",
      "[741]\ttraining's auc: 0.841758\ttraining's binary_logloss: 0.326462\n",
      "[742]\ttraining's auc: 0.841782\ttraining's binary_logloss: 0.326438\n",
      "[743]\ttraining's auc: 0.841802\ttraining's binary_logloss: 0.32642\n",
      "[744]\ttraining's auc: 0.84182\ttraining's binary_logloss: 0.326404\n",
      "[745]\ttraining's auc: 0.841837\ttraining's binary_logloss: 0.326386\n",
      "[746]\ttraining's auc: 0.841859\ttraining's binary_logloss: 0.326364\n",
      "[747]\ttraining's auc: 0.841871\ttraining's binary_logloss: 0.326352\n",
      "[748]\ttraining's auc: 0.84189\ttraining's binary_logloss: 0.326336\n",
      "[749]\ttraining's auc: 0.841906\ttraining's binary_logloss: 0.326322\n",
      "[750]\ttraining's auc: 0.841925\ttraining's binary_logloss: 0.326301\n",
      "[751]\ttraining's auc: 0.841947\ttraining's binary_logloss: 0.326278\n",
      "[752]\ttraining's auc: 0.841959\ttraining's binary_logloss: 0.326266\n",
      "[753]\ttraining's auc: 0.841975\ttraining's binary_logloss: 0.326251\n",
      "[754]\ttraining's auc: 0.841992\ttraining's binary_logloss: 0.326236\n",
      "[755]\ttraining's auc: 0.842009\ttraining's binary_logloss: 0.32622\n",
      "[756]\ttraining's auc: 0.842035\ttraining's binary_logloss: 0.326195\n",
      "[757]\ttraining's auc: 0.84206\ttraining's binary_logloss: 0.326171\n",
      "[758]\ttraining's auc: 0.842077\ttraining's binary_logloss: 0.326155\n",
      "[759]\ttraining's auc: 0.842094\ttraining's binary_logloss: 0.326139\n",
      "[760]\ttraining's auc: 0.842111\ttraining's binary_logloss: 0.326123\n",
      "[761]\ttraining's auc: 0.842131\ttraining's binary_logloss: 0.326102\n",
      "[762]\ttraining's auc: 0.842148\ttraining's binary_logloss: 0.326086\n",
      "[763]\ttraining's auc: 0.842163\ttraining's binary_logloss: 0.326071\n",
      "[764]\ttraining's auc: 0.842181\ttraining's binary_logloss: 0.326054\n",
      "[765]\ttraining's auc: 0.842194\ttraining's binary_logloss: 0.326041\n",
      "[766]\ttraining's auc: 0.842216\ttraining's binary_logloss: 0.326017\n",
      "[767]\ttraining's auc: 0.842231\ttraining's binary_logloss: 0.326002\n",
      "[768]\ttraining's auc: 0.842258\ttraining's binary_logloss: 0.325977\n",
      "[769]\ttraining's auc: 0.842273\ttraining's binary_logloss: 0.325961\n",
      "[770]\ttraining's auc: 0.842294\ttraining's binary_logloss: 0.325942\n",
      "[771]\ttraining's auc: 0.842307\ttraining's binary_logloss: 0.32593\n",
      "[772]\ttraining's auc: 0.842325\ttraining's binary_logloss: 0.325913\n",
      "[773]\ttraining's auc: 0.842339\ttraining's binary_logloss: 0.325899\n",
      "[774]\ttraining's auc: 0.842355\ttraining's binary_logloss: 0.325883\n",
      "[775]\ttraining's auc: 0.842373\ttraining's binary_logloss: 0.325867\n",
      "[776]\ttraining's auc: 0.842398\ttraining's binary_logloss: 0.325842\n",
      "[777]\ttraining's auc: 0.842415\ttraining's binary_logloss: 0.325827\n",
      "[778]\ttraining's auc: 0.842431\ttraining's binary_logloss: 0.325809\n",
      "[779]\ttraining's auc: 0.84245\ttraining's binary_logloss: 0.325793\n",
      "[780]\ttraining's auc: 0.842464\ttraining's binary_logloss: 0.32578\n",
      "[781]\ttraining's auc: 0.842478\ttraining's binary_logloss: 0.325767\n",
      "[782]\ttraining's auc: 0.842495\ttraining's binary_logloss: 0.325751\n",
      "[783]\ttraining's auc: 0.842512\ttraining's binary_logloss: 0.325736\n",
      "[784]\ttraining's auc: 0.84253\ttraining's binary_logloss: 0.325719\n",
      "[785]\ttraining's auc: 0.842551\ttraining's binary_logloss: 0.325699\n",
      "[786]\ttraining's auc: 0.842564\ttraining's binary_logloss: 0.325686\n",
      "[787]\ttraining's auc: 0.842582\ttraining's binary_logloss: 0.325669\n",
      "[788]\ttraining's auc: 0.842598\ttraining's binary_logloss: 0.325653\n",
      "[789]\ttraining's auc: 0.842611\ttraining's binary_logloss: 0.325639\n",
      "[790]\ttraining's auc: 0.842629\ttraining's binary_logloss: 0.325621\n",
      "[791]\ttraining's auc: 0.842649\ttraining's binary_logloss: 0.325601\n",
      "[792]\ttraining's auc: 0.842671\ttraining's binary_logloss: 0.325579\n",
      "[793]\ttraining's auc: 0.842685\ttraining's binary_logloss: 0.325567\n",
      "[794]\ttraining's auc: 0.842711\ttraining's binary_logloss: 0.325543\n",
      "[795]\ttraining's auc: 0.84273\ttraining's binary_logloss: 0.325526\n",
      "[796]\ttraining's auc: 0.842747\ttraining's binary_logloss: 0.32551\n",
      "[797]\ttraining's auc: 0.84276\ttraining's binary_logloss: 0.325498\n",
      "[798]\ttraining's auc: 0.842786\ttraining's binary_logloss: 0.325473\n",
      "[799]\ttraining's auc: 0.842804\ttraining's binary_logloss: 0.325457\n",
      "[800]\ttraining's auc: 0.842821\ttraining's binary_logloss: 0.325441\n",
      "[801]\ttraining's auc: 0.842833\ttraining's binary_logloss: 0.325428\n",
      "[802]\ttraining's auc: 0.842849\ttraining's binary_logloss: 0.325414\n",
      "[803]\ttraining's auc: 0.842864\ttraining's binary_logloss: 0.3254\n",
      "[804]\ttraining's auc: 0.842887\ttraining's binary_logloss: 0.325378\n",
      "[805]\ttraining's auc: 0.842899\ttraining's binary_logloss: 0.325365\n",
      "[806]\ttraining's auc: 0.842917\ttraining's binary_logloss: 0.325347\n",
      "[807]\ttraining's auc: 0.842935\ttraining's binary_logloss: 0.325328\n",
      "[808]\ttraining's auc: 0.842949\ttraining's binary_logloss: 0.325316\n",
      "[809]\ttraining's auc: 0.842964\ttraining's binary_logloss: 0.325303\n",
      "[810]\ttraining's auc: 0.842977\ttraining's binary_logloss: 0.32529\n",
      "[811]\ttraining's auc: 0.843001\ttraining's binary_logloss: 0.325265\n",
      "[812]\ttraining's auc: 0.843016\ttraining's binary_logloss: 0.325251\n",
      "[813]\ttraining's auc: 0.843037\ttraining's binary_logloss: 0.325231\n",
      "[814]\ttraining's auc: 0.843053\ttraining's binary_logloss: 0.325217\n",
      "[815]\ttraining's auc: 0.843074\ttraining's binary_logloss: 0.325199\n",
      "[816]\ttraining's auc: 0.84309\ttraining's binary_logloss: 0.325184\n",
      "[817]\ttraining's auc: 0.843108\ttraining's binary_logloss: 0.325168\n",
      "[818]\ttraining's auc: 0.843125\ttraining's binary_logloss: 0.325153\n",
      "[819]\ttraining's auc: 0.84314\ttraining's binary_logloss: 0.325139\n",
      "[820]\ttraining's auc: 0.843162\ttraining's binary_logloss: 0.325118\n",
      "[821]\ttraining's auc: 0.843175\ttraining's binary_logloss: 0.325106\n",
      "[822]\ttraining's auc: 0.843188\ttraining's binary_logloss: 0.325094\n",
      "[823]\ttraining's auc: 0.843202\ttraining's binary_logloss: 0.32508\n",
      "[824]\ttraining's auc: 0.843222\ttraining's binary_logloss: 0.325061\n",
      "[825]\ttraining's auc: 0.843242\ttraining's binary_logloss: 0.325039\n",
      "[826]\ttraining's auc: 0.843256\ttraining's binary_logloss: 0.325026\n",
      "[827]\ttraining's auc: 0.843269\ttraining's binary_logloss: 0.325015\n",
      "[828]\ttraining's auc: 0.843286\ttraining's binary_logloss: 0.324999\n",
      "[829]\ttraining's auc: 0.843317\ttraining's binary_logloss: 0.324967\n",
      "[830]\ttraining's auc: 0.84333\ttraining's binary_logloss: 0.324957\n",
      "[831]\ttraining's auc: 0.843343\ttraining's binary_logloss: 0.324943\n",
      "[832]\ttraining's auc: 0.843358\ttraining's binary_logloss: 0.324929\n",
      "[833]\ttraining's auc: 0.843372\ttraining's binary_logloss: 0.324915\n",
      "[834]\ttraining's auc: 0.843391\ttraining's binary_logloss: 0.324899\n",
      "[835]\ttraining's auc: 0.843405\ttraining's binary_logloss: 0.324886\n",
      "[836]\ttraining's auc: 0.843419\ttraining's binary_logloss: 0.324872\n",
      "[837]\ttraining's auc: 0.843435\ttraining's binary_logloss: 0.324858\n",
      "[838]\ttraining's auc: 0.843467\ttraining's binary_logloss: 0.324823\n",
      "[839]\ttraining's auc: 0.843481\ttraining's binary_logloss: 0.32481\n",
      "[840]\ttraining's auc: 0.843492\ttraining's binary_logloss: 0.3248\n",
      "[841]\ttraining's auc: 0.843506\ttraining's binary_logloss: 0.324787\n",
      "[842]\ttraining's auc: 0.843522\ttraining's binary_logloss: 0.324773\n",
      "[843]\ttraining's auc: 0.843543\ttraining's binary_logloss: 0.324753\n",
      "[844]\ttraining's auc: 0.843563\ttraining's binary_logloss: 0.32473\n",
      "[845]\ttraining's auc: 0.843575\ttraining's binary_logloss: 0.32472\n",
      "[846]\ttraining's auc: 0.84359\ttraining's binary_logloss: 0.324703\n",
      "[847]\ttraining's auc: 0.843605\ttraining's binary_logloss: 0.324689\n",
      "[848]\ttraining's auc: 0.84362\ttraining's binary_logloss: 0.324675\n",
      "[849]\ttraining's auc: 0.843633\ttraining's binary_logloss: 0.324663\n",
      "[850]\ttraining's auc: 0.843648\ttraining's binary_logloss: 0.324648\n",
      "[851]\ttraining's auc: 0.843661\ttraining's binary_logloss: 0.324636\n",
      "[852]\ttraining's auc: 0.84368\ttraining's binary_logloss: 0.324618\n",
      "[853]\ttraining's auc: 0.843698\ttraining's binary_logloss: 0.324604\n",
      "[854]\ttraining's auc: 0.843714\ttraining's binary_logloss: 0.324586\n",
      "[855]\ttraining's auc: 0.84373\ttraining's binary_logloss: 0.324571\n",
      "[856]\ttraining's auc: 0.843747\ttraining's binary_logloss: 0.324555\n",
      "[857]\ttraining's auc: 0.843759\ttraining's binary_logloss: 0.324543\n",
      "[858]\ttraining's auc: 0.843772\ttraining's binary_logloss: 0.324532\n",
      "[859]\ttraining's auc: 0.843785\ttraining's binary_logloss: 0.324522\n",
      "[860]\ttraining's auc: 0.843798\ttraining's binary_logloss: 0.32451\n",
      "[861]\ttraining's auc: 0.84381\ttraining's binary_logloss: 0.324498\n",
      "[862]\ttraining's auc: 0.843825\ttraining's binary_logloss: 0.324484\n",
      "[863]\ttraining's auc: 0.843838\ttraining's binary_logloss: 0.324471\n",
      "[864]\ttraining's auc: 0.84385\ttraining's binary_logloss: 0.32446\n",
      "[865]\ttraining's auc: 0.843866\ttraining's binary_logloss: 0.324446\n",
      "[866]\ttraining's auc: 0.843895\ttraining's binary_logloss: 0.324415\n",
      "[867]\ttraining's auc: 0.843913\ttraining's binary_logloss: 0.324398\n",
      "[868]\ttraining's auc: 0.843931\ttraining's binary_logloss: 0.32438\n",
      "[869]\ttraining's auc: 0.843948\ttraining's binary_logloss: 0.324364\n",
      "[870]\ttraining's auc: 0.843965\ttraining's binary_logloss: 0.324346\n",
      "[871]\ttraining's auc: 0.84398\ttraining's binary_logloss: 0.324331\n",
      "[872]\ttraining's auc: 0.843993\ttraining's binary_logloss: 0.324317\n",
      "[873]\ttraining's auc: 0.844008\ttraining's binary_logloss: 0.324304\n",
      "[874]\ttraining's auc: 0.844019\ttraining's binary_logloss: 0.324294\n",
      "[875]\ttraining's auc: 0.844035\ttraining's binary_logloss: 0.324279\n",
      "[876]\ttraining's auc: 0.844049\ttraining's binary_logloss: 0.324266\n",
      "[877]\ttraining's auc: 0.844063\ttraining's binary_logloss: 0.324252\n",
      "[878]\ttraining's auc: 0.84408\ttraining's binary_logloss: 0.324236\n",
      "[879]\ttraining's auc: 0.844093\ttraining's binary_logloss: 0.324222\n",
      "[880]\ttraining's auc: 0.844118\ttraining's binary_logloss: 0.324192\n",
      "[881]\ttraining's auc: 0.844132\ttraining's binary_logloss: 0.32418\n",
      "[882]\ttraining's auc: 0.844146\ttraining's binary_logloss: 0.324164\n",
      "[883]\ttraining's auc: 0.844159\ttraining's binary_logloss: 0.324151\n",
      "[884]\ttraining's auc: 0.844176\ttraining's binary_logloss: 0.324134\n",
      "[885]\ttraining's auc: 0.844193\ttraining's binary_logloss: 0.324119\n",
      "[886]\ttraining's auc: 0.844204\ttraining's binary_logloss: 0.324109\n",
      "[887]\ttraining's auc: 0.844218\ttraining's binary_logloss: 0.324095\n",
      "[888]\ttraining's auc: 0.844232\ttraining's binary_logloss: 0.324083\n",
      "[889]\ttraining's auc: 0.844251\ttraining's binary_logloss: 0.324064\n",
      "[890]\ttraining's auc: 0.844268\ttraining's binary_logloss: 0.324048\n",
      "[891]\ttraining's auc: 0.844289\ttraining's binary_logloss: 0.324026\n",
      "[892]\ttraining's auc: 0.844308\ttraining's binary_logloss: 0.324007\n",
      "[893]\ttraining's auc: 0.844325\ttraining's binary_logloss: 0.323991\n",
      "[894]\ttraining's auc: 0.844339\ttraining's binary_logloss: 0.323978\n",
      "[895]\ttraining's auc: 0.844349\ttraining's binary_logloss: 0.323967\n",
      "[896]\ttraining's auc: 0.844367\ttraining's binary_logloss: 0.32395\n",
      "[897]\ttraining's auc: 0.844387\ttraining's binary_logloss: 0.323929\n",
      "[898]\ttraining's auc: 0.844401\ttraining's binary_logloss: 0.323914\n",
      "[899]\ttraining's auc: 0.844418\ttraining's binary_logloss: 0.323899\n",
      "[900]\ttraining's auc: 0.844432\ttraining's binary_logloss: 0.323886\n",
      "[901]\ttraining's auc: 0.844453\ttraining's binary_logloss: 0.323868\n",
      "[902]\ttraining's auc: 0.844467\ttraining's binary_logloss: 0.323856\n",
      "[903]\ttraining's auc: 0.844483\ttraining's binary_logloss: 0.323842\n",
      "[904]\ttraining's auc: 0.844505\ttraining's binary_logloss: 0.323818\n",
      "[905]\ttraining's auc: 0.844537\ttraining's binary_logloss: 0.323784\n",
      "[906]\ttraining's auc: 0.844551\ttraining's binary_logloss: 0.323771\n",
      "[907]\ttraining's auc: 0.844565\ttraining's binary_logloss: 0.323755\n",
      "[908]\ttraining's auc: 0.844579\ttraining's binary_logloss: 0.323742\n",
      "[909]\ttraining's auc: 0.844591\ttraining's binary_logloss: 0.323731\n",
      "[910]\ttraining's auc: 0.844604\ttraining's binary_logloss: 0.32372\n",
      "[911]\ttraining's auc: 0.844617\ttraining's binary_logloss: 0.323709\n",
      "[912]\ttraining's auc: 0.844632\ttraining's binary_logloss: 0.323694\n",
      "[913]\ttraining's auc: 0.844645\ttraining's binary_logloss: 0.323683\n",
      "[914]\ttraining's auc: 0.844661\ttraining's binary_logloss: 0.323668\n",
      "[915]\ttraining's auc: 0.844678\ttraining's binary_logloss: 0.323654\n",
      "[916]\ttraining's auc: 0.844687\ttraining's binary_logloss: 0.323645\n",
      "[917]\ttraining's auc: 0.844705\ttraining's binary_logloss: 0.323629\n",
      "[918]\ttraining's auc: 0.844716\ttraining's binary_logloss: 0.323618\n",
      "[919]\ttraining's auc: 0.844728\ttraining's binary_logloss: 0.323606\n",
      "[920]\ttraining's auc: 0.844743\ttraining's binary_logloss: 0.323594\n",
      "[921]\ttraining's auc: 0.84476\ttraining's binary_logloss: 0.323578\n",
      "[922]\ttraining's auc: 0.844777\ttraining's binary_logloss: 0.32356\n",
      "[923]\ttraining's auc: 0.844795\ttraining's binary_logloss: 0.323543\n",
      "[924]\ttraining's auc: 0.84481\ttraining's binary_logloss: 0.323527\n",
      "[925]\ttraining's auc: 0.844836\ttraining's binary_logloss: 0.323499\n",
      "[926]\ttraining's auc: 0.844849\ttraining's binary_logloss: 0.323486\n",
      "[927]\ttraining's auc: 0.844862\ttraining's binary_logloss: 0.323475\n",
      "[928]\ttraining's auc: 0.844875\ttraining's binary_logloss: 0.323464\n",
      "[929]\ttraining's auc: 0.844884\ttraining's binary_logloss: 0.323455\n",
      "[930]\ttraining's auc: 0.844897\ttraining's binary_logloss: 0.323443\n",
      "[931]\ttraining's auc: 0.844912\ttraining's binary_logloss: 0.323427\n",
      "[932]\ttraining's auc: 0.844938\ttraining's binary_logloss: 0.323399\n",
      "[933]\ttraining's auc: 0.844949\ttraining's binary_logloss: 0.323388\n",
      "[934]\ttraining's auc: 0.844961\ttraining's binary_logloss: 0.323377\n",
      "[935]\ttraining's auc: 0.844979\ttraining's binary_logloss: 0.323359\n",
      "[936]\ttraining's auc: 0.844995\ttraining's binary_logloss: 0.323343\n",
      "[937]\ttraining's auc: 0.845009\ttraining's binary_logloss: 0.32333\n",
      "[938]\ttraining's auc: 0.845028\ttraining's binary_logloss: 0.323312\n",
      "[939]\ttraining's auc: 0.845042\ttraining's binary_logloss: 0.323299\n",
      "[940]\ttraining's auc: 0.845059\ttraining's binary_logloss: 0.323284\n",
      "[941]\ttraining's auc: 0.84508\ttraining's binary_logloss: 0.323264\n",
      "[942]\ttraining's auc: 0.845096\ttraining's binary_logloss: 0.32325\n",
      "[943]\ttraining's auc: 0.845109\ttraining's binary_logloss: 0.323239\n",
      "[944]\ttraining's auc: 0.845118\ttraining's binary_logloss: 0.32323\n",
      "[945]\ttraining's auc: 0.845129\ttraining's binary_logloss: 0.323219\n",
      "[946]\ttraining's auc: 0.84514\ttraining's binary_logloss: 0.323209\n",
      "[947]\ttraining's auc: 0.845154\ttraining's binary_logloss: 0.323195\n",
      "[948]\ttraining's auc: 0.845165\ttraining's binary_logloss: 0.323185\n",
      "[949]\ttraining's auc: 0.845176\ttraining's binary_logloss: 0.323175\n",
      "[950]\ttraining's auc: 0.845188\ttraining's binary_logloss: 0.323164\n",
      "[951]\ttraining's auc: 0.845202\ttraining's binary_logloss: 0.323151\n",
      "[952]\ttraining's auc: 0.845216\ttraining's binary_logloss: 0.323137\n",
      "[953]\ttraining's auc: 0.84523\ttraining's binary_logloss: 0.323124\n",
      "[954]\ttraining's auc: 0.845246\ttraining's binary_logloss: 0.323108\n",
      "[955]\ttraining's auc: 0.845261\ttraining's binary_logloss: 0.323095\n",
      "[956]\ttraining's auc: 0.845273\ttraining's binary_logloss: 0.323084\n",
      "[957]\ttraining's auc: 0.84529\ttraining's binary_logloss: 0.32307\n",
      "[958]\ttraining's auc: 0.84531\ttraining's binary_logloss: 0.323047\n",
      "[959]\ttraining's auc: 0.845325\ttraining's binary_logloss: 0.323033\n",
      "[960]\ttraining's auc: 0.84534\ttraining's binary_logloss: 0.323019\n",
      "[961]\ttraining's auc: 0.845349\ttraining's binary_logloss: 0.323011\n",
      "[962]\ttraining's auc: 0.845376\ttraining's binary_logloss: 0.32298\n",
      "[963]\ttraining's auc: 0.845391\ttraining's binary_logloss: 0.322965\n",
      "[964]\ttraining's auc: 0.845404\ttraining's binary_logloss: 0.322955\n",
      "[965]\ttraining's auc: 0.845422\ttraining's binary_logloss: 0.322937\n",
      "[966]\ttraining's auc: 0.845437\ttraining's binary_logloss: 0.322924\n",
      "[967]\ttraining's auc: 0.845453\ttraining's binary_logloss: 0.32291\n",
      "[968]\ttraining's auc: 0.845467\ttraining's binary_logloss: 0.322897\n",
      "[969]\ttraining's auc: 0.845481\ttraining's binary_logloss: 0.322882\n",
      "[970]\ttraining's auc: 0.845497\ttraining's binary_logloss: 0.322868\n",
      "[971]\ttraining's auc: 0.845509\ttraining's binary_logloss: 0.322857\n",
      "[972]\ttraining's auc: 0.84552\ttraining's binary_logloss: 0.322847\n",
      "[973]\ttraining's auc: 0.84553\ttraining's binary_logloss: 0.322838\n",
      "[974]\ttraining's auc: 0.845546\ttraining's binary_logloss: 0.322822\n",
      "[975]\ttraining's auc: 0.845565\ttraining's binary_logloss: 0.322802\n",
      "[976]\ttraining's auc: 0.845584\ttraining's binary_logloss: 0.322785\n",
      "[977]\ttraining's auc: 0.845599\ttraining's binary_logloss: 0.322771\n",
      "[978]\ttraining's auc: 0.845615\ttraining's binary_logloss: 0.322757\n",
      "[979]\ttraining's auc: 0.845627\ttraining's binary_logloss: 0.322744\n",
      "[980]\ttraining's auc: 0.845638\ttraining's binary_logloss: 0.322734\n",
      "[981]\ttraining's auc: 0.845654\ttraining's binary_logloss: 0.322718\n",
      "[982]\ttraining's auc: 0.845665\ttraining's binary_logloss: 0.322709\n",
      "[983]\ttraining's auc: 0.845678\ttraining's binary_logloss: 0.322697\n",
      "[984]\ttraining's auc: 0.845687\ttraining's binary_logloss: 0.322689\n",
      "[985]\ttraining's auc: 0.845699\ttraining's binary_logloss: 0.32268\n",
      "[986]\ttraining's auc: 0.845711\ttraining's binary_logloss: 0.322669\n",
      "[987]\ttraining's auc: 0.845731\ttraining's binary_logloss: 0.322651\n",
      "[988]\ttraining's auc: 0.845741\ttraining's binary_logloss: 0.32264\n",
      "[989]\ttraining's auc: 0.845756\ttraining's binary_logloss: 0.322625\n",
      "[990]\ttraining's auc: 0.845767\ttraining's binary_logloss: 0.322614\n",
      "[991]\ttraining's auc: 0.84578\ttraining's binary_logloss: 0.322603\n",
      "[992]\ttraining's auc: 0.845799\ttraining's binary_logloss: 0.322585\n",
      "[993]\ttraining's auc: 0.845808\ttraining's binary_logloss: 0.322577\n",
      "[994]\ttraining's auc: 0.845823\ttraining's binary_logloss: 0.322562\n",
      "[995]\ttraining's auc: 0.845847\ttraining's binary_logloss: 0.322538\n",
      "[996]\ttraining's auc: 0.845856\ttraining's binary_logloss: 0.322529\n",
      "[997]\ttraining's auc: 0.845872\ttraining's binary_logloss: 0.322514\n",
      "[998]\ttraining's auc: 0.845884\ttraining's binary_logloss: 0.322502\n",
      "[999]\ttraining's auc: 0.845894\ttraining's binary_logloss: 0.322493\n",
      "[1000]\ttraining's auc: 0.84591\ttraining's binary_logloss: 0.322478\n",
      "[1001]\ttraining's auc: 0.845922\ttraining's binary_logloss: 0.322467\n",
      "[1002]\ttraining's auc: 0.845938\ttraining's binary_logloss: 0.322451\n",
      "[1003]\ttraining's auc: 0.845948\ttraining's binary_logloss: 0.322441\n",
      "[1004]\ttraining's auc: 0.84596\ttraining's binary_logloss: 0.32243\n",
      "[1005]\ttraining's auc: 0.845973\ttraining's binary_logloss: 0.322419\n",
      "[1006]\ttraining's auc: 0.845985\ttraining's binary_logloss: 0.322408\n",
      "[1007]\ttraining's auc: 0.845996\ttraining's binary_logloss: 0.322398\n",
      "[1008]\ttraining's auc: 0.846015\ttraining's binary_logloss: 0.322381\n",
      "[1009]\ttraining's auc: 0.846026\ttraining's binary_logloss: 0.322371\n",
      "[1010]\ttraining's auc: 0.846037\ttraining's binary_logloss: 0.322359\n",
      "[1011]\ttraining's auc: 0.84605\ttraining's binary_logloss: 0.322346\n",
      "[1012]\ttraining's auc: 0.846061\ttraining's binary_logloss: 0.322335\n",
      "[1013]\ttraining's auc: 0.846076\ttraining's binary_logloss: 0.322321\n",
      "[1014]\ttraining's auc: 0.846095\ttraining's binary_logloss: 0.322303\n",
      "[1015]\ttraining's auc: 0.846112\ttraining's binary_logloss: 0.322287\n",
      "[1016]\ttraining's auc: 0.846124\ttraining's binary_logloss: 0.322275\n",
      "[1017]\ttraining's auc: 0.846134\ttraining's binary_logloss: 0.322265\n",
      "[1018]\ttraining's auc: 0.846147\ttraining's binary_logloss: 0.322253\n",
      "[1019]\ttraining's auc: 0.846159\ttraining's binary_logloss: 0.322244\n",
      "[1020]\ttraining's auc: 0.846172\ttraining's binary_logloss: 0.322232\n",
      "[1021]\ttraining's auc: 0.846182\ttraining's binary_logloss: 0.322224\n",
      "[1022]\ttraining's auc: 0.846197\ttraining's binary_logloss: 0.322208\n",
      "[1023]\ttraining's auc: 0.846211\ttraining's binary_logloss: 0.322197\n",
      "[1024]\ttraining's auc: 0.846225\ttraining's binary_logloss: 0.322185\n",
      "[1025]\ttraining's auc: 0.846239\ttraining's binary_logloss: 0.322173\n",
      "[1026]\ttraining's auc: 0.84625\ttraining's binary_logloss: 0.322163\n",
      "[1027]\ttraining's auc: 0.846269\ttraining's binary_logloss: 0.322145\n",
      "[1028]\ttraining's auc: 0.846279\ttraining's binary_logloss: 0.322136\n",
      "[1029]\ttraining's auc: 0.846291\ttraining's binary_logloss: 0.322126\n",
      "[1030]\ttraining's auc: 0.846308\ttraining's binary_logloss: 0.322112\n",
      "[1031]\ttraining's auc: 0.846319\ttraining's binary_logloss: 0.322102\n",
      "[1032]\ttraining's auc: 0.846332\ttraining's binary_logloss: 0.32209\n",
      "[1033]\ttraining's auc: 0.846344\ttraining's binary_logloss: 0.322079\n",
      "[1034]\ttraining's auc: 0.846354\ttraining's binary_logloss: 0.32207\n",
      "[1035]\ttraining's auc: 0.846367\ttraining's binary_logloss: 0.322055\n",
      "[1036]\ttraining's auc: 0.846376\ttraining's binary_logloss: 0.322048\n",
      "[1037]\ttraining's auc: 0.846387\ttraining's binary_logloss: 0.322038\n",
      "[1038]\ttraining's auc: 0.846399\ttraining's binary_logloss: 0.322027\n",
      "[1039]\ttraining's auc: 0.846412\ttraining's binary_logloss: 0.322012\n",
      "[1040]\ttraining's auc: 0.846423\ttraining's binary_logloss: 0.322003\n",
      "[1041]\ttraining's auc: 0.846434\ttraining's binary_logloss: 0.321992\n",
      "[1042]\ttraining's auc: 0.846451\ttraining's binary_logloss: 0.321976\n",
      "[1043]\ttraining's auc: 0.846464\ttraining's binary_logloss: 0.321965\n",
      "[1044]\ttraining's auc: 0.846479\ttraining's binary_logloss: 0.321951\n",
      "[1045]\ttraining's auc: 0.846493\ttraining's binary_logloss: 0.321936\n",
      "[1046]\ttraining's auc: 0.846506\ttraining's binary_logloss: 0.321924\n",
      "[1047]\ttraining's auc: 0.846515\ttraining's binary_logloss: 0.321915\n",
      "[1048]\ttraining's auc: 0.846524\ttraining's binary_logloss: 0.321907\n",
      "[1049]\ttraining's auc: 0.846537\ttraining's binary_logloss: 0.321896\n",
      "[1050]\ttraining's auc: 0.846558\ttraining's binary_logloss: 0.321871\n",
      "[1051]\ttraining's auc: 0.846572\ttraining's binary_logloss: 0.321856\n",
      "[1052]\ttraining's auc: 0.84659\ttraining's binary_logloss: 0.32184\n",
      "[1053]\ttraining's auc: 0.846602\ttraining's binary_logloss: 0.321829\n",
      "[1054]\ttraining's auc: 0.846613\ttraining's binary_logloss: 0.32182\n",
      "[1055]\ttraining's auc: 0.846624\ttraining's binary_logloss: 0.321809\n",
      "[1056]\ttraining's auc: 0.846633\ttraining's binary_logloss: 0.3218\n",
      "[1057]\ttraining's auc: 0.846645\ttraining's binary_logloss: 0.32179\n",
      "[1058]\ttraining's auc: 0.846658\ttraining's binary_logloss: 0.321778\n",
      "[1059]\ttraining's auc: 0.846668\ttraining's binary_logloss: 0.32177\n",
      "[1060]\ttraining's auc: 0.846678\ttraining's binary_logloss: 0.321761\n",
      "[1061]\ttraining's auc: 0.846689\ttraining's binary_logloss: 0.321751\n",
      "[1062]\ttraining's auc: 0.846699\ttraining's binary_logloss: 0.321743\n",
      "[1063]\ttraining's auc: 0.846712\ttraining's binary_logloss: 0.32173\n",
      "[1064]\ttraining's auc: 0.846726\ttraining's binary_logloss: 0.321718\n",
      "[1065]\ttraining's auc: 0.846741\ttraining's binary_logloss: 0.321703\n",
      "[1066]\ttraining's auc: 0.84675\ttraining's binary_logloss: 0.321694\n",
      "[1067]\ttraining's auc: 0.846761\ttraining's binary_logloss: 0.321683\n",
      "[1068]\ttraining's auc: 0.846775\ttraining's binary_logloss: 0.321671\n",
      "[1069]\ttraining's auc: 0.846787\ttraining's binary_logloss: 0.321661\n",
      "[1070]\ttraining's auc: 0.8468\ttraining's binary_logloss: 0.321646\n",
      "[1071]\ttraining's auc: 0.846811\ttraining's binary_logloss: 0.321637\n",
      "[1072]\ttraining's auc: 0.846823\ttraining's binary_logloss: 0.321625\n",
      "[1073]\ttraining's auc: 0.846835\ttraining's binary_logloss: 0.321613\n",
      "[1074]\ttraining's auc: 0.846845\ttraining's binary_logloss: 0.321605\n",
      "[1075]\ttraining's auc: 0.846858\ttraining's binary_logloss: 0.321594\n",
      "[1076]\ttraining's auc: 0.84687\ttraining's binary_logloss: 0.321584\n",
      "[1077]\ttraining's auc: 0.846881\ttraining's binary_logloss: 0.321574\n",
      "[1078]\ttraining's auc: 0.846896\ttraining's binary_logloss: 0.321557\n",
      "[1079]\ttraining's auc: 0.846908\ttraining's binary_logloss: 0.321548\n",
      "[1080]\ttraining's auc: 0.846917\ttraining's binary_logloss: 0.321538\n",
      "[1081]\ttraining's auc: 0.846926\ttraining's binary_logloss: 0.321531\n",
      "[1082]\ttraining's auc: 0.846936\ttraining's binary_logloss: 0.321522\n",
      "[1083]\ttraining's auc: 0.846947\ttraining's binary_logloss: 0.321513\n",
      "[1084]\ttraining's auc: 0.846962\ttraining's binary_logloss: 0.321496\n",
      "[1085]\ttraining's auc: 0.846975\ttraining's binary_logloss: 0.321482\n",
      "[1086]\ttraining's auc: 0.846986\ttraining's binary_logloss: 0.321472\n",
      "[1087]\ttraining's auc: 0.846997\ttraining's binary_logloss: 0.321461\n",
      "[1088]\ttraining's auc: 0.847013\ttraining's binary_logloss: 0.321444\n",
      "[1089]\ttraining's auc: 0.847025\ttraining's binary_logloss: 0.321433\n",
      "[1090]\ttraining's auc: 0.847038\ttraining's binary_logloss: 0.321422\n",
      "[1091]\ttraining's auc: 0.84705\ttraining's binary_logloss: 0.321411\n",
      "[1092]\ttraining's auc: 0.84706\ttraining's binary_logloss: 0.321402\n",
      "[1093]\ttraining's auc: 0.847075\ttraining's binary_logloss: 0.321387\n",
      "[1094]\ttraining's auc: 0.847087\ttraining's binary_logloss: 0.321375\n",
      "[1095]\ttraining's auc: 0.847101\ttraining's binary_logloss: 0.32136\n",
      "[1096]\ttraining's auc: 0.847112\ttraining's binary_logloss: 0.32135\n",
      "[1097]\ttraining's auc: 0.847134\ttraining's binary_logloss: 0.321329\n",
      "[1098]\ttraining's auc: 0.847143\ttraining's binary_logloss: 0.321321\n",
      "[1099]\ttraining's auc: 0.847152\ttraining's binary_logloss: 0.321312\n",
      "[1100]\ttraining's auc: 0.847165\ttraining's binary_logloss: 0.3213\n",
      "[1101]\ttraining's auc: 0.847173\ttraining's binary_logloss: 0.321292\n",
      "[1102]\ttraining's auc: 0.847187\ttraining's binary_logloss: 0.32128\n",
      "[1103]\ttraining's auc: 0.847196\ttraining's binary_logloss: 0.321272\n",
      "[1104]\ttraining's auc: 0.847207\ttraining's binary_logloss: 0.321263\n",
      "[1105]\ttraining's auc: 0.847222\ttraining's binary_logloss: 0.321246\n",
      "[1106]\ttraining's auc: 0.847236\ttraining's binary_logloss: 0.321233\n",
      "[1107]\ttraining's auc: 0.847248\ttraining's binary_logloss: 0.321222\n",
      "[1108]\ttraining's auc: 0.847258\ttraining's binary_logloss: 0.321214\n",
      "[1109]\ttraining's auc: 0.847278\ttraining's binary_logloss: 0.321194\n",
      "[1110]\ttraining's auc: 0.847289\ttraining's binary_logloss: 0.321186\n",
      "[1111]\ttraining's auc: 0.8473\ttraining's binary_logloss: 0.321176\n",
      "[1112]\ttraining's auc: 0.847312\ttraining's binary_logloss: 0.321166\n",
      "[1113]\ttraining's auc: 0.847325\ttraining's binary_logloss: 0.321153\n",
      "[1114]\ttraining's auc: 0.847336\ttraining's binary_logloss: 0.321141\n",
      "[1115]\ttraining's auc: 0.847347\ttraining's binary_logloss: 0.321131\n",
      "[1116]\ttraining's auc: 0.847358\ttraining's binary_logloss: 0.321122\n",
      "[1117]\ttraining's auc: 0.847371\ttraining's binary_logloss: 0.321109\n",
      "[1118]\ttraining's auc: 0.84738\ttraining's binary_logloss: 0.3211\n",
      "[1119]\ttraining's auc: 0.847391\ttraining's binary_logloss: 0.321089\n",
      "[1120]\ttraining's auc: 0.847403\ttraining's binary_logloss: 0.321077\n",
      "[1121]\ttraining's auc: 0.847415\ttraining's binary_logloss: 0.321067\n",
      "[1122]\ttraining's auc: 0.847428\ttraining's binary_logloss: 0.321054\n",
      "[1123]\ttraining's auc: 0.847439\ttraining's binary_logloss: 0.321044\n",
      "[1124]\ttraining's auc: 0.84745\ttraining's binary_logloss: 0.321035\n",
      "[1125]\ttraining's auc: 0.847461\ttraining's binary_logloss: 0.321025\n",
      "[1126]\ttraining's auc: 0.847476\ttraining's binary_logloss: 0.321007\n",
      "[1127]\ttraining's auc: 0.847491\ttraining's binary_logloss: 0.320993\n",
      "[1128]\ttraining's auc: 0.847501\ttraining's binary_logloss: 0.320984\n",
      "[1129]\ttraining's auc: 0.847514\ttraining's binary_logloss: 0.320973\n",
      "[1130]\ttraining's auc: 0.847523\ttraining's binary_logloss: 0.320964\n",
      "[1131]\ttraining's auc: 0.847536\ttraining's binary_logloss: 0.320951\n",
      "[1132]\ttraining's auc: 0.847544\ttraining's binary_logloss: 0.320944\n",
      "[1133]\ttraining's auc: 0.84756\ttraining's binary_logloss: 0.32093\n",
      "[1134]\ttraining's auc: 0.847574\ttraining's binary_logloss: 0.320916\n",
      "[1135]\ttraining's auc: 0.847587\ttraining's binary_logloss: 0.320905\n",
      "[1136]\ttraining's auc: 0.847599\ttraining's binary_logloss: 0.320893\n",
      "[1137]\ttraining's auc: 0.847616\ttraining's binary_logloss: 0.320877\n",
      "[1138]\ttraining's auc: 0.847625\ttraining's binary_logloss: 0.320869\n",
      "[1139]\ttraining's auc: 0.847635\ttraining's binary_logloss: 0.320858\n",
      "[1140]\ttraining's auc: 0.847644\ttraining's binary_logloss: 0.32085\n",
      "[1141]\ttraining's auc: 0.847658\ttraining's binary_logloss: 0.320837\n",
      "[1142]\ttraining's auc: 0.847671\ttraining's binary_logloss: 0.320827\n",
      "[1143]\ttraining's auc: 0.847679\ttraining's binary_logloss: 0.320819\n",
      "[1144]\ttraining's auc: 0.847688\ttraining's binary_logloss: 0.320811\n",
      "[1145]\ttraining's auc: 0.847697\ttraining's binary_logloss: 0.320803\n",
      "[1146]\ttraining's auc: 0.847708\ttraining's binary_logloss: 0.320792\n",
      "[1147]\ttraining's auc: 0.847717\ttraining's binary_logloss: 0.320784\n",
      "[1148]\ttraining's auc: 0.847731\ttraining's binary_logloss: 0.32077\n",
      "[1149]\ttraining's auc: 0.847743\ttraining's binary_logloss: 0.320758\n",
      "[1150]\ttraining's auc: 0.847758\ttraining's binary_logloss: 0.32074\n",
      "[1151]\ttraining's auc: 0.847767\ttraining's binary_logloss: 0.320732\n",
      "[1152]\ttraining's auc: 0.847776\ttraining's binary_logloss: 0.320725\n",
      "[1153]\ttraining's auc: 0.847784\ttraining's binary_logloss: 0.320715\n",
      "[1154]\ttraining's auc: 0.847797\ttraining's binary_logloss: 0.320704\n",
      "[1155]\ttraining's auc: 0.847808\ttraining's binary_logloss: 0.320695\n",
      "[1156]\ttraining's auc: 0.847818\ttraining's binary_logloss: 0.320685\n",
      "[1157]\ttraining's auc: 0.84783\ttraining's binary_logloss: 0.320673\n",
      "[1158]\ttraining's auc: 0.84784\ttraining's binary_logloss: 0.320664\n",
      "[1159]\ttraining's auc: 0.847849\ttraining's binary_logloss: 0.320656\n",
      "[1160]\ttraining's auc: 0.847858\ttraining's binary_logloss: 0.320647\n",
      "[1161]\ttraining's auc: 0.847869\ttraining's binary_logloss: 0.320637\n",
      "[1162]\ttraining's auc: 0.847893\ttraining's binary_logloss: 0.320611\n",
      "[1163]\ttraining's auc: 0.847908\ttraining's binary_logloss: 0.320598\n",
      "[1164]\ttraining's auc: 0.847918\ttraining's binary_logloss: 0.320589\n",
      "[1165]\ttraining's auc: 0.847929\ttraining's binary_logloss: 0.32058\n",
      "[1166]\ttraining's auc: 0.847942\ttraining's binary_logloss: 0.320569\n",
      "[1167]\ttraining's auc: 0.847954\ttraining's binary_logloss: 0.320559\n",
      "[1168]\ttraining's auc: 0.847966\ttraining's binary_logloss: 0.320548\n",
      "[1169]\ttraining's auc: 0.847977\ttraining's binary_logloss: 0.320539\n",
      "[1170]\ttraining's auc: 0.847985\ttraining's binary_logloss: 0.320531\n",
      "[1171]\ttraining's auc: 0.847994\ttraining's binary_logloss: 0.320524\n",
      "[1172]\ttraining's auc: 0.848003\ttraining's binary_logloss: 0.320515\n",
      "[1173]\ttraining's auc: 0.848015\ttraining's binary_logloss: 0.320505\n",
      "[1174]\ttraining's auc: 0.848025\ttraining's binary_logloss: 0.320496\n",
      "[1175]\ttraining's auc: 0.848034\ttraining's binary_logloss: 0.320488\n",
      "[1176]\ttraining's auc: 0.848047\ttraining's binary_logloss: 0.320477\n",
      "[1177]\ttraining's auc: 0.848063\ttraining's binary_logloss: 0.320461\n",
      "[1178]\ttraining's auc: 0.848073\ttraining's binary_logloss: 0.320452\n",
      "[1179]\ttraining's auc: 0.848082\ttraining's binary_logloss: 0.320443\n",
      "[1180]\ttraining's auc: 0.848092\ttraining's binary_logloss: 0.320434\n",
      "[1181]\ttraining's auc: 0.848102\ttraining's binary_logloss: 0.320425\n",
      "[1182]\ttraining's auc: 0.848116\ttraining's binary_logloss: 0.320413\n",
      "[1183]\ttraining's auc: 0.848127\ttraining's binary_logloss: 0.320402\n",
      "[1184]\ttraining's auc: 0.848139\ttraining's binary_logloss: 0.320389\n",
      "[1185]\ttraining's auc: 0.84815\ttraining's binary_logloss: 0.320378\n",
      "[1186]\ttraining's auc: 0.848161\ttraining's binary_logloss: 0.320369\n",
      "[1187]\ttraining's auc: 0.848169\ttraining's binary_logloss: 0.320361\n",
      "[1188]\ttraining's auc: 0.848184\ttraining's binary_logloss: 0.320348\n",
      "[1189]\ttraining's auc: 0.848193\ttraining's binary_logloss: 0.32034\n",
      "[1190]\ttraining's auc: 0.848206\ttraining's binary_logloss: 0.320328\n",
      "[1191]\ttraining's auc: 0.848218\ttraining's binary_logloss: 0.320317\n",
      "[1192]\ttraining's auc: 0.848228\ttraining's binary_logloss: 0.320308\n",
      "[1193]\ttraining's auc: 0.848238\ttraining's binary_logloss: 0.320298\n",
      "[1194]\ttraining's auc: 0.848249\ttraining's binary_logloss: 0.320288\n",
      "[1195]\ttraining's auc: 0.848262\ttraining's binary_logloss: 0.320275\n",
      "[1196]\ttraining's auc: 0.848272\ttraining's binary_logloss: 0.320266\n",
      "[1197]\ttraining's auc: 0.848284\ttraining's binary_logloss: 0.320254\n",
      "[1198]\ttraining's auc: 0.848295\ttraining's binary_logloss: 0.320243\n",
      "[1199]\ttraining's auc: 0.848304\ttraining's binary_logloss: 0.320233\n",
      "[1200]\ttraining's auc: 0.848315\ttraining's binary_logloss: 0.320222\n",
      "[1201]\ttraining's auc: 0.848334\ttraining's binary_logloss: 0.320201\n",
      "[1202]\ttraining's auc: 0.848345\ttraining's binary_logloss: 0.32019\n",
      "[1203]\ttraining's auc: 0.848355\ttraining's binary_logloss: 0.320182\n",
      "[1204]\ttraining's auc: 0.848362\ttraining's binary_logloss: 0.320175\n",
      "[1205]\ttraining's auc: 0.848372\ttraining's binary_logloss: 0.320167\n",
      "[1206]\ttraining's auc: 0.848382\ttraining's binary_logloss: 0.320159\n",
      "[1207]\ttraining's auc: 0.848392\ttraining's binary_logloss: 0.32015\n",
      "[1208]\ttraining's auc: 0.848404\ttraining's binary_logloss: 0.320138\n",
      "[1209]\ttraining's auc: 0.848415\ttraining's binary_logloss: 0.320128\n",
      "[1210]\ttraining's auc: 0.848426\ttraining's binary_logloss: 0.320118\n",
      "[1211]\ttraining's auc: 0.848442\ttraining's binary_logloss: 0.320101\n",
      "[1212]\ttraining's auc: 0.848452\ttraining's binary_logloss: 0.320091\n",
      "[1213]\ttraining's auc: 0.848468\ttraining's binary_logloss: 0.320077\n",
      "[1214]\ttraining's auc: 0.848479\ttraining's binary_logloss: 0.320067\n",
      "[1215]\ttraining's auc: 0.84849\ttraining's binary_logloss: 0.320057\n",
      "[1216]\ttraining's auc: 0.848499\ttraining's binary_logloss: 0.320048\n",
      "[1217]\ttraining's auc: 0.848509\ttraining's binary_logloss: 0.320039\n",
      "[1218]\ttraining's auc: 0.848518\ttraining's binary_logloss: 0.320031\n",
      "[1219]\ttraining's auc: 0.848527\ttraining's binary_logloss: 0.320022\n",
      "[1220]\ttraining's auc: 0.84854\ttraining's binary_logloss: 0.320011\n",
      "[1221]\ttraining's auc: 0.848551\ttraining's binary_logloss: 0.32\n",
      "[1222]\ttraining's auc: 0.848559\ttraining's binary_logloss: 0.319993\n",
      "[1223]\ttraining's auc: 0.848569\ttraining's binary_logloss: 0.319985\n",
      "[1224]\ttraining's auc: 0.84858\ttraining's binary_logloss: 0.319975\n",
      "[1225]\ttraining's auc: 0.848588\ttraining's binary_logloss: 0.319968\n",
      "[1226]\ttraining's auc: 0.8486\ttraining's binary_logloss: 0.319956\n",
      "[1227]\ttraining's auc: 0.848613\ttraining's binary_logloss: 0.319945\n",
      "[1228]\ttraining's auc: 0.848621\ttraining's binary_logloss: 0.319936\n",
      "[1229]\ttraining's auc: 0.848635\ttraining's binary_logloss: 0.319923\n",
      "[1230]\ttraining's auc: 0.848643\ttraining's binary_logloss: 0.319916\n",
      "[1231]\ttraining's auc: 0.848655\ttraining's binary_logloss: 0.319906\n",
      "[1232]\ttraining's auc: 0.848667\ttraining's binary_logloss: 0.319895\n",
      "[1233]\ttraining's auc: 0.848682\ttraining's binary_logloss: 0.31988\n",
      "[1234]\ttraining's auc: 0.848693\ttraining's binary_logloss: 0.31987\n",
      "[1235]\ttraining's auc: 0.8487\ttraining's binary_logloss: 0.319864\n",
      "[1236]\ttraining's auc: 0.84871\ttraining's binary_logloss: 0.319854\n",
      "[1237]\ttraining's auc: 0.848717\ttraining's binary_logloss: 0.319848\n",
      "[1238]\ttraining's auc: 0.848733\ttraining's binary_logloss: 0.319835\n",
      "[1239]\ttraining's auc: 0.84874\ttraining's binary_logloss: 0.319829\n",
      "[1240]\ttraining's auc: 0.848748\ttraining's binary_logloss: 0.319822\n",
      "[1241]\ttraining's auc: 0.848759\ttraining's binary_logloss: 0.319812\n",
      "[1242]\ttraining's auc: 0.848767\ttraining's binary_logloss: 0.319804\n",
      "[1243]\ttraining's auc: 0.848777\ttraining's binary_logloss: 0.319795\n",
      "[1244]\ttraining's auc: 0.848787\ttraining's binary_logloss: 0.319785\n",
      "[1245]\ttraining's auc: 0.848795\ttraining's binary_logloss: 0.319779\n",
      "[1246]\ttraining's auc: 0.848802\ttraining's binary_logloss: 0.319772\n",
      "[1247]\ttraining's auc: 0.848812\ttraining's binary_logloss: 0.319764\n",
      "[1248]\ttraining's auc: 0.84882\ttraining's binary_logloss: 0.319756\n",
      "[1249]\ttraining's auc: 0.848832\ttraining's binary_logloss: 0.319745\n",
      "[1250]\ttraining's auc: 0.848844\ttraining's binary_logloss: 0.319733\n",
      "[1251]\ttraining's auc: 0.848857\ttraining's binary_logloss: 0.319721\n",
      "[1252]\ttraining's auc: 0.848866\ttraining's binary_logloss: 0.319711\n",
      "[1253]\ttraining's auc: 0.848876\ttraining's binary_logloss: 0.319699\n",
      "[1254]\ttraining's auc: 0.848888\ttraining's binary_logloss: 0.319686\n",
      "[1255]\ttraining's auc: 0.848896\ttraining's binary_logloss: 0.319679\n",
      "[1256]\ttraining's auc: 0.848903\ttraining's binary_logloss: 0.319671\n",
      "[1257]\ttraining's auc: 0.848913\ttraining's binary_logloss: 0.31966\n",
      "[1258]\ttraining's auc: 0.848928\ttraining's binary_logloss: 0.319644\n",
      "[1259]\ttraining's auc: 0.848942\ttraining's binary_logloss: 0.319631\n",
      "[1260]\ttraining's auc: 0.848952\ttraining's binary_logloss: 0.319621\n",
      "[1261]\ttraining's auc: 0.848963\ttraining's binary_logloss: 0.319611\n",
      "[1262]\ttraining's auc: 0.848971\ttraining's binary_logloss: 0.319603\n",
      "[1263]\ttraining's auc: 0.848981\ttraining's binary_logloss: 0.319594\n",
      "[1264]\ttraining's auc: 0.84899\ttraining's binary_logloss: 0.319586\n",
      "[1265]\ttraining's auc: 0.848998\ttraining's binary_logloss: 0.319579\n",
      "[1266]\ttraining's auc: 0.849007\ttraining's binary_logloss: 0.319571\n",
      "[1267]\ttraining's auc: 0.849017\ttraining's binary_logloss: 0.319562\n",
      "[1268]\ttraining's auc: 0.84903\ttraining's binary_logloss: 0.31955\n",
      "[1269]\ttraining's auc: 0.849038\ttraining's binary_logloss: 0.319543\n",
      "[1270]\ttraining's auc: 0.849052\ttraining's binary_logloss: 0.319526\n",
      "[1271]\ttraining's auc: 0.849062\ttraining's binary_logloss: 0.319517\n",
      "[1272]\ttraining's auc: 0.84907\ttraining's binary_logloss: 0.319508\n",
      "[1273]\ttraining's auc: 0.84908\ttraining's binary_logloss: 0.3195\n",
      "[1274]\ttraining's auc: 0.849093\ttraining's binary_logloss: 0.319489\n",
      "[1275]\ttraining's auc: 0.849101\ttraining's binary_logloss: 0.319482\n",
      "[1276]\ttraining's auc: 0.849108\ttraining's binary_logloss: 0.319475\n",
      "[1277]\ttraining's auc: 0.849118\ttraining's binary_logloss: 0.319466\n",
      "[1278]\ttraining's auc: 0.849128\ttraining's binary_logloss: 0.319457\n",
      "[1279]\ttraining's auc: 0.849138\ttraining's binary_logloss: 0.319448\n",
      "[1280]\ttraining's auc: 0.849144\ttraining's binary_logloss: 0.319442\n",
      "[1281]\ttraining's auc: 0.849153\ttraining's binary_logloss: 0.319434\n",
      "[1282]\ttraining's auc: 0.849161\ttraining's binary_logloss: 0.319427\n",
      "[1283]\ttraining's auc: 0.849168\ttraining's binary_logloss: 0.319421\n",
      "[1284]\ttraining's auc: 0.84918\ttraining's binary_logloss: 0.31941\n",
      "[1285]\ttraining's auc: 0.84919\ttraining's binary_logloss: 0.319401\n",
      "[1286]\ttraining's auc: 0.849197\ttraining's binary_logloss: 0.319394\n",
      "[1287]\ttraining's auc: 0.849204\ttraining's binary_logloss: 0.319388\n",
      "[1288]\ttraining's auc: 0.849213\ttraining's binary_logloss: 0.31938\n",
      "[1289]\ttraining's auc: 0.849222\ttraining's binary_logloss: 0.319371\n",
      "[1290]\ttraining's auc: 0.849231\ttraining's binary_logloss: 0.319363\n",
      "[1291]\ttraining's auc: 0.849239\ttraining's binary_logloss: 0.319356\n",
      "[1292]\ttraining's auc: 0.849253\ttraining's binary_logloss: 0.319344\n",
      "[1293]\ttraining's auc: 0.849261\ttraining's binary_logloss: 0.319337\n",
      "[1294]\ttraining's auc: 0.849271\ttraining's binary_logloss: 0.319327\n",
      "[1295]\ttraining's auc: 0.849279\ttraining's binary_logloss: 0.319321\n",
      "[1296]\ttraining's auc: 0.849287\ttraining's binary_logloss: 0.319314\n",
      "[1297]\ttraining's auc: 0.849294\ttraining's binary_logloss: 0.319307\n",
      "[1298]\ttraining's auc: 0.8493\ttraining's binary_logloss: 0.319301\n",
      "[1299]\ttraining's auc: 0.849319\ttraining's binary_logloss: 0.31928\n",
      "[1300]\ttraining's auc: 0.849328\ttraining's binary_logloss: 0.319272\n",
      "[1301]\ttraining's auc: 0.849339\ttraining's binary_logloss: 0.319262\n",
      "[1302]\ttraining's auc: 0.849349\ttraining's binary_logloss: 0.319253\n",
      "[1303]\ttraining's auc: 0.849362\ttraining's binary_logloss: 0.31924\n",
      "[1304]\ttraining's auc: 0.849371\ttraining's binary_logloss: 0.319232\n",
      "[1305]\ttraining's auc: 0.849385\ttraining's binary_logloss: 0.319219\n",
      "[1306]\ttraining's auc: 0.849404\ttraining's binary_logloss: 0.319199\n",
      "[1307]\ttraining's auc: 0.849414\ttraining's binary_logloss: 0.319189\n",
      "[1308]\ttraining's auc: 0.849423\ttraining's binary_logloss: 0.319182\n",
      "[1309]\ttraining's auc: 0.849432\ttraining's binary_logloss: 0.319174\n",
      "[1310]\ttraining's auc: 0.849446\ttraining's binary_logloss: 0.319158\n",
      "[1311]\ttraining's auc: 0.849457\ttraining's binary_logloss: 0.319146\n",
      "[1312]\ttraining's auc: 0.849465\ttraining's binary_logloss: 0.319138\n",
      "[1313]\ttraining's auc: 0.849475\ttraining's binary_logloss: 0.319129\n",
      "[1314]\ttraining's auc: 0.849483\ttraining's binary_logloss: 0.319123\n",
      "[1315]\ttraining's auc: 0.849491\ttraining's binary_logloss: 0.319115\n",
      "[1316]\ttraining's auc: 0.849501\ttraining's binary_logloss: 0.319107\n",
      "[1317]\ttraining's auc: 0.849509\ttraining's binary_logloss: 0.3191\n",
      "[1318]\ttraining's auc: 0.849518\ttraining's binary_logloss: 0.319091\n",
      "[1319]\ttraining's auc: 0.849526\ttraining's binary_logloss: 0.319084\n",
      "[1320]\ttraining's auc: 0.84954\ttraining's binary_logloss: 0.319073\n",
      "[1321]\ttraining's auc: 0.84955\ttraining's binary_logloss: 0.319064\n",
      "[1322]\ttraining's auc: 0.849558\ttraining's binary_logloss: 0.319054\n",
      "[1323]\ttraining's auc: 0.849567\ttraining's binary_logloss: 0.319046\n",
      "[1324]\ttraining's auc: 0.849579\ttraining's binary_logloss: 0.319032\n",
      "[1325]\ttraining's auc: 0.849589\ttraining's binary_logloss: 0.319023\n",
      "[1326]\ttraining's auc: 0.849597\ttraining's binary_logloss: 0.319015\n",
      "[1327]\ttraining's auc: 0.849603\ttraining's binary_logloss: 0.319009\n",
      "[1328]\ttraining's auc: 0.849614\ttraining's binary_logloss: 0.318999\n",
      "[1329]\ttraining's auc: 0.849621\ttraining's binary_logloss: 0.318993\n",
      "[1330]\ttraining's auc: 0.849629\ttraining's binary_logloss: 0.318985\n",
      "[1331]\ttraining's auc: 0.849637\ttraining's binary_logloss: 0.318978\n",
      "[1332]\ttraining's auc: 0.849651\ttraining's binary_logloss: 0.318964\n",
      "[1333]\ttraining's auc: 0.849662\ttraining's binary_logloss: 0.318953\n",
      "[1334]\ttraining's auc: 0.84967\ttraining's binary_logloss: 0.318945\n",
      "[1335]\ttraining's auc: 0.849679\ttraining's binary_logloss: 0.318936\n",
      "[1336]\ttraining's auc: 0.849687\ttraining's binary_logloss: 0.318929\n",
      "[1337]\ttraining's auc: 0.849705\ttraining's binary_logloss: 0.31891\n",
      "[1338]\ttraining's auc: 0.849713\ttraining's binary_logloss: 0.318903\n",
      "[1339]\ttraining's auc: 0.849722\ttraining's binary_logloss: 0.318895\n",
      "[1340]\ttraining's auc: 0.849729\ttraining's binary_logloss: 0.318889\n",
      "[1341]\ttraining's auc: 0.849737\ttraining's binary_logloss: 0.318882\n",
      "[1342]\ttraining's auc: 0.849747\ttraining's binary_logloss: 0.318872\n",
      "[1343]\ttraining's auc: 0.849756\ttraining's binary_logloss: 0.318862\n",
      "[1344]\ttraining's auc: 0.849769\ttraining's binary_logloss: 0.318852\n",
      "[1345]\ttraining's auc: 0.84978\ttraining's binary_logloss: 0.318842\n",
      "[1346]\ttraining's auc: 0.84979\ttraining's binary_logloss: 0.318833\n",
      "[1347]\ttraining's auc: 0.8498\ttraining's binary_logloss: 0.318824\n",
      "[1348]\ttraining's auc: 0.84981\ttraining's binary_logloss: 0.318814\n",
      "[1349]\ttraining's auc: 0.84982\ttraining's binary_logloss: 0.318805\n",
      "[1350]\ttraining's auc: 0.849828\ttraining's binary_logloss: 0.318798\n",
      "[1351]\ttraining's auc: 0.849836\ttraining's binary_logloss: 0.318791\n",
      "[1352]\ttraining's auc: 0.849844\ttraining's binary_logloss: 0.318785\n",
      "[1353]\ttraining's auc: 0.849852\ttraining's binary_logloss: 0.318778\n",
      "[1354]\ttraining's auc: 0.849861\ttraining's binary_logloss: 0.31877\n",
      "[1355]\ttraining's auc: 0.849871\ttraining's binary_logloss: 0.318761\n",
      "[1356]\ttraining's auc: 0.849879\ttraining's binary_logloss: 0.318753\n",
      "[1357]\ttraining's auc: 0.849887\ttraining's binary_logloss: 0.318747\n",
      "[1358]\ttraining's auc: 0.849896\ttraining's binary_logloss: 0.318738\n",
      "[1359]\ttraining's auc: 0.849904\ttraining's binary_logloss: 0.318731\n",
      "[1360]\ttraining's auc: 0.849914\ttraining's binary_logloss: 0.318721\n",
      "[1361]\ttraining's auc: 0.849923\ttraining's binary_logloss: 0.318713\n",
      "[1362]\ttraining's auc: 0.849933\ttraining's binary_logloss: 0.318703\n",
      "[1363]\ttraining's auc: 0.849941\ttraining's binary_logloss: 0.318696\n",
      "[1364]\ttraining's auc: 0.849948\ttraining's binary_logloss: 0.31869\n",
      "[1365]\ttraining's auc: 0.849959\ttraining's binary_logloss: 0.318679\n",
      "[1366]\ttraining's auc: 0.849967\ttraining's binary_logloss: 0.318672\n",
      "[1367]\ttraining's auc: 0.849976\ttraining's binary_logloss: 0.318663\n",
      "[1368]\ttraining's auc: 0.849983\ttraining's binary_logloss: 0.318656\n",
      "[1369]\ttraining's auc: 0.849992\ttraining's binary_logloss: 0.318648\n",
      "[1370]\ttraining's auc: 0.850003\ttraining's binary_logloss: 0.318637\n",
      "[1371]\ttraining's auc: 0.850011\ttraining's binary_logloss: 0.31863\n",
      "[1372]\ttraining's auc: 0.85002\ttraining's binary_logloss: 0.318622\n",
      "[1373]\ttraining's auc: 0.850031\ttraining's binary_logloss: 0.318611\n",
      "[1374]\ttraining's auc: 0.850046\ttraining's binary_logloss: 0.318596\n",
      "[1375]\ttraining's auc: 0.850054\ttraining's binary_logloss: 0.318588\n",
      "[1376]\ttraining's auc: 0.850061\ttraining's binary_logloss: 0.318581\n",
      "[1377]\ttraining's auc: 0.850069\ttraining's binary_logloss: 0.318574\n",
      "[1378]\ttraining's auc: 0.850078\ttraining's binary_logloss: 0.318566\n",
      "[1379]\ttraining's auc: 0.850087\ttraining's binary_logloss: 0.318558\n",
      "[1380]\ttraining's auc: 0.850097\ttraining's binary_logloss: 0.31855\n",
      "[1381]\ttraining's auc: 0.850103\ttraining's binary_logloss: 0.318544\n",
      "[1382]\ttraining's auc: 0.850114\ttraining's binary_logloss: 0.318534\n",
      "[1383]\ttraining's auc: 0.850121\ttraining's binary_logloss: 0.318528\n",
      "[1384]\ttraining's auc: 0.850129\ttraining's binary_logloss: 0.318521\n",
      "[1385]\ttraining's auc: 0.850137\ttraining's binary_logloss: 0.318515\n",
      "[1386]\ttraining's auc: 0.850148\ttraining's binary_logloss: 0.318504\n",
      "[1387]\ttraining's auc: 0.850155\ttraining's binary_logloss: 0.318497\n",
      "[1388]\ttraining's auc: 0.850167\ttraining's binary_logloss: 0.318485\n",
      "[1389]\ttraining's auc: 0.850178\ttraining's binary_logloss: 0.318473\n",
      "[1390]\ttraining's auc: 0.850189\ttraining's binary_logloss: 0.318464\n",
      "[1391]\ttraining's auc: 0.850196\ttraining's binary_logloss: 0.318457\n",
      "[1392]\ttraining's auc: 0.850204\ttraining's binary_logloss: 0.31845\n",
      "[1393]\ttraining's auc: 0.85021\ttraining's binary_logloss: 0.318444\n",
      "[1394]\ttraining's auc: 0.85022\ttraining's binary_logloss: 0.318435\n",
      "[1395]\ttraining's auc: 0.850228\ttraining's binary_logloss: 0.318427\n",
      "[1396]\ttraining's auc: 0.850239\ttraining's binary_logloss: 0.318417\n",
      "[1397]\ttraining's auc: 0.850247\ttraining's binary_logloss: 0.31841\n",
      "[1398]\ttraining's auc: 0.850257\ttraining's binary_logloss: 0.318401\n",
      "[1399]\ttraining's auc: 0.850265\ttraining's binary_logloss: 0.318393\n",
      "[1400]\ttraining's auc: 0.850274\ttraining's binary_logloss: 0.318385\n",
      "[1401]\ttraining's auc: 0.850282\ttraining's binary_logloss: 0.318378\n",
      "[1402]\ttraining's auc: 0.850291\ttraining's binary_logloss: 0.318371\n",
      "[1403]\ttraining's auc: 0.850302\ttraining's binary_logloss: 0.31836\n",
      "[1404]\ttraining's auc: 0.850313\ttraining's binary_logloss: 0.318348\n",
      "[1405]\ttraining's auc: 0.850321\ttraining's binary_logloss: 0.318341\n",
      "[1406]\ttraining's auc: 0.850329\ttraining's binary_logloss: 0.318334\n",
      "[1407]\ttraining's auc: 0.850336\ttraining's binary_logloss: 0.318327\n",
      "[1408]\ttraining's auc: 0.850345\ttraining's binary_logloss: 0.318319\n",
      "[1409]\ttraining's auc: 0.850356\ttraining's binary_logloss: 0.318307\n",
      "[1410]\ttraining's auc: 0.850366\ttraining's binary_logloss: 0.318298\n",
      "[1411]\ttraining's auc: 0.850373\ttraining's binary_logloss: 0.318291\n",
      "[1412]\ttraining's auc: 0.850381\ttraining's binary_logloss: 0.318284\n",
      "[1413]\ttraining's auc: 0.850392\ttraining's binary_logloss: 0.318272\n",
      "[1414]\ttraining's auc: 0.850401\ttraining's binary_logloss: 0.318263\n",
      "[1415]\ttraining's auc: 0.850409\ttraining's binary_logloss: 0.318257\n",
      "[1416]\ttraining's auc: 0.850415\ttraining's binary_logloss: 0.318251\n",
      "[1417]\ttraining's auc: 0.850425\ttraining's binary_logloss: 0.318239\n",
      "[1418]\ttraining's auc: 0.850434\ttraining's binary_logloss: 0.318232\n",
      "[1419]\ttraining's auc: 0.850441\ttraining's binary_logloss: 0.318225\n",
      "[1420]\ttraining's auc: 0.850448\ttraining's binary_logloss: 0.318218\n",
      "[1421]\ttraining's auc: 0.850456\ttraining's binary_logloss: 0.318212\n",
      "[1422]\ttraining's auc: 0.850463\ttraining's binary_logloss: 0.318205\n",
      "[1423]\ttraining's auc: 0.850471\ttraining's binary_logloss: 0.318198\n",
      "[1424]\ttraining's auc: 0.85048\ttraining's binary_logloss: 0.318189\n",
      "[1425]\ttraining's auc: 0.850487\ttraining's binary_logloss: 0.318182\n",
      "[1426]\ttraining's auc: 0.850495\ttraining's binary_logloss: 0.318176\n",
      "[1427]\ttraining's auc: 0.850502\ttraining's binary_logloss: 0.318168\n",
      "[1428]\ttraining's auc: 0.850511\ttraining's binary_logloss: 0.318161\n",
      "[1429]\ttraining's auc: 0.85052\ttraining's binary_logloss: 0.318153\n",
      "[1430]\ttraining's auc: 0.850528\ttraining's binary_logloss: 0.318146\n",
      "[1431]\ttraining's auc: 0.850536\ttraining's binary_logloss: 0.318139\n",
      "[1432]\ttraining's auc: 0.850547\ttraining's binary_logloss: 0.318127\n",
      "[1433]\ttraining's auc: 0.850554\ttraining's binary_logloss: 0.31812\n",
      "[1434]\ttraining's auc: 0.850562\ttraining's binary_logloss: 0.318113\n",
      "[1435]\ttraining's auc: 0.850568\ttraining's binary_logloss: 0.318107\n",
      "[1436]\ttraining's auc: 0.850575\ttraining's binary_logloss: 0.3181\n",
      "[1437]\ttraining's auc: 0.850586\ttraining's binary_logloss: 0.318091\n",
      "[1438]\ttraining's auc: 0.850592\ttraining's binary_logloss: 0.318086\n",
      "[1439]\ttraining's auc: 0.850603\ttraining's binary_logloss: 0.318076\n",
      "[1440]\ttraining's auc: 0.85061\ttraining's binary_logloss: 0.31807\n",
      "[1441]\ttraining's auc: 0.850618\ttraining's binary_logloss: 0.318064\n",
      "[1442]\ttraining's auc: 0.850626\ttraining's binary_logloss: 0.318057\n",
      "[1443]\ttraining's auc: 0.850634\ttraining's binary_logloss: 0.31805\n",
      "[1444]\ttraining's auc: 0.850643\ttraining's binary_logloss: 0.318042\n",
      "[1445]\ttraining's auc: 0.850653\ttraining's binary_logloss: 0.318034\n",
      "[1446]\ttraining's auc: 0.850662\ttraining's binary_logloss: 0.318025\n",
      "[1447]\ttraining's auc: 0.850672\ttraining's binary_logloss: 0.318016\n",
      "[1448]\ttraining's auc: 0.85068\ttraining's binary_logloss: 0.31801\n",
      "[1449]\ttraining's auc: 0.850694\ttraining's binary_logloss: 0.317994\n",
      "[1450]\ttraining's auc: 0.850702\ttraining's binary_logloss: 0.317987\n",
      "[1451]\ttraining's auc: 0.850713\ttraining's binary_logloss: 0.317978\n",
      "[1452]\ttraining's auc: 0.85072\ttraining's binary_logloss: 0.317971\n",
      "[1453]\ttraining's auc: 0.850727\ttraining's binary_logloss: 0.317966\n",
      "[1454]\ttraining's auc: 0.850737\ttraining's binary_logloss: 0.317954\n",
      "[1455]\ttraining's auc: 0.850745\ttraining's binary_logloss: 0.317948\n",
      "[1456]\ttraining's auc: 0.850754\ttraining's binary_logloss: 0.31794\n",
      "[1457]\ttraining's auc: 0.850762\ttraining's binary_logloss: 0.317932\n",
      "[1458]\ttraining's auc: 0.85077\ttraining's binary_logloss: 0.317925\n",
      "[1459]\ttraining's auc: 0.850777\ttraining's binary_logloss: 0.317919\n",
      "[1460]\ttraining's auc: 0.850783\ttraining's binary_logloss: 0.317913\n",
      "[1461]\ttraining's auc: 0.85079\ttraining's binary_logloss: 0.317908\n",
      "[1462]\ttraining's auc: 0.850799\ttraining's binary_logloss: 0.3179\n",
      "[1463]\ttraining's auc: 0.850806\ttraining's binary_logloss: 0.317894\n",
      "[1464]\ttraining's auc: 0.850814\ttraining's binary_logloss: 0.317887\n",
      "[1465]\ttraining's auc: 0.85082\ttraining's binary_logloss: 0.317882\n",
      "[1466]\ttraining's auc: 0.850827\ttraining's binary_logloss: 0.317876\n",
      "[1467]\ttraining's auc: 0.850835\ttraining's binary_logloss: 0.317869\n",
      "[1468]\ttraining's auc: 0.850843\ttraining's binary_logloss: 0.317862\n",
      "[1469]\ttraining's auc: 0.850852\ttraining's binary_logloss: 0.317855\n",
      "[1470]\ttraining's auc: 0.85086\ttraining's binary_logloss: 0.317847\n",
      "[1471]\ttraining's auc: 0.850871\ttraining's binary_logloss: 0.317837\n",
      "[1472]\ttraining's auc: 0.850878\ttraining's binary_logloss: 0.31783\n",
      "[1473]\ttraining's auc: 0.850889\ttraining's binary_logloss: 0.31782\n",
      "[1474]\ttraining's auc: 0.850898\ttraining's binary_logloss: 0.317811\n",
      "[1475]\ttraining's auc: 0.85091\ttraining's binary_logloss: 0.317799\n",
      "[1476]\ttraining's auc: 0.850916\ttraining's binary_logloss: 0.317793\n",
      "[1477]\ttraining's auc: 0.850923\ttraining's binary_logloss: 0.317788\n",
      "[1478]\ttraining's auc: 0.850929\ttraining's binary_logloss: 0.317782\n",
      "[1479]\ttraining's auc: 0.850937\ttraining's binary_logloss: 0.317776\n",
      "[1480]\ttraining's auc: 0.850945\ttraining's binary_logloss: 0.317767\n",
      "[1481]\ttraining's auc: 0.850956\ttraining's binary_logloss: 0.317757\n",
      "[1482]\ttraining's auc: 0.850963\ttraining's binary_logloss: 0.317751\n",
      "[1483]\ttraining's auc: 0.850969\ttraining's binary_logloss: 0.317744\n",
      "[1484]\ttraining's auc: 0.850978\ttraining's binary_logloss: 0.317736\n",
      "[1485]\ttraining's auc: 0.850986\ttraining's binary_logloss: 0.317729\n",
      "[1486]\ttraining's auc: 0.850993\ttraining's binary_logloss: 0.317722\n",
      "[1487]\ttraining's auc: 0.851001\ttraining's binary_logloss: 0.317714\n",
      "[1488]\ttraining's auc: 0.851008\ttraining's binary_logloss: 0.317704\n",
      "[1489]\ttraining's auc: 0.851015\ttraining's binary_logloss: 0.317698\n",
      "[1490]\ttraining's auc: 0.851026\ttraining's binary_logloss: 0.317688\n",
      "[1491]\ttraining's auc: 0.851035\ttraining's binary_logloss: 0.317681\n",
      "[1492]\ttraining's auc: 0.851043\ttraining's binary_logloss: 0.317675\n",
      "[1493]\ttraining's auc: 0.851051\ttraining's binary_logloss: 0.317668\n",
      "[1494]\ttraining's auc: 0.851057\ttraining's binary_logloss: 0.317662\n",
      "[1495]\ttraining's auc: 0.851065\ttraining's binary_logloss: 0.317655\n",
      "[1496]\ttraining's auc: 0.851072\ttraining's binary_logloss: 0.317649\n",
      "[1497]\ttraining's auc: 0.85108\ttraining's binary_logloss: 0.317641\n",
      "[1498]\ttraining's auc: 0.851091\ttraining's binary_logloss: 0.317629\n",
      "[1499]\ttraining's auc: 0.851097\ttraining's binary_logloss: 0.317624\n",
      "[1500]\ttraining's auc: 0.851104\ttraining's binary_logloss: 0.317617\n",
      "[1501]\ttraining's auc: 0.85111\ttraining's binary_logloss: 0.317612\n",
      "[1502]\ttraining's auc: 0.851119\ttraining's binary_logloss: 0.317603\n",
      "[1503]\ttraining's auc: 0.851126\ttraining's binary_logloss: 0.317597\n",
      "[1504]\ttraining's auc: 0.851133\ttraining's binary_logloss: 0.317591\n",
      "[1505]\ttraining's auc: 0.851142\ttraining's binary_logloss: 0.317583\n",
      "[1506]\ttraining's auc: 0.851151\ttraining's binary_logloss: 0.317575\n",
      "[1507]\ttraining's auc: 0.851158\ttraining's binary_logloss: 0.317569\n",
      "[1508]\ttraining's auc: 0.851165\ttraining's binary_logloss: 0.317563\n",
      "[1509]\ttraining's auc: 0.851174\ttraining's binary_logloss: 0.317555\n",
      "[1510]\ttraining's auc: 0.851183\ttraining's binary_logloss: 0.317545\n",
      "[1511]\ttraining's auc: 0.85119\ttraining's binary_logloss: 0.317539\n",
      "[1512]\ttraining's auc: 0.851199\ttraining's binary_logloss: 0.317531\n",
      "[1513]\ttraining's auc: 0.85121\ttraining's binary_logloss: 0.317519\n",
      "[1514]\ttraining's auc: 0.851219\ttraining's binary_logloss: 0.31751\n",
      "[1515]\ttraining's auc: 0.851227\ttraining's binary_logloss: 0.317503\n",
      "[1516]\ttraining's auc: 0.851235\ttraining's binary_logloss: 0.317495\n",
      "[1517]\ttraining's auc: 0.851246\ttraining's binary_logloss: 0.317485\n",
      "[1518]\ttraining's auc: 0.851258\ttraining's binary_logloss: 0.317474\n",
      "[1519]\ttraining's auc: 0.851264\ttraining's binary_logloss: 0.317468\n",
      "[1520]\ttraining's auc: 0.851272\ttraining's binary_logloss: 0.317461\n",
      "[1521]\ttraining's auc: 0.851283\ttraining's binary_logloss: 0.317449\n",
      "[1522]\ttraining's auc: 0.85129\ttraining's binary_logloss: 0.317443\n",
      "[1523]\ttraining's auc: 0.851298\ttraining's binary_logloss: 0.317436\n",
      "[1524]\ttraining's auc: 0.851305\ttraining's binary_logloss: 0.31743\n",
      "[1525]\ttraining's auc: 0.851312\ttraining's binary_logloss: 0.317423\n",
      "[1526]\ttraining's auc: 0.85132\ttraining's binary_logloss: 0.317418\n",
      "[1527]\ttraining's auc: 0.851326\ttraining's binary_logloss: 0.317411\n",
      "[1528]\ttraining's auc: 0.851335\ttraining's binary_logloss: 0.317401\n",
      "[1529]\ttraining's auc: 0.851345\ttraining's binary_logloss: 0.317393\n",
      "[1530]\ttraining's auc: 0.851352\ttraining's binary_logloss: 0.317387\n",
      "[1531]\ttraining's auc: 0.851358\ttraining's binary_logloss: 0.317381\n",
      "[1532]\ttraining's auc: 0.851367\ttraining's binary_logloss: 0.317373\n",
      "[1533]\ttraining's auc: 0.851376\ttraining's binary_logloss: 0.317365\n",
      "[1534]\ttraining's auc: 0.851383\ttraining's binary_logloss: 0.317359\n",
      "[1535]\ttraining's auc: 0.851392\ttraining's binary_logloss: 0.317351\n",
      "[1536]\ttraining's auc: 0.8514\ttraining's binary_logloss: 0.317342\n",
      "[1537]\ttraining's auc: 0.851409\ttraining's binary_logloss: 0.317334\n",
      "[1538]\ttraining's auc: 0.851415\ttraining's binary_logloss: 0.317329\n",
      "[1539]\ttraining's auc: 0.851422\ttraining's binary_logloss: 0.317322\n",
      "[1540]\ttraining's auc: 0.851432\ttraining's binary_logloss: 0.31731\n",
      "[1541]\ttraining's auc: 0.85144\ttraining's binary_logloss: 0.317303\n",
      "[1542]\ttraining's auc: 0.85145\ttraining's binary_logloss: 0.317292\n",
      "[1543]\ttraining's auc: 0.851458\ttraining's binary_logloss: 0.317284\n",
      "[1544]\ttraining's auc: 0.851464\ttraining's binary_logloss: 0.317278\n",
      "[1545]\ttraining's auc: 0.851475\ttraining's binary_logloss: 0.317267\n",
      "[1546]\ttraining's auc: 0.851481\ttraining's binary_logloss: 0.317261\n",
      "[1547]\ttraining's auc: 0.851488\ttraining's binary_logloss: 0.317255\n",
      "[1548]\ttraining's auc: 0.851496\ttraining's binary_logloss: 0.317245\n",
      "[1549]\ttraining's auc: 0.851504\ttraining's binary_logloss: 0.317238\n",
      "[1550]\ttraining's auc: 0.851511\ttraining's binary_logloss: 0.317233\n",
      "[1551]\ttraining's auc: 0.851518\ttraining's binary_logloss: 0.317226\n",
      "[1552]\ttraining's auc: 0.851531\ttraining's binary_logloss: 0.317215\n",
      "[1553]\ttraining's auc: 0.851538\ttraining's binary_logloss: 0.317208\n",
      "[1554]\ttraining's auc: 0.851546\ttraining's binary_logloss: 0.317202\n",
      "[1555]\ttraining's auc: 0.851555\ttraining's binary_logloss: 0.317193\n",
      "[1556]\ttraining's auc: 0.851561\ttraining's binary_logloss: 0.317188\n",
      "[1557]\ttraining's auc: 0.851568\ttraining's binary_logloss: 0.317182\n",
      "[1558]\ttraining's auc: 0.851575\ttraining's binary_logloss: 0.317175\n",
      "[1559]\ttraining's auc: 0.851582\ttraining's binary_logloss: 0.317169\n",
      "[1560]\ttraining's auc: 0.851589\ttraining's binary_logloss: 0.317163\n",
      "[1561]\ttraining's auc: 0.851598\ttraining's binary_logloss: 0.317156\n",
      "[1562]\ttraining's auc: 0.851605\ttraining's binary_logloss: 0.31715\n",
      "[1563]\ttraining's auc: 0.851611\ttraining's binary_logloss: 0.317145\n",
      "[1564]\ttraining's auc: 0.851618\ttraining's binary_logloss: 0.317139\n",
      "[1565]\ttraining's auc: 0.85163\ttraining's binary_logloss: 0.317129\n",
      "[1566]\ttraining's auc: 0.851638\ttraining's binary_logloss: 0.317122\n",
      "[1567]\ttraining's auc: 0.851644\ttraining's binary_logloss: 0.317117\n",
      "[1568]\ttraining's auc: 0.851653\ttraining's binary_logloss: 0.317108\n",
      "[1569]\ttraining's auc: 0.85166\ttraining's binary_logloss: 0.317101\n",
      "[1570]\ttraining's auc: 0.851667\ttraining's binary_logloss: 0.317094\n",
      "[1571]\ttraining's auc: 0.851677\ttraining's binary_logloss: 0.317085\n",
      "[1572]\ttraining's auc: 0.851682\ttraining's binary_logloss: 0.31708\n",
      "[1573]\ttraining's auc: 0.85169\ttraining's binary_logloss: 0.317074\n",
      "[1574]\ttraining's auc: 0.851696\ttraining's binary_logloss: 0.317069\n",
      "[1575]\ttraining's auc: 0.851703\ttraining's binary_logloss: 0.317063\n",
      "[1576]\ttraining's auc: 0.851709\ttraining's binary_logloss: 0.317058\n",
      "[1577]\ttraining's auc: 0.851717\ttraining's binary_logloss: 0.317051\n",
      "[1578]\ttraining's auc: 0.851725\ttraining's binary_logloss: 0.317043\n",
      "[1579]\ttraining's auc: 0.851734\ttraining's binary_logloss: 0.317035\n",
      "[1580]\ttraining's auc: 0.85174\ttraining's binary_logloss: 0.31703\n",
      "[1581]\ttraining's auc: 0.851748\ttraining's binary_logloss: 0.317023\n",
      "[1582]\ttraining's auc: 0.851755\ttraining's binary_logloss: 0.317017\n",
      "[1583]\ttraining's auc: 0.85176\ttraining's binary_logloss: 0.317011\n",
      "[1584]\ttraining's auc: 0.851768\ttraining's binary_logloss: 0.317004\n",
      "[1585]\ttraining's auc: 0.851775\ttraining's binary_logloss: 0.316999\n",
      "[1586]\ttraining's auc: 0.851782\ttraining's binary_logloss: 0.316993\n",
      "[1587]\ttraining's auc: 0.851793\ttraining's binary_logloss: 0.316983\n",
      "[1588]\ttraining's auc: 0.8518\ttraining's binary_logloss: 0.316977\n",
      "[1589]\ttraining's auc: 0.851807\ttraining's binary_logloss: 0.316971\n",
      "[1590]\ttraining's auc: 0.851812\ttraining's binary_logloss: 0.316965\n",
      "[1591]\ttraining's auc: 0.85182\ttraining's binary_logloss: 0.316958\n",
      "[1592]\ttraining's auc: 0.851828\ttraining's binary_logloss: 0.31695\n",
      "[1593]\ttraining's auc: 0.851836\ttraining's binary_logloss: 0.316943\n",
      "[1594]\ttraining's auc: 0.851843\ttraining's binary_logloss: 0.316938\n",
      "[1595]\ttraining's auc: 0.851852\ttraining's binary_logloss: 0.31693\n",
      "[1596]\ttraining's auc: 0.851857\ttraining's binary_logloss: 0.316925\n",
      "[1597]\ttraining's auc: 0.851867\ttraining's binary_logloss: 0.316917\n",
      "[1598]\ttraining's auc: 0.851874\ttraining's binary_logloss: 0.316909\n",
      "[1599]\ttraining's auc: 0.85188\ttraining's binary_logloss: 0.316904\n",
      "[1600]\ttraining's auc: 0.851886\ttraining's binary_logloss: 0.316898\n",
      "[1601]\ttraining's auc: 0.851893\ttraining's binary_logloss: 0.316892\n",
      "[1602]\ttraining's auc: 0.851901\ttraining's binary_logloss: 0.316886\n",
      "[1603]\ttraining's auc: 0.851909\ttraining's binary_logloss: 0.316878\n",
      "[1604]\ttraining's auc: 0.851917\ttraining's binary_logloss: 0.316871\n",
      "[1605]\ttraining's auc: 0.851925\ttraining's binary_logloss: 0.316863\n",
      "[1606]\ttraining's auc: 0.851932\ttraining's binary_logloss: 0.316857\n",
      "[1607]\ttraining's auc: 0.851938\ttraining's binary_logloss: 0.316851\n",
      "[1608]\ttraining's auc: 0.851944\ttraining's binary_logloss: 0.316846\n",
      "[1609]\ttraining's auc: 0.851955\ttraining's binary_logloss: 0.316835\n",
      "[1610]\ttraining's auc: 0.851962\ttraining's binary_logloss: 0.316829\n",
      "[1611]\ttraining's auc: 0.851975\ttraining's binary_logloss: 0.316814\n",
      "[1612]\ttraining's auc: 0.851982\ttraining's binary_logloss: 0.316807\n",
      "[1613]\ttraining's auc: 0.851989\ttraining's binary_logloss: 0.316801\n",
      "[1614]\ttraining's auc: 0.851993\ttraining's binary_logloss: 0.316795\n",
      "[1615]\ttraining's auc: 0.852006\ttraining's binary_logloss: 0.316782\n",
      "[1616]\ttraining's auc: 0.852015\ttraining's binary_logloss: 0.316774\n",
      "[1617]\ttraining's auc: 0.852023\ttraining's binary_logloss: 0.316767\n",
      "[1618]\ttraining's auc: 0.852029\ttraining's binary_logloss: 0.316762\n",
      "[1619]\ttraining's auc: 0.852034\ttraining's binary_logloss: 0.316757\n",
      "[1620]\ttraining's auc: 0.852045\ttraining's binary_logloss: 0.316745\n",
      "[1621]\ttraining's auc: 0.852052\ttraining's binary_logloss: 0.316739\n",
      "[1622]\ttraining's auc: 0.85206\ttraining's binary_logloss: 0.316732\n",
      "[1623]\ttraining's auc: 0.852065\ttraining's binary_logloss: 0.316727\n",
      "[1624]\ttraining's auc: 0.852072\ttraining's binary_logloss: 0.316721\n",
      "[1625]\ttraining's auc: 0.852079\ttraining's binary_logloss: 0.316715\n",
      "[1626]\ttraining's auc: 0.852085\ttraining's binary_logloss: 0.31671\n",
      "[1627]\ttraining's auc: 0.852092\ttraining's binary_logloss: 0.316704\n",
      "[1628]\ttraining's auc: 0.852103\ttraining's binary_logloss: 0.316693\n",
      "[1629]\ttraining's auc: 0.852111\ttraining's binary_logloss: 0.316686\n",
      "[1630]\ttraining's auc: 0.852116\ttraining's binary_logloss: 0.316681\n",
      "[1631]\ttraining's auc: 0.852123\ttraining's binary_logloss: 0.316675\n",
      "[1632]\ttraining's auc: 0.852129\ttraining's binary_logloss: 0.316669\n",
      "[1633]\ttraining's auc: 0.852137\ttraining's binary_logloss: 0.316661\n",
      "[1634]\ttraining's auc: 0.852144\ttraining's binary_logloss: 0.316656\n",
      "[1635]\ttraining's auc: 0.852161\ttraining's binary_logloss: 0.316636\n",
      "[1636]\ttraining's auc: 0.852168\ttraining's binary_logloss: 0.316631\n",
      "[1637]\ttraining's auc: 0.852175\ttraining's binary_logloss: 0.316625\n",
      "[1638]\ttraining's auc: 0.852185\ttraining's binary_logloss: 0.316616\n",
      "[1639]\ttraining's auc: 0.852195\ttraining's binary_logloss: 0.316607\n",
      "[1640]\ttraining's auc: 0.852203\ttraining's binary_logloss: 0.316599\n",
      "[1641]\ttraining's auc: 0.85221\ttraining's binary_logloss: 0.316592\n",
      "[1642]\ttraining's auc: 0.852217\ttraining's binary_logloss: 0.316586\n",
      "[1643]\ttraining's auc: 0.852222\ttraining's binary_logloss: 0.31658\n",
      "[1644]\ttraining's auc: 0.852234\ttraining's binary_logloss: 0.316568\n",
      "[1645]\ttraining's auc: 0.852241\ttraining's binary_logloss: 0.316562\n",
      "[1646]\ttraining's auc: 0.852248\ttraining's binary_logloss: 0.316556\n",
      "[1647]\ttraining's auc: 0.852253\ttraining's binary_logloss: 0.31655\n",
      "[1648]\ttraining's auc: 0.852261\ttraining's binary_logloss: 0.316544\n",
      "[1649]\ttraining's auc: 0.852268\ttraining's binary_logloss: 0.316537\n",
      "[1650]\ttraining's auc: 0.852275\ttraining's binary_logloss: 0.316532\n",
      "[1651]\ttraining's auc: 0.852282\ttraining's binary_logloss: 0.316526\n",
      "[1652]\ttraining's auc: 0.852291\ttraining's binary_logloss: 0.316518\n",
      "[1653]\ttraining's auc: 0.852299\ttraining's binary_logloss: 0.316511\n",
      "[1654]\ttraining's auc: 0.852304\ttraining's binary_logloss: 0.316505\n",
      "[1655]\ttraining's auc: 0.85231\ttraining's binary_logloss: 0.316499\n",
      "[1656]\ttraining's auc: 0.852317\ttraining's binary_logloss: 0.316493\n",
      "[1657]\ttraining's auc: 0.852325\ttraining's binary_logloss: 0.316485\n",
      "[1658]\ttraining's auc: 0.852331\ttraining's binary_logloss: 0.316481\n",
      "[1659]\ttraining's auc: 0.85234\ttraining's binary_logloss: 0.316473\n",
      "[1660]\ttraining's auc: 0.852348\ttraining's binary_logloss: 0.316466\n",
      "[1661]\ttraining's auc: 0.852355\ttraining's binary_logloss: 0.31646\n",
      "[1662]\ttraining's auc: 0.852365\ttraining's binary_logloss: 0.316451\n",
      "[1663]\ttraining's auc: 0.852375\ttraining's binary_logloss: 0.316442\n",
      "[1664]\ttraining's auc: 0.852381\ttraining's binary_logloss: 0.316436\n",
      "[1665]\ttraining's auc: 0.852386\ttraining's binary_logloss: 0.316431\n",
      "[1666]\ttraining's auc: 0.852395\ttraining's binary_logloss: 0.316421\n",
      "[1667]\ttraining's auc: 0.852402\ttraining's binary_logloss: 0.316415\n",
      "[1668]\ttraining's auc: 0.85241\ttraining's binary_logloss: 0.316407\n",
      "[1669]\ttraining's auc: 0.852417\ttraining's binary_logloss: 0.316402\n",
      "[1670]\ttraining's auc: 0.852425\ttraining's binary_logloss: 0.316395\n",
      "[1671]\ttraining's auc: 0.852432\ttraining's binary_logloss: 0.316388\n",
      "[1672]\ttraining's auc: 0.85244\ttraining's binary_logloss: 0.316381\n",
      "[1673]\ttraining's auc: 0.852445\ttraining's binary_logloss: 0.316377\n",
      "[1674]\ttraining's auc: 0.852452\ttraining's binary_logloss: 0.316371\n",
      "[1675]\ttraining's auc: 0.852458\ttraining's binary_logloss: 0.316366\n",
      "[1676]\ttraining's auc: 0.852465\ttraining's binary_logloss: 0.31636\n",
      "[1677]\ttraining's auc: 0.852473\ttraining's binary_logloss: 0.316352\n",
      "[1678]\ttraining's auc: 0.852481\ttraining's binary_logloss: 0.316345\n",
      "[1679]\ttraining's auc: 0.852489\ttraining's binary_logloss: 0.316338\n",
      "[1680]\ttraining's auc: 0.852499\ttraining's binary_logloss: 0.316328\n",
      "[1681]\ttraining's auc: 0.852507\ttraining's binary_logloss: 0.316321\n",
      "[1682]\ttraining's auc: 0.852514\ttraining's binary_logloss: 0.316315\n",
      "[1683]\ttraining's auc: 0.852519\ttraining's binary_logloss: 0.316309\n",
      "[1684]\ttraining's auc: 0.852524\ttraining's binary_logloss: 0.316305\n",
      "[1685]\ttraining's auc: 0.852531\ttraining's binary_logloss: 0.316299\n",
      "[1686]\ttraining's auc: 0.852539\ttraining's binary_logloss: 0.316291\n",
      "[1687]\ttraining's auc: 0.852545\ttraining's binary_logloss: 0.316286\n",
      "[1688]\ttraining's auc: 0.852553\ttraining's binary_logloss: 0.316279\n",
      "[1689]\ttraining's auc: 0.852559\ttraining's binary_logloss: 0.316274\n",
      "[1690]\ttraining's auc: 0.852566\ttraining's binary_logloss: 0.316268\n",
      "[1691]\ttraining's auc: 0.852572\ttraining's binary_logloss: 0.316263\n",
      "[1692]\ttraining's auc: 0.852579\ttraining's binary_logloss: 0.316256\n",
      "[1693]\ttraining's auc: 0.852586\ttraining's binary_logloss: 0.316251\n",
      "[1694]\ttraining's auc: 0.852593\ttraining's binary_logloss: 0.316245\n",
      "[1695]\ttraining's auc: 0.852599\ttraining's binary_logloss: 0.316239\n",
      "[1696]\ttraining's auc: 0.852608\ttraining's binary_logloss: 0.31623\n",
      "[1697]\ttraining's auc: 0.852615\ttraining's binary_logloss: 0.316222\n",
      "[1698]\ttraining's auc: 0.85262\ttraining's binary_logloss: 0.316218\n",
      "[1699]\ttraining's auc: 0.852625\ttraining's binary_logloss: 0.316213\n",
      "[1700]\ttraining's auc: 0.852631\ttraining's binary_logloss: 0.316207\n",
      "[1701]\ttraining's auc: 0.852648\ttraining's binary_logloss: 0.316186\n",
      "[1702]\ttraining's auc: 0.852657\ttraining's binary_logloss: 0.316179\n",
      "[1703]\ttraining's auc: 0.852662\ttraining's binary_logloss: 0.316173\n",
      "[1704]\ttraining's auc: 0.852673\ttraining's binary_logloss: 0.316163\n",
      "[1705]\ttraining's auc: 0.85268\ttraining's binary_logloss: 0.316157\n",
      "[1706]\ttraining's auc: 0.85269\ttraining's binary_logloss: 0.316147\n",
      "[1707]\ttraining's auc: 0.852696\ttraining's binary_logloss: 0.316141\n",
      "[1708]\ttraining's auc: 0.852702\ttraining's binary_logloss: 0.316136\n",
      "[1709]\ttraining's auc: 0.852708\ttraining's binary_logloss: 0.31613\n",
      "[1710]\ttraining's auc: 0.852713\ttraining's binary_logloss: 0.316125\n",
      "[1711]\ttraining's auc: 0.852718\ttraining's binary_logloss: 0.316121\n",
      "[1712]\ttraining's auc: 0.852724\ttraining's binary_logloss: 0.316116\n",
      "[1713]\ttraining's auc: 0.85273\ttraining's binary_logloss: 0.31611\n",
      "[1714]\ttraining's auc: 0.852742\ttraining's binary_logloss: 0.316099\n",
      "[1715]\ttraining's auc: 0.852747\ttraining's binary_logloss: 0.316094\n",
      "[1716]\ttraining's auc: 0.852754\ttraining's binary_logloss: 0.316088\n",
      "[1717]\ttraining's auc: 0.85276\ttraining's binary_logloss: 0.316083\n",
      "[1718]\ttraining's auc: 0.852769\ttraining's binary_logloss: 0.316074\n",
      "[1719]\ttraining's auc: 0.852775\ttraining's binary_logloss: 0.316069\n",
      "[1720]\ttraining's auc: 0.852785\ttraining's binary_logloss: 0.316062\n",
      "[1721]\ttraining's auc: 0.852791\ttraining's binary_logloss: 0.316056\n",
      "[1722]\ttraining's auc: 0.852798\ttraining's binary_logloss: 0.316049\n",
      "[1723]\ttraining's auc: 0.852807\ttraining's binary_logloss: 0.31604\n",
      "[1724]\ttraining's auc: 0.852813\ttraining's binary_logloss: 0.316034\n",
      "[1725]\ttraining's auc: 0.852821\ttraining's binary_logloss: 0.316025\n",
      "[1726]\ttraining's auc: 0.852829\ttraining's binary_logloss: 0.316018\n",
      "[1727]\ttraining's auc: 0.852839\ttraining's binary_logloss: 0.316011\n",
      "[1728]\ttraining's auc: 0.852847\ttraining's binary_logloss: 0.316004\n",
      "[1729]\ttraining's auc: 0.852854\ttraining's binary_logloss: 0.315998\n",
      "[1730]\ttraining's auc: 0.852862\ttraining's binary_logloss: 0.315991\n",
      "[1731]\ttraining's auc: 0.852871\ttraining's binary_logloss: 0.315983\n",
      "[1732]\ttraining's auc: 0.852878\ttraining's binary_logloss: 0.315977\n",
      "[1733]\ttraining's auc: 0.852885\ttraining's binary_logloss: 0.315972\n",
      "[1734]\ttraining's auc: 0.85289\ttraining's binary_logloss: 0.315968\n",
      "[1735]\ttraining's auc: 0.852896\ttraining's binary_logloss: 0.315963\n",
      "[1736]\ttraining's auc: 0.852902\ttraining's binary_logloss: 0.315957\n",
      "[1737]\ttraining's auc: 0.852907\ttraining's binary_logloss: 0.315952\n",
      "[1738]\ttraining's auc: 0.852913\ttraining's binary_logloss: 0.315947\n",
      "[1739]\ttraining's auc: 0.852919\ttraining's binary_logloss: 0.315941\n",
      "[1740]\ttraining's auc: 0.852928\ttraining's binary_logloss: 0.315931\n",
      "[1741]\ttraining's auc: 0.852935\ttraining's binary_logloss: 0.315926\n",
      "[1742]\ttraining's auc: 0.85294\ttraining's binary_logloss: 0.315922\n",
      "[1743]\ttraining's auc: 0.852945\ttraining's binary_logloss: 0.315917\n",
      "[1744]\ttraining's auc: 0.852952\ttraining's binary_logloss: 0.315912\n",
      "[1745]\ttraining's auc: 0.852959\ttraining's binary_logloss: 0.315904\n",
      "[1746]\ttraining's auc: 0.852967\ttraining's binary_logloss: 0.315897\n",
      "[1747]\ttraining's auc: 0.852972\ttraining's binary_logloss: 0.315893\n",
      "[1748]\ttraining's auc: 0.852981\ttraining's binary_logloss: 0.315884\n",
      "[1749]\ttraining's auc: 0.852992\ttraining's binary_logloss: 0.315873\n",
      "[1750]\ttraining's auc: 0.852998\ttraining's binary_logloss: 0.315868\n",
      "[1751]\ttraining's auc: 0.853004\ttraining's binary_logloss: 0.315863\n",
      "[1752]\ttraining's auc: 0.853009\ttraining's binary_logloss: 0.315857\n",
      "[1753]\ttraining's auc: 0.853014\ttraining's binary_logloss: 0.315853\n",
      "[1754]\ttraining's auc: 0.853023\ttraining's binary_logloss: 0.315845\n",
      "[1755]\ttraining's auc: 0.85303\ttraining's binary_logloss: 0.31584\n",
      "[1756]\ttraining's auc: 0.853036\ttraining's binary_logloss: 0.315834\n",
      "[1757]\ttraining's auc: 0.853042\ttraining's binary_logloss: 0.315829\n",
      "[1758]\ttraining's auc: 0.853048\ttraining's binary_logloss: 0.315823\n",
      "[1759]\ttraining's auc: 0.853054\ttraining's binary_logloss: 0.315817\n",
      "[1760]\ttraining's auc: 0.85306\ttraining's binary_logloss: 0.315812\n",
      "[1761]\ttraining's auc: 0.853066\ttraining's binary_logloss: 0.315807\n",
      "[1762]\ttraining's auc: 0.853072\ttraining's binary_logloss: 0.315801\n",
      "[1763]\ttraining's auc: 0.853078\ttraining's binary_logloss: 0.315796\n",
      "[1764]\ttraining's auc: 0.853085\ttraining's binary_logloss: 0.31579\n",
      "[1765]\ttraining's auc: 0.853091\ttraining's binary_logloss: 0.315786\n",
      "[1766]\ttraining's auc: 0.853097\ttraining's binary_logloss: 0.315779\n",
      "[1767]\ttraining's auc: 0.853103\ttraining's binary_logloss: 0.315774\n",
      "[1768]\ttraining's auc: 0.853109\ttraining's binary_logloss: 0.315768\n",
      "[1769]\ttraining's auc: 0.853115\ttraining's binary_logloss: 0.315763\n",
      "[1770]\ttraining's auc: 0.853121\ttraining's binary_logloss: 0.315757\n",
      "[1771]\ttraining's auc: 0.853126\ttraining's binary_logloss: 0.315752\n",
      "[1772]\ttraining's auc: 0.853132\ttraining's binary_logloss: 0.315747\n",
      "[1773]\ttraining's auc: 0.853138\ttraining's binary_logloss: 0.315742\n",
      "[1774]\ttraining's auc: 0.853144\ttraining's binary_logloss: 0.315736\n",
      "[1775]\ttraining's auc: 0.85315\ttraining's binary_logloss: 0.315731\n",
      "[1776]\ttraining's auc: 0.853157\ttraining's binary_logloss: 0.315724\n",
      "[1777]\ttraining's auc: 0.853163\ttraining's binary_logloss: 0.315718\n",
      "[1778]\ttraining's auc: 0.85317\ttraining's binary_logloss: 0.315711\n",
      "[1779]\ttraining's auc: 0.853178\ttraining's binary_logloss: 0.315704\n",
      "[1780]\ttraining's auc: 0.853184\ttraining's binary_logloss: 0.315699\n",
      "[1781]\ttraining's auc: 0.853189\ttraining's binary_logloss: 0.315694\n",
      "[1782]\ttraining's auc: 0.853195\ttraining's binary_logloss: 0.315689\n",
      "[1783]\ttraining's auc: 0.853202\ttraining's binary_logloss: 0.315682\n",
      "[1784]\ttraining's auc: 0.853209\ttraining's binary_logloss: 0.315676\n",
      "[1785]\ttraining's auc: 0.853214\ttraining's binary_logloss: 0.315672\n",
      "[1786]\ttraining's auc: 0.853219\ttraining's binary_logloss: 0.315667\n",
      "[1787]\ttraining's auc: 0.853226\ttraining's binary_logloss: 0.315659\n",
      "[1788]\ttraining's auc: 0.853231\ttraining's binary_logloss: 0.315655\n",
      "[1789]\ttraining's auc: 0.853238\ttraining's binary_logloss: 0.315649\n",
      "[1790]\ttraining's auc: 0.853248\ttraining's binary_logloss: 0.315638\n",
      "[1791]\ttraining's auc: 0.853253\ttraining's binary_logloss: 0.315633\n",
      "[1792]\ttraining's auc: 0.85326\ttraining's binary_logloss: 0.315628\n",
      "[1793]\ttraining's auc: 0.853264\ttraining's binary_logloss: 0.315624\n",
      "[1794]\ttraining's auc: 0.853271\ttraining's binary_logloss: 0.315618\n",
      "[1795]\ttraining's auc: 0.853277\ttraining's binary_logloss: 0.315613\n",
      "[1796]\ttraining's auc: 0.853283\ttraining's binary_logloss: 0.315608\n",
      "[1797]\ttraining's auc: 0.853288\ttraining's binary_logloss: 0.315603\n",
      "[1798]\ttraining's auc: 0.853296\ttraining's binary_logloss: 0.315596\n",
      "[1799]\ttraining's auc: 0.853302\ttraining's binary_logloss: 0.315589\n",
      "[1800]\ttraining's auc: 0.853309\ttraining's binary_logloss: 0.315583\n",
      "[1801]\ttraining's auc: 0.853314\ttraining's binary_logloss: 0.315579\n",
      "[1802]\ttraining's auc: 0.853319\ttraining's binary_logloss: 0.315575\n",
      "[1803]\ttraining's auc: 0.853326\ttraining's binary_logloss: 0.315568\n",
      "[1804]\ttraining's auc: 0.853334\ttraining's binary_logloss: 0.315561\n",
      "[1805]\ttraining's auc: 0.853339\ttraining's binary_logloss: 0.315557\n",
      "[1806]\ttraining's auc: 0.853346\ttraining's binary_logloss: 0.31555\n",
      "[1807]\ttraining's auc: 0.853353\ttraining's binary_logloss: 0.315544\n",
      "[1808]\ttraining's auc: 0.853358\ttraining's binary_logloss: 0.315539\n",
      "[1809]\ttraining's auc: 0.853363\ttraining's binary_logloss: 0.315535\n",
      "[1810]\ttraining's auc: 0.853368\ttraining's binary_logloss: 0.315531\n",
      "[1811]\ttraining's auc: 0.853377\ttraining's binary_logloss: 0.315523\n",
      "[1812]\ttraining's auc: 0.853382\ttraining's binary_logloss: 0.315518\n",
      "[1813]\ttraining's auc: 0.853389\ttraining's binary_logloss: 0.315511\n",
      "[1814]\ttraining's auc: 0.853393\ttraining's binary_logloss: 0.315507\n",
      "[1815]\ttraining's auc: 0.8534\ttraining's binary_logloss: 0.315501\n",
      "[1816]\ttraining's auc: 0.853406\ttraining's binary_logloss: 0.315496\n",
      "[1817]\ttraining's auc: 0.853411\ttraining's binary_logloss: 0.315491\n",
      "[1818]\ttraining's auc: 0.853417\ttraining's binary_logloss: 0.315486\n",
      "[1819]\ttraining's auc: 0.853424\ttraining's binary_logloss: 0.315481\n",
      "[1820]\ttraining's auc: 0.853429\ttraining's binary_logloss: 0.315475\n",
      "[1821]\ttraining's auc: 0.85344\ttraining's binary_logloss: 0.315464\n",
      "[1822]\ttraining's auc: 0.853445\ttraining's binary_logloss: 0.315459\n",
      "[1823]\ttraining's auc: 0.853451\ttraining's binary_logloss: 0.315454\n",
      "[1824]\ttraining's auc: 0.853456\ttraining's binary_logloss: 0.315449\n",
      "[1825]\ttraining's auc: 0.853463\ttraining's binary_logloss: 0.315443\n",
      "[1826]\ttraining's auc: 0.853468\ttraining's binary_logloss: 0.315439\n",
      "[1827]\ttraining's auc: 0.853475\ttraining's binary_logloss: 0.315433\n",
      "[1828]\ttraining's auc: 0.853483\ttraining's binary_logloss: 0.315426\n",
      "[1829]\ttraining's auc: 0.853491\ttraining's binary_logloss: 0.315417\n",
      "[1830]\ttraining's auc: 0.853497\ttraining's binary_logloss: 0.315413\n",
      "[1831]\ttraining's auc: 0.853504\ttraining's binary_logloss: 0.315405\n",
      "[1832]\ttraining's auc: 0.853509\ttraining's binary_logloss: 0.315401\n",
      "[1833]\ttraining's auc: 0.853514\ttraining's binary_logloss: 0.315396\n",
      "[1834]\ttraining's auc: 0.853519\ttraining's binary_logloss: 0.315391\n",
      "[1835]\ttraining's auc: 0.853526\ttraining's binary_logloss: 0.315384\n",
      "[1836]\ttraining's auc: 0.853532\ttraining's binary_logloss: 0.315378\n",
      "[1837]\ttraining's auc: 0.85354\ttraining's binary_logloss: 0.315372\n",
      "[1838]\ttraining's auc: 0.853545\ttraining's binary_logloss: 0.315367\n",
      "[1839]\ttraining's auc: 0.85355\ttraining's binary_logloss: 0.315362\n",
      "[1840]\ttraining's auc: 0.853557\ttraining's binary_logloss: 0.315355\n",
      "[1841]\ttraining's auc: 0.853561\ttraining's binary_logloss: 0.315351\n",
      "[1842]\ttraining's auc: 0.853566\ttraining's binary_logloss: 0.315346\n",
      "[1843]\ttraining's auc: 0.853577\ttraining's binary_logloss: 0.315337\n",
      "[1844]\ttraining's auc: 0.853583\ttraining's binary_logloss: 0.315332\n",
      "[1845]\ttraining's auc: 0.85359\ttraining's binary_logloss: 0.315325\n",
      "[1846]\ttraining's auc: 0.853596\ttraining's binary_logloss: 0.31532\n",
      "[1847]\ttraining's auc: 0.853602\ttraining's binary_logloss: 0.315315\n",
      "[1848]\ttraining's auc: 0.853609\ttraining's binary_logloss: 0.315308\n",
      "[1849]\ttraining's auc: 0.853616\ttraining's binary_logloss: 0.315303\n",
      "[1850]\ttraining's auc: 0.853622\ttraining's binary_logloss: 0.315299\n",
      "[1851]\ttraining's auc: 0.853629\ttraining's binary_logloss: 0.315293\n",
      "[1852]\ttraining's auc: 0.853634\ttraining's binary_logloss: 0.315288\n",
      "[1853]\ttraining's auc: 0.853639\ttraining's binary_logloss: 0.315283\n",
      "[1854]\ttraining's auc: 0.853649\ttraining's binary_logloss: 0.315274\n",
      "[1855]\ttraining's auc: 0.853657\ttraining's binary_logloss: 0.315266\n",
      "[1856]\ttraining's auc: 0.853662\ttraining's binary_logloss: 0.315262\n",
      "[1857]\ttraining's auc: 0.853668\ttraining's binary_logloss: 0.315256\n",
      "[1858]\ttraining's auc: 0.853674\ttraining's binary_logloss: 0.315251\n",
      "[1859]\ttraining's auc: 0.85368\ttraining's binary_logloss: 0.315244\n",
      "[1860]\ttraining's auc: 0.85369\ttraining's binary_logloss: 0.315233\n",
      "[1861]\ttraining's auc: 0.853696\ttraining's binary_logloss: 0.315227\n",
      "[1862]\ttraining's auc: 0.853702\ttraining's binary_logloss: 0.315223\n",
      "[1863]\ttraining's auc: 0.85371\ttraining's binary_logloss: 0.315215\n",
      "[1864]\ttraining's auc: 0.853714\ttraining's binary_logloss: 0.315211\n",
      "[1865]\ttraining's auc: 0.85372\ttraining's binary_logloss: 0.315206\n",
      "[1866]\ttraining's auc: 0.853727\ttraining's binary_logloss: 0.3152\n",
      "[1867]\ttraining's auc: 0.853733\ttraining's binary_logloss: 0.315193\n",
      "[1868]\ttraining's auc: 0.853739\ttraining's binary_logloss: 0.315188\n",
      "[1869]\ttraining's auc: 0.853743\ttraining's binary_logloss: 0.315185\n",
      "[1870]\ttraining's auc: 0.853748\ttraining's binary_logloss: 0.31518\n",
      "[1871]\ttraining's auc: 0.853755\ttraining's binary_logloss: 0.315173\n",
      "[1872]\ttraining's auc: 0.853762\ttraining's binary_logloss: 0.315166\n",
      "[1873]\ttraining's auc: 0.85377\ttraining's binary_logloss: 0.315159\n",
      "[1874]\ttraining's auc: 0.853774\ttraining's binary_logloss: 0.315156\n",
      "[1875]\ttraining's auc: 0.853778\ttraining's binary_logloss: 0.315151\n",
      "[1876]\ttraining's auc: 0.853787\ttraining's binary_logloss: 0.315144\n",
      "[1877]\ttraining's auc: 0.853792\ttraining's binary_logloss: 0.315139\n",
      "[1878]\ttraining's auc: 0.853798\ttraining's binary_logloss: 0.315135\n",
      "[1879]\ttraining's auc: 0.853804\ttraining's binary_logloss: 0.315129\n",
      "[1880]\ttraining's auc: 0.853809\ttraining's binary_logloss: 0.315125\n",
      "[1881]\ttraining's auc: 0.853816\ttraining's binary_logloss: 0.315119\n",
      "[1882]\ttraining's auc: 0.853821\ttraining's binary_logloss: 0.315114\n",
      "[1883]\ttraining's auc: 0.853826\ttraining's binary_logloss: 0.31511\n",
      "[1884]\ttraining's auc: 0.853833\ttraining's binary_logloss: 0.315104\n",
      "[1885]\ttraining's auc: 0.853838\ttraining's binary_logloss: 0.315099\n",
      "[1886]\ttraining's auc: 0.853843\ttraining's binary_logloss: 0.315095\n",
      "[1887]\ttraining's auc: 0.853849\ttraining's binary_logloss: 0.315089\n",
      "[1888]\ttraining's auc: 0.853856\ttraining's binary_logloss: 0.315082\n",
      "[1889]\ttraining's auc: 0.853864\ttraining's binary_logloss: 0.315075\n",
      "[1890]\ttraining's auc: 0.853868\ttraining's binary_logloss: 0.31507\n",
      "[1891]\ttraining's auc: 0.853874\ttraining's binary_logloss: 0.315065\n",
      "[1892]\ttraining's auc: 0.853881\ttraining's binary_logloss: 0.31506\n",
      "[1893]\ttraining's auc: 0.853885\ttraining's binary_logloss: 0.315056\n",
      "[1894]\ttraining's auc: 0.853891\ttraining's binary_logloss: 0.315048\n",
      "[1895]\ttraining's auc: 0.853897\ttraining's binary_logloss: 0.315043\n",
      "[1896]\ttraining's auc: 0.853902\ttraining's binary_logloss: 0.315038\n",
      "[1897]\ttraining's auc: 0.853911\ttraining's binary_logloss: 0.31503\n",
      "[1898]\ttraining's auc: 0.853916\ttraining's binary_logloss: 0.315025\n",
      "[1899]\ttraining's auc: 0.853922\ttraining's binary_logloss: 0.31502\n",
      "[1900]\ttraining's auc: 0.85393\ttraining's binary_logloss: 0.315014\n",
      "[1901]\ttraining's auc: 0.853937\ttraining's binary_logloss: 0.315008\n",
      "[1902]\ttraining's auc: 0.853944\ttraining's binary_logloss: 0.315002\n",
      "[1903]\ttraining's auc: 0.853948\ttraining's binary_logloss: 0.314998\n",
      "[1904]\ttraining's auc: 0.853953\ttraining's binary_logloss: 0.314994\n",
      "[1905]\ttraining's auc: 0.853958\ttraining's binary_logloss: 0.314989\n",
      "[1906]\ttraining's auc: 0.853962\ttraining's binary_logloss: 0.314985\n",
      "[1907]\ttraining's auc: 0.853968\ttraining's binary_logloss: 0.31498\n",
      "[1908]\ttraining's auc: 0.853977\ttraining's binary_logloss: 0.314969\n",
      "[1909]\ttraining's auc: 0.853982\ttraining's binary_logloss: 0.314964\n",
      "[1910]\ttraining's auc: 0.853989\ttraining's binary_logloss: 0.314958\n",
      "[1911]\ttraining's auc: 0.853994\ttraining's binary_logloss: 0.314952\n",
      "[1912]\ttraining's auc: 0.853999\ttraining's binary_logloss: 0.314948\n",
      "[1913]\ttraining's auc: 0.854005\ttraining's binary_logloss: 0.314944\n",
      "[1914]\ttraining's auc: 0.854011\ttraining's binary_logloss: 0.314939\n",
      "[1915]\ttraining's auc: 0.854018\ttraining's binary_logloss: 0.314933\n",
      "[1916]\ttraining's auc: 0.854025\ttraining's binary_logloss: 0.314925\n",
      "[1917]\ttraining's auc: 0.854032\ttraining's binary_logloss: 0.31492\n",
      "[1918]\ttraining's auc: 0.854041\ttraining's binary_logloss: 0.31491\n",
      "[1919]\ttraining's auc: 0.854047\ttraining's binary_logloss: 0.314905\n",
      "[1920]\ttraining's auc: 0.854055\ttraining's binary_logloss: 0.314898\n",
      "[1921]\ttraining's auc: 0.854063\ttraining's binary_logloss: 0.314891\n",
      "[1922]\ttraining's auc: 0.854069\ttraining's binary_logloss: 0.314887\n",
      "[1923]\ttraining's auc: 0.854076\ttraining's binary_logloss: 0.314881\n",
      "[1924]\ttraining's auc: 0.854083\ttraining's binary_logloss: 0.314875\n",
      "[1925]\ttraining's auc: 0.854088\ttraining's binary_logloss: 0.314871\n",
      "[1926]\ttraining's auc: 0.854094\ttraining's binary_logloss: 0.314866\n",
      "[1927]\ttraining's auc: 0.854101\ttraining's binary_logloss: 0.314859\n",
      "[1928]\ttraining's auc: 0.854106\ttraining's binary_logloss: 0.314854\n",
      "[1929]\ttraining's auc: 0.85411\ttraining's binary_logloss: 0.314851\n",
      "[1930]\ttraining's auc: 0.854115\ttraining's binary_logloss: 0.314846\n",
      "[1931]\ttraining's auc: 0.854119\ttraining's binary_logloss: 0.314842\n",
      "[1932]\ttraining's auc: 0.854123\ttraining's binary_logloss: 0.314839\n",
      "[1933]\ttraining's auc: 0.854128\ttraining's binary_logloss: 0.314834\n",
      "[1934]\ttraining's auc: 0.854131\ttraining's binary_logloss: 0.31483\n",
      "[1935]\ttraining's auc: 0.854136\ttraining's binary_logloss: 0.314826\n",
      "[1936]\ttraining's auc: 0.854141\ttraining's binary_logloss: 0.314822\n",
      "[1937]\ttraining's auc: 0.854146\ttraining's binary_logloss: 0.314818\n",
      "[1938]\ttraining's auc: 0.854152\ttraining's binary_logloss: 0.314813\n",
      "[1939]\ttraining's auc: 0.854158\ttraining's binary_logloss: 0.314806\n",
      "[1940]\ttraining's auc: 0.854162\ttraining's binary_logloss: 0.314802\n",
      "[1941]\ttraining's auc: 0.854169\ttraining's binary_logloss: 0.314797\n",
      "[1942]\ttraining's auc: 0.854174\ttraining's binary_logloss: 0.314792\n",
      "[1943]\ttraining's auc: 0.854181\ttraining's binary_logloss: 0.314786\n",
      "[1944]\ttraining's auc: 0.854187\ttraining's binary_logloss: 0.31478\n",
      "[1945]\ttraining's auc: 0.854194\ttraining's binary_logloss: 0.314774\n",
      "[1946]\ttraining's auc: 0.854199\ttraining's binary_logloss: 0.31477\n",
      "[1947]\ttraining's auc: 0.854204\ttraining's binary_logloss: 0.314765\n",
      "[1948]\ttraining's auc: 0.85421\ttraining's binary_logloss: 0.31476\n",
      "[1949]\ttraining's auc: 0.854219\ttraining's binary_logloss: 0.314752\n",
      "[1950]\ttraining's auc: 0.854223\ttraining's binary_logloss: 0.314748\n",
      "[1951]\ttraining's auc: 0.854227\ttraining's binary_logloss: 0.314744\n",
      "[1952]\ttraining's auc: 0.854234\ttraining's binary_logloss: 0.314738\n",
      "[1953]\ttraining's auc: 0.85424\ttraining's binary_logloss: 0.314733\n",
      "[1954]\ttraining's auc: 0.854246\ttraining's binary_logloss: 0.314729\n",
      "[1955]\ttraining's auc: 0.854252\ttraining's binary_logloss: 0.314723\n",
      "[1956]\ttraining's auc: 0.854256\ttraining's binary_logloss: 0.314719\n",
      "[1957]\ttraining's auc: 0.854261\ttraining's binary_logloss: 0.314715\n",
      "[1958]\ttraining's auc: 0.854266\ttraining's binary_logloss: 0.314709\n",
      "[1959]\ttraining's auc: 0.85427\ttraining's binary_logloss: 0.314706\n",
      "[1960]\ttraining's auc: 0.854274\ttraining's binary_logloss: 0.314702\n",
      "[1961]\ttraining's auc: 0.854279\ttraining's binary_logloss: 0.314697\n",
      "[1962]\ttraining's auc: 0.854285\ttraining's binary_logloss: 0.314691\n",
      "[1963]\ttraining's auc: 0.854292\ttraining's binary_logloss: 0.314683\n",
      "[1964]\ttraining's auc: 0.854299\ttraining's binary_logloss: 0.314677\n",
      "[1965]\ttraining's auc: 0.854304\ttraining's binary_logloss: 0.314673\n",
      "[1966]\ttraining's auc: 0.85431\ttraining's binary_logloss: 0.314667\n",
      "[1967]\ttraining's auc: 0.854316\ttraining's binary_logloss: 0.314663\n",
      "[1968]\ttraining's auc: 0.854322\ttraining's binary_logloss: 0.314658\n",
      "[1969]\ttraining's auc: 0.854327\ttraining's binary_logloss: 0.314654\n",
      "[1970]\ttraining's auc: 0.854331\ttraining's binary_logloss: 0.31465\n",
      "[1971]\ttraining's auc: 0.854335\ttraining's binary_logloss: 0.314646\n",
      "[1972]\ttraining's auc: 0.854341\ttraining's binary_logloss: 0.314641\n",
      "[1973]\ttraining's auc: 0.854347\ttraining's binary_logloss: 0.314636\n",
      "[1974]\ttraining's auc: 0.854352\ttraining's binary_logloss: 0.314631\n",
      "[1975]\ttraining's auc: 0.854356\ttraining's binary_logloss: 0.314628\n",
      "[1976]\ttraining's auc: 0.854362\ttraining's binary_logloss: 0.314623\n",
      "[1977]\ttraining's auc: 0.85437\ttraining's binary_logloss: 0.314615\n",
      "[1978]\ttraining's auc: 0.854375\ttraining's binary_logloss: 0.31461\n",
      "[1979]\ttraining's auc: 0.854384\ttraining's binary_logloss: 0.314602\n",
      "[1980]\ttraining's auc: 0.854388\ttraining's binary_logloss: 0.314598\n",
      "[1981]\ttraining's auc: 0.854394\ttraining's binary_logloss: 0.314594\n",
      "[1982]\ttraining's auc: 0.854402\ttraining's binary_logloss: 0.314586\n",
      "[1983]\ttraining's auc: 0.854408\ttraining's binary_logloss: 0.314582\n",
      "[1984]\ttraining's auc: 0.854414\ttraining's binary_logloss: 0.314576\n",
      "[1985]\ttraining's auc: 0.854418\ttraining's binary_logloss: 0.314572\n",
      "[1986]\ttraining's auc: 0.854424\ttraining's binary_logloss: 0.314568\n",
      "[1987]\ttraining's auc: 0.85443\ttraining's binary_logloss: 0.314562\n",
      "[1988]\ttraining's auc: 0.854436\ttraining's binary_logloss: 0.314556\n",
      "[1989]\ttraining's auc: 0.85444\ttraining's binary_logloss: 0.314552\n",
      "[1990]\ttraining's auc: 0.854444\ttraining's binary_logloss: 0.314549\n",
      "[1991]\ttraining's auc: 0.854449\ttraining's binary_logloss: 0.314545\n",
      "[1992]\ttraining's auc: 0.854456\ttraining's binary_logloss: 0.314538\n",
      "[1993]\ttraining's auc: 0.85446\ttraining's binary_logloss: 0.314534\n",
      "[1994]\ttraining's auc: 0.854466\ttraining's binary_logloss: 0.314528\n",
      "[1995]\ttraining's auc: 0.854474\ttraining's binary_logloss: 0.314522\n",
      "[1996]\ttraining's auc: 0.854479\ttraining's binary_logloss: 0.314517\n",
      "[1997]\ttraining's auc: 0.854485\ttraining's binary_logloss: 0.314512\n",
      "[1998]\ttraining's auc: 0.85449\ttraining's binary_logloss: 0.314507\n",
      "[1999]\ttraining's auc: 0.854495\ttraining's binary_logloss: 0.314502\n",
      "[2000]\ttraining's auc: 0.854504\ttraining's binary_logloss: 0.314494\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining's auc: 0.854504\ttraining's binary_logloss: 0.314494\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "params = {\n",
    "    'boosting_type':'gbdt','objective':'binary','metric':['auc', 'binary_logloss'],\n",
    "    'n_jobs':-1,'num_threads':30,'n_estimators':2000,'subsample':0.8,'subsample_freq':1,\n",
    "    'num_leaves':64,'learning_rate':0.01,\n",
    "    'max_bin':425,'subsample_for_bin':50000,'min_split_gain':0,\n",
    "    'min_child_weight':5,'min_child_samples':10,\n",
    "    'colsample_bytree':1,'reg_alpha':3,'reg_lambda':5,\n",
    "    'seed':1000,'silent':True    \n",
    "}\n",
    "lgb_train = lgb.Dataset(x_train,y_train,free_raw_data=False, )#, feature_name=train_secs.to_list(), categorical_feature=category_columns)\n",
    "gbm = lgb.train(params,lgb_train,valid_sets=lgb_train,early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7fac254fac50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.save_model('model7_r.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boost = lgb.Booster(model_file='base_model.txt')\n",
    "# y_pred = boost.predict(x_test)\n",
    "# test_df['is_answer'] = y_pred\n",
    "# test_df.to_csv('result.txt', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(x_test)\n",
    "test_df['is_answer'] = y_pred\n",
    "test_df.to_csv('result.txt', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('salt_value', 11473),\n",
       " ('question_idcount', 11454),\n",
       " ('user_idcount', 9945),\n",
       " ('answer_prob', 7779),\n",
       " ('invite_create_hour', 6649),\n",
       " ('user_answer_nums', 5319),\n",
       " ('question_answer_day', 5176),\n",
       " ('answer_create_day_user', 4514),\n",
       " ('invite_create_day', 3488),\n",
       " ('create_day', 3140),\n",
       " ('title_W_vec_his_max', 2653),\n",
       " ('describe_len', 2638),\n",
       " ('create_hour', 2566),\n",
       " ('topic_eucliddis', 2419),\n",
       " ('his_quesiont_dis_mean', 2331),\n",
       " ('title_SW_vec_his_max', 2318),\n",
       " ('question_prob', 2250),\n",
       " ('title_W_vec_his_mean', 1833),\n",
       " ('word_count_user', 1736),\n",
       " ('qt_len', 1627),\n",
       " ('his_quesiont_dis_max', 1615),\n",
       " ('topic_svd_dis', 1608),\n",
       " ('topic_cosdis', 1601),\n",
       " ('que|inter', 1482),\n",
       " ('num_agree_user', 1367),\n",
       " ('describe_W_vec_his_max', 1358),\n",
       " ('title_len', 1321),\n",
       " ('describe_len_user', 1301),\n",
       " ('title_len_user', 1258),\n",
       " ('question_answer_nums', 1230),\n",
       " ('que|fol|inter', 1202),\n",
       " ('num_thanks_user', 1199),\n",
       " ('title_SW_vec_his_mean', 1191),\n",
       " ('describe_W_vec_his_mean', 1050),\n",
       " ('word_count_question', 951),\n",
       " ('gender_label', 918),\n",
       " ('answer_create_day_question', 804),\n",
       " ('num_comment_user', 765),\n",
       " ('visit_freq_label', 701),\n",
       " ('num_agree_question', 590),\n",
       " ('num_favorite_user', 583),\n",
       " ('topic_dis', 541),\n",
       " ('has_picture_user', 540),\n",
       " ('visit_freq_labelcount', 533),\n",
       " ('category_C_labelcount', 522),\n",
       " ('num_favorite_question', 485),\n",
       " ('follow_len', 471),\n",
       " ('num_cancel_angree_question', 469),\n",
       " ('num_cancel_angree_user', 468),\n",
       " ('category_A_labelcount', 420),\n",
       " ('category_C_label', 414),\n",
       " ('num_comment_question', 403),\n",
       " ('category_D_label', 393),\n",
       " ('category_D_labelcount', 391),\n",
       " ('max_interest', 391),\n",
       " ('question_id_label', 364),\n",
       " ('user_id_label', 341),\n",
       " ('num_thanks_question', 331),\n",
       " ('category_B_label', 326),\n",
       " ('has_picture_question', 321),\n",
       " ('binary_D', 292),\n",
       " ('category_A_label', 251),\n",
       " ('total_interest', 246),\n",
       " ('que|fol', 219),\n",
       " ('category_B_labelcount', 215),\n",
       " ('num_useless_user', 198),\n",
       " ('category_E_label', 161),\n",
       " ('num_report_user', 160),\n",
       " ('binary_B', 136),\n",
       " ('has_video_question', 114),\n",
       " ('que&fol', 114),\n",
       " ('num_report_question', 107),\n",
       " ('binary_C', 75),\n",
       " ('binary_A', 50),\n",
       " ('is_good_question', 48),\n",
       " ('num_useless_question', 47),\n",
       " ('has_video_user', 19),\n",
       " ('is_good_user', 12),\n",
       " ('que&inter', 5),\n",
       " ('binary_E', 4),\n",
       " ('num_oppose_question', 0),\n",
       " ('num_oppose_user', 0),\n",
       " ('category_E_labelcount', 0),\n",
       " ('gender_labelcount', 0),\n",
       " ('binary_Acount', 0),\n",
       " ('binary_Bcount', 0),\n",
       " ('binary_Ccount', 0),\n",
       " ('binary_Dcount', 0),\n",
       " ('binary_Ecount', 0)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(zip(train_secs.to_list(),gbm.feature_importance())),key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_importances = sorted(list(zip(train_secs.to_list(),gbm.feature_importance())),key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question_idcount', 'salt_value', 'user_idcount', 'answer_prob', 'invite_create_hour', 'question_answer_day', 'user_answer_nums', 'invite_create_day', 'answer_create_day_user', 'create_day']\n"
     ]
    }
   ],
   "source": [
    "print([i[0] for i in feat_importances[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不加入id_count , 0.69\n",
    "# 加入user_id_count，question_id_count有明显提升,到 0.76\n",
    "# 数据预归一化处理\n",
    "# 加入 answer_prob 训练0.29，0.88，测试0.58,过拟合\n",
    "# 加入 question_count,answer_count 也是过拟合\n",
    "# 加入 top相关特征 训练0.74-0.79 提交0.77\n",
    "# 加入回答的统计特征'num_agree_user','word_count_user','num_agree_question','word_count_question','question_answer_nums'，'uesr_answer_nums'，训练0.87 测试0.75\n",
    "# 加入  均值填充nan 'num_agree_user','word_count_user','user_answer_nums'， 0.76-0.82， 0.46-0.35， 77.01 \n",
    "# 同上 0均值 77.3\n",
    "#  0均值 0.771 过拟合    'num_agree_user','word_count_user','user_answer_nums','num_agree_question' train's auc: 0.785558\ttrain's binary_logloss: 0.465657 (0.8465617931917354, 0.33882461713047074)\n",
    "#\tmean   0.75 过拟合 'num_agree_user','word_count_user','num_agree_question','word_count_question' train's auc: 0.81656\ttrain's binary_logloss: 0.465285 (0.8714833442600826, 0.316634370916078)\n",
    "# mean num_agree_user num_agree_question 0.76\n",
    "\n",
    "## \n",
    "# 计算answer_df时不使用最近10天的数据，训练0.79-0.88 提交79.5\n",
    "# 加入 answer_prob(-10), 使用（-10）数据 ，训练0.77-0.83 提交80.1\n",
    "# 使用-10 76-82 79.58\n",
    "# answer question prob （-10）0.77-0.83 78.45\n",
    "# answer question prob （-20） 数据-10  0.76-0.83 79.8\n",
    "# answer question prob （-20） 数据-20  0.78-0.86  79.8\n",
    "# 加入 新特征 answer(-10) 数据-10 0.77-0.839 78.4\n",
    "# 加入 新特征 answer(-20) 数据-10 0.77-0.831 80.0\n",
    "# 去除 answer question 数据 -10 80.55\n",
    "\n",
    "#retrain 前80.1 retrain后80.4\n",
    "# 加入 category_columns, 使用默认参数，训练0.75-0.85 79.9\n",
    "# 使用修改后的参数， 训练0.77-0.83 80.51\n",
    "# 加入 topic_cosdi -10 同上 80.53 似乎不用加入category_columns\n",
    "# 加入 topic_cosdis 全部数据 80.22\n",
    "# 同上 去除 category columns, 训练 0.77-0.83 80.56\n",
    "# 测试以下参数\n",
    "# 数据天数 -14 77-84 80.57\n",
    "# 数据天数 -7 76-83 80.63\n",
    "# 数据天数 -10 80.64\n",
    "#，预训练   7+7 1000,2000  80.80\n",
    "# 7+7+7 2000 80.856\n",
    "# 10 +7 2000 2000 80.85\n",
    "\n",
    "# 加入 answer_prob -7 -7 82.454\n",
    "# 加入 answer_prob -7 82.37\n",
    "# 加入 answer_prob question  -7 82.40\n",
    "# 加入 answer_prob question  -7 -7 -7 训练84.75 提交82.58\n",
    "# 加入欧式距离 未归一化 ，使用category -7 82.21\n",
    "# 加入欧式距离 归一化 ，-7 82.44\n",
    "# 加入话题距离 -7 82.61\n",
    "# 加入 category -7 82.43\n",
    "# 加 user_df和question_df的idcount -7 82.61\n",
    "# 加入标题信息 -7 82.71\n",
    "\n",
    "# gbdt categroy 处理方式\n",
    "# deepFM spares_feature\n",
    "# lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_lgb.predict_proba(x_test)[:, 1]\n",
    "test_df['is_answer'] = y_pred\n",
    "test_df.to_csv('result.txt', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构造特征\n",
    "# 问题，历史回答用户的信息平均值 \n",
    "# 用户，历史回答问题的特征\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_answer_praba = train_df[train_df['invite_create_day']<train_end-10][['user_id','is_answer']].groupby('user_id').mean().to_dict()['is_answer']\n",
    "data_df['answer_prob'][0:train_df.shape[0]] = data_df['user_id'][0:train_df.shape[0]].map(train_answer_praba)\n",
    "\n",
    "train_answer_praba = train_df[['user_id','is_answer']].groupby('user_id').mean().to_dict()['is_answer']\n",
    "data_df['answer_prob'][train_df.shape[0]:] = data_df['user_id'][train_df.shape[0]:].map(train_answer_praba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_answer_praba = train_df[train_df['invite_create_day']<train_end-10][['question_id','is_answer']].groupby('question_id').mean().to_dict()['is_answer']\n",
    "data_df['question_prob'][0:train_df.shape[0]] = data_df['question_id'][0:train_df.shape[0]].map(train_answer_praba)\n",
    "\n",
    "train_answer_praba = train_df[['question_id','is_answer']].groupby('question_id').mean().to_dict()['is_answer']\n",
    "data_df['question_prob'][train_df.shape[0]:] = data_df['question_id'][train_df.shape[0]:].map(train_answer_praba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep-fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepctr_torch.models import DeepFM,wdl\n",
    "from deepctr_torch.inputs import  SparseFeat, DenseFeat,get_feature_names\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = lgb.Booster(model_file='base_model.txt')\n",
    "train_gbdt_feat = boost.predict(x_train,pred_leaf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features = [str(i) for i in range(1000)]\n",
    "# sparse_features = []\n",
    "dense_features = ['question_idcount', 'salt_value', 'user_idcount', 'answer_prob', 'invite_create_hour', 'question_answer_day', 'user_answer_nums', 'invite_create_day', 'answer_create_day_user', 'create_day']\n",
    "dense_pos = [train_secs.get_loc(sec) for sec in dense_features]\n",
    "fixlen_feature_columns = [SparseFeat(feat, 64) for feat in sparse_features] + [DenseFeat(feat, 1,) for feat in dense_features]\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "feature_names = get_feature_names(\n",
    "    linear_feature_columns+ dnn_feature_columns)\n",
    "\n",
    "train_model_input={}\n",
    "for sec in sparse_features:\n",
    "    train_model_input[sec]=train_gbdt_feat[:,int(sec)]\n",
    "for i,sec in enumerate(dense_features):\n",
    "    train_model_input[sec]=x_train[:,dense_pos[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepFM(linear_feature_columns=linear_feature_columns, dnn_feature_columns=dnn_feature_columns, task='binary',\n",
    "               l2_reg_embedding=1e-5, device='cuda:1')\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\",\n",
    "              metrics=[\"binary_crossentropy\", \"auc\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "Train on 4837532 samples, validate on 0 samples, 18897 steps per epoch\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "reduce failed to synchronize: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-2acc289765ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(train_model_input, y_train,\n\u001b[0;32m----> 2\u001b[0;31m           batch_size=256, epochs=50, validation_split=0, verbose=2)\n\u001b[0m",
      "\u001b[0;32m~/miniconda2/envs/ydzhang/lib/python3.6/site-packages/deepctr_torch/models/basemodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, initial_epoch, validation_split, validation_data, shuffle)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ydzhang/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2113\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: reduce failed to synchronize: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "model.fit(train_model_input, y_train,\n",
    "          batch_size=256, epochs=50, validation_split=0, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('data_df.csv',index_col=0)\n",
    "\n",
    "category_df = member_df[['user_id', 'category_A', 'category_B','category_C', 'category_D', 'category_E','gender', 'visit_freq']]\n",
    "\n",
    "# category_df.columns = ['user_id','category_A_raw','category_B_raw','category_C_raw','category_D_raw','category_E_raw']\n",
    "category_df.columns = [x+'_raw' if x !='user_id' else x for x in category_df.columns ]\n",
    "\n",
    "data_df = pd.merge(data_df,category_df,on='user_id',how='left')\n",
    "\n",
    "drop_columns = ['is_answer', 'question_id', 'user_id', 'topic_id','follow_topics', 'interest_topics']\n",
    "train_secs = data_df.columns.drop(drop_columns)\n",
    "category_columns = category_df.columns.drop('user_id')\n",
    "dense_features = data_df.columns.drop(category_columns).drop(drop_columns)\n",
    "sparse_features = category_columns\n",
    "target = ['is_answer']\n",
    "\n",
    "data_df[sparse_features] = data_df[sparse_features].fillna('-1', )\n",
    "data_df[dense_features] = data_df[dense_features].fillna(0, )\n",
    "\n",
    "for feat in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data_df[feat] = lbe.fit_transform(data_df[feat])\n",
    "\n",
    "# 2.count #unique features for each sparse field,and record dense feature field name\n",
    "\n",
    "fixlen_feature_columns = [SparseFeat(feat, data_df[feat].nunique())\n",
    "                          for feat in sparse_features] + [DenseFeat(feat, 1,)\n",
    "                                                          for feat in dense_features]\n",
    "\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "feature_names = get_feature_names(\n",
    "    linear_feature_columns+ dnn_feature_columns)\n",
    "\n",
    "trian_start = 3838; train_end = 3867\n",
    "train = data_df[train_secs.tolist()+['is_answer']][:train_df.shape[0]][train_df['invite_create_day']>train_end-7]\n",
    "\n",
    "train_model_input = {name:train[name] for name in feature_names}\n",
    "\n",
    "model = wdl.WDL(linear_feature_columns=dnn_feature_columns, dnn_feature_columns=dnn_feature_columns, task='binary',\n",
    "               l2_reg_embedding=1e-5, device='cuda:1')\n",
    "model.compile(\"adam\", \"binary_crossentropy\",\n",
    "              metrics=[\"binary_crossentropy\", \"auc\"],)\n",
    "model.fit(train_model_input, train[target].values,\n",
    "          batch_size=512, epochs=35, validation_split=0, verbose=2)\n",
    "\n",
    "model = DeepFM(linear_feature_columns=linear_feature_columns, dnn_feature_columns=dnn_feature_columns, task='binary',\n",
    "               l2_reg_embedding=1e-5, device='cuda:1')\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\",\n",
    "              metrics=[\"binary_crossentropy\", \"auc\"],)\n",
    "model.fit(train_model_input, train[target].values,\n",
    "          batch_size=256, epochs=50, validation_split=0, verbose=2)\n",
    "\n",
    "test = data_df[train_secs.tolist()+['is_answer']][train_df.shape[0]:].reset_index(drop=True)\n",
    "\n",
    "test_model_input = {name:test[name] for name in feature_names}\n",
    "\n",
    "pred_ans = model.predict(test_model_input, 256)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['is_answer'] = pred_ans\n",
    "test_df.to_csv('result.txt', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "for sec in numeric_sectors:\n",
    "    print(data_df[[sec,'is_answer']].groupby('is_answer').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_sec = [x for x in data_df_copy.columns if 'day' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['invite_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0\n",
    "for sec in numeric_sectors:\n",
    "    print(data_df[[sec,'is_answer']].groupby('is_answer').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入num_answer_nums影响不大，加入word_count_question会显著导致过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[numeric_sectors].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trian_start = 3838; train_end = 3867\n",
    "test_start = 3868; test_end = 3874\n",
    "ans_start = 3807; ans_end = 3867\n",
    "ques_start = 753;qus_end = 3874\n",
    "/home/sc1/zk/pytorch-CycleGAN-and-pix2pix-master/results/cuhk_per_big/test_latest/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df[numeric_sectors] = ans_data.fillna(0,axis=0)\n",
    "# data_df[numeric_sectors] = ans_data.fillna(mean_values,axis=0)\n",
    "# question_bins = [-1,0,1,3,500]\n",
    "# member_bins = [-1,0,1,5,1000]\n",
    "# data_df['question_answer_nums'] =pd.cut(data_df['question_answer_nums'],member_bins,labels=[1,2,3,4]).astype('int')\n",
    "# data_df['user_answer_nums'] =pd.cut(data_df['user_answer_nums'],member_bins,labels=[1,2,3,4]).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMClassifier\n",
    "# model_lgb = LGBMClassifier(boosting_type='gbdt', num_leaves=64, learning_rate=0.01, n_estimators=2000,\n",
    "#                            max_bin=425, subsample_for_bin=50000, objective='binary', min_split_gain=0,\n",
    "#                            min_child_weight=5, min_child_samples=10, subsample=0.8, subsample_freq=1,\n",
    "#                            colsample_bytree=1, reg_alpha=3, reg_lambda=5, seed=1000, n_jobs=-1, silent=True,\n",
    "#                            num_threads = 30)\n",
    "# model_lgb.fit(x_train, y_train, \n",
    "#                   eval_names=['train'],\n",
    "#                   eval_metric=['logloss','auc'],\n",
    "#                   eval_set=[(x_train, y_train)],\n",
    "#                   early_stopping_rounds=3)\n",
    "\n",
    "# y_pred = model_lgb.predict_proba(x_test)[:, 1]\n",
    "# test_df['is_answer'] = y_pred\n",
    "# test_df.to_csv('result.txt', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sec in ques_vecs_secs:\n",
    "    print(data_df[[sec,'is_answer']].groupby('is_answer').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
